{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "print('Libraries imported - ready to use PyTorch', torch.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read ../Datasets/asl_alphabet_train/\n"
     ]
    }
   ],
   "source": [
    "%run prepare_augment_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from torchvision import transforms\n",
    "    from torchvision.datasets import ImageFolder\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.utils.data import random_split\n",
    "\n",
    "    # define transformation to randomly augment image data\n",
    "    transformation = transforms.Compose([\n",
    "        # random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        # random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # normalise pixel values (RGB channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # load all images and transform them\n",
    "    full_dataset = ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "\n",
    "    # split dataset into 70% training and 30% test\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "\n",
    "    # random training/test split\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "    # define loader for training data\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # define loader for testing data\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read ../Datasets/asl_alphabet_train/\n"
     ]
    }
   ],
   "source": [
    "train_folder = '../Datasets/asl_alphabet_train/'\n",
    "\n",
    "train_loader, test_loader = load_dataset(train_folder)\n",
    "batch_size = train_loader.batch_size\n",
    "print('Data loaders ready to read', train_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base model for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                      # generate predications\n",
    "        loss = F.cross_entropy(out, labels)     # calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                      # generate predictions\n",
    "        loss = F.cross_entropy(out, labels)     # calculate loss\n",
    "        acc = accuracy(out, labels)             # calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()       # combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()          # combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print('Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}'.format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']\n",
    "        ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture of CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(ImageClassificationBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Defining the Constructor - define each layer to be used in our model\"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(82944, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 6)\n",
    "        )\n",
    "        # # RGB images, so we have 3 input channels\n",
    "        # super(Net, self).__init__()\n",
    "\n",
    "        # # apply 12 filters in first conv. layer\n",
    "        # self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # # second conv. layer takes 12 input channels and generates 24 outputs\n",
    "        # self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        # # apply max pooling with kernel size 2\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # # drop layer to delete 20% of features to help prevent overfitting\n",
    "        # self.drop = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        # # 200x200 image tensors pooled twice with kernel size 2 -> feature tensors now 50x50, and 24 of them generated\n",
    "\n",
    "        # # flatten feature tensors to feed to fully-connected layer\n",
    "        # self.fc = nn.Linear(in_features=50*50*24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass data through layers defined in constructor\"\"\"\n",
    "        return self.network(x)\n",
    "        # #  use ReLU activation function after layer 1 (convolution 1 & pool)\n",
    "        # x = F.relu(self.pool(self.conv1(x)))\n",
    "        # # use ReLU activation function after layer 2\n",
    "        # x = F.relu(self.pool(self.conv2(x)))\n",
    "        # # select some features to drop to prevent overfitting\n",
    "        # x = F.dropout(self.drop(x), training=self.training)\n",
    "        # # flatten\n",
    "        # x = x.view(-1, 50*50*24)\n",
    "        # # feed to fully-connected layer to predict class\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        # return torch.log_softmax(x, dim=1) # return class probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters, model training, and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=82944, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load data into GPU\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Set device to GPU or CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move data to the device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking = True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into GPU\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(test_loader, device)\n",
    "to_device(model, device)\n",
    "\n",
    "# load model to the device\n",
    "model = to_device(Net(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x160000 and 82944x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# initial evaluation of model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluate(model, test_loader)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML_signLanguage/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[63], line 8\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(model, val_loader):\n\u001b[1;32m      7\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mvalidation_step(batch) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m val_loader]\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mvalidation_epoch_end(outputs)\n",
      "Cell \u001b[0;32mIn[63], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(model, val_loader):\n\u001b[1;32m      7\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39;49mvalidation_step(batch) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m val_loader]\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mvalidation_epoch_end(outputs)\n",
      "Cell \u001b[0;32mIn[61], line 14\u001b[0m, in \u001b[0;36mImageClassificationBase.validation_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m     13\u001b[0m     images, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 14\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(images)                      \u001b[39m# generate predictions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(out, labels)     \u001b[39m# calculate loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     acc \u001b[39m=\u001b[39m accuracy(out, labels)             \u001b[39m# calculate accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML_signLanguage/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[62], line 52\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     51\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Pass data through layers defined in constructor\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML_signLanguage/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML_signLanguage/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML_signLanguage/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML_signLanguage/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x160000 and 82944x1024)"
     ]
    }
   ],
   "source": [
    "# initial evaluation of model\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "\n",
    "history = fit(num_epochs,lr,model,train_loader,test_loader,opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, train_loader, optimizer, epoch):\n",
    "#     # set model to training mode\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     print(\"Epoch: \", epoch)\n",
    "\n",
    "#     # process images in batches\n",
    "#     for batch_idx, (data,target) in enumerate(train_loader):\n",
    "#         # use CPU or GPU as appropriate - but GPU is optimized for our operations\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "\n",
    "#         # reset the optimizer\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # push data forward through model layers\n",
    "#         output = model(data)\n",
    "\n",
    "#         # get loss\n",
    "#         loss = loss_criteria(output, target)\n",
    "\n",
    "#         # keep a running total\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         # backpropagate\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print metrics to see progress\n",
    "#         print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "    \n",
    "#     # return average loss for the epoch\n",
    "#     avg_loss = train_loss / (batch_idx + 1)\n",
    "#     print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "#     return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, device, test_loader):\n",
    "#     # switch to evaluation mode (so we don't backpropagate or drop)\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         batch_count = 0\n",
    "#         for data, target in test_loader:\n",
    "#             batch_count += 1\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "\n",
    "#             # get predicted classes for this batch\n",
    "#             output = model(data)\n",
    "\n",
    "#             # calculate loss for this batch\n",
    "#             test_loss += loss_criteria(output, target).item()\n",
    "\n",
    "#             # calculate accuracy for this batch\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             correct += torch.sum(target == predicted).item()\n",
    "    \n",
    "#     # calculate average loss and total accuracy for this epoch\n",
    "#     avg_loss = test_loss / batch_count\n",
    "#     print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         avg_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)\n",
    "#     ))\n",
    "#     return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch:  1\n",
      "\tTraining batch 1 Loss: 3.356421\n",
      "\tTraining batch 2 Loss: 34.516510\n",
      "\tTraining batch 3 Loss: 15.443350\n",
      "\tTraining batch 4 Loss: 4.228014\n",
      "\tTraining batch 5 Loss: 3.354880\n",
      "\tTraining batch 6 Loss: 3.337567\n",
      "\tTraining batch 7 Loss: 3.334898\n",
      "\tTraining batch 8 Loss: 3.330907\n",
      "\tTraining batch 9 Loss: 3.332478\n",
      "\tTraining batch 10 Loss: 3.337686\n",
      "\tTraining batch 11 Loss: 3.331772\n",
      "\tTraining batch 12 Loss: 3.333709\n",
      "\tTraining batch 13 Loss: 3.327481\n",
      "\tTraining batch 14 Loss: 3.335116\n",
      "\tTraining batch 15 Loss: 3.331629\n",
      "\tTraining batch 16 Loss: 3.333228\n",
      "\tTraining batch 17 Loss: 3.334115\n",
      "\tTraining batch 18 Loss: 3.332591\n",
      "\tTraining batch 19 Loss: 3.334254\n",
      "\tTraining batch 20 Loss: 3.334701\n",
      "\tTraining batch 21 Loss: 3.335555\n",
      "\tTraining batch 22 Loss: 3.331683\n",
      "\tTraining batch 23 Loss: 3.340427\n",
      "\tTraining batch 24 Loss: 3.331813\n",
      "\tTraining batch 25 Loss: 3.324115\n",
      "\tTraining batch 26 Loss: 3.329528\n",
      "\tTraining batch 27 Loss: 3.344965\n",
      "\tTraining batch 28 Loss: 3.322431\n",
      "\tTraining batch 29 Loss: 3.334960\n",
      "\tTraining batch 30 Loss: 3.340191\n",
      "\tTraining batch 31 Loss: 3.338798\n",
      "\tTraining batch 32 Loss: 3.327507\n",
      "\tTraining batch 33 Loss: 3.346486\n",
      "\tTraining batch 34 Loss: 3.330611\n",
      "\tTraining batch 35 Loss: 3.345866\n",
      "\tTraining batch 36 Loss: 3.347148\n",
      "\tTraining batch 37 Loss: 3.337287\n",
      "\tTraining batch 38 Loss: 3.341172\n",
      "\tTraining batch 39 Loss: 3.335782\n",
      "\tTraining batch 40 Loss: 3.327956\n",
      "\tTraining batch 41 Loss: 3.347394\n",
      "\tTraining batch 42 Loss: 3.336814\n",
      "\tTraining batch 43 Loss: 3.350434\n",
      "\tTraining batch 44 Loss: 3.314080\n",
      "\tTraining batch 45 Loss: 3.321712\n",
      "\tTraining batch 46 Loss: 3.346380\n",
      "\tTraining batch 47 Loss: 3.327267\n",
      "\tTraining batch 48 Loss: 3.325134\n",
      "\tTraining batch 49 Loss: 3.333780\n",
      "\tTraining batch 50 Loss: 3.328714\n",
      "\tTraining batch 51 Loss: 3.326518\n",
      "\tTraining batch 52 Loss: 3.332984\n",
      "\tTraining batch 53 Loss: 3.338803\n",
      "\tTraining batch 54 Loss: 3.330567\n",
      "\tTraining batch 55 Loss: 3.333495\n",
      "\tTraining batch 56 Loss: 3.346137\n",
      "\tTraining batch 57 Loss: 3.324025\n",
      "\tTraining batch 58 Loss: 3.328506\n",
      "\tTraining batch 59 Loss: 3.330489\n",
      "\tTraining batch 60 Loss: 3.358095\n",
      "\tTraining batch 61 Loss: 3.334697\n",
      "\tTraining batch 62 Loss: 3.327284\n",
      "\tTraining batch 63 Loss: 3.325526\n",
      "\tTraining batch 64 Loss: 3.331578\n",
      "\tTraining batch 65 Loss: 3.335244\n",
      "\tTraining batch 66 Loss: 3.340783\n",
      "\tTraining batch 67 Loss: 3.326764\n",
      "\tTraining batch 68 Loss: 3.345227\n",
      "\tTraining batch 69 Loss: 3.331507\n",
      "\tTraining batch 70 Loss: 3.346442\n",
      "\tTraining batch 71 Loss: 3.336112\n",
      "\tTraining batch 72 Loss: 3.331795\n",
      "\tTraining batch 73 Loss: 3.327383\n",
      "\tTraining batch 74 Loss: 3.319560\n",
      "\tTraining batch 75 Loss: 3.344326\n",
      "\tTraining batch 76 Loss: 3.337807\n",
      "\tTraining batch 77 Loss: 3.339989\n",
      "\tTraining batch 78 Loss: 3.323058\n",
      "\tTraining batch 79 Loss: 3.329519\n",
      "\tTraining batch 80 Loss: 3.323355\n",
      "\tTraining batch 81 Loss: 3.338716\n",
      "\tTraining batch 82 Loss: 3.327965\n",
      "\tTraining batch 83 Loss: 3.340386\n",
      "\tTraining batch 84 Loss: 3.345061\n",
      "\tTraining batch 85 Loss: 3.322999\n",
      "\tTraining batch 86 Loss: 3.338712\n",
      "\tTraining batch 87 Loss: 3.333607\n",
      "\tTraining batch 88 Loss: 3.316697\n",
      "\tTraining batch 89 Loss: 3.312873\n",
      "\tTraining batch 90 Loss: 3.331989\n",
      "\tTraining batch 91 Loss: 3.336281\n",
      "\tTraining batch 92 Loss: 3.344588\n",
      "\tTraining batch 93 Loss: 3.334197\n",
      "\tTraining batch 94 Loss: 3.330177\n",
      "\tTraining batch 95 Loss: 3.329263\n",
      "\tTraining batch 96 Loss: 3.320366\n",
      "\tTraining batch 97 Loss: 3.335451\n",
      "\tTraining batch 98 Loss: 3.322093\n",
      "\tTraining batch 99 Loss: 3.359433\n",
      "\tTraining batch 100 Loss: 3.323037\n",
      "\tTraining batch 101 Loss: 3.325200\n",
      "\tTraining batch 102 Loss: 3.343632\n",
      "\tTraining batch 103 Loss: 3.319358\n",
      "\tTraining batch 104 Loss: 3.328909\n",
      "\tTraining batch 105 Loss: 3.342882\n",
      "\tTraining batch 106 Loss: 3.332139\n",
      "\tTraining batch 107 Loss: 3.333559\n",
      "\tTraining batch 108 Loss: 3.346810\n",
      "\tTraining batch 109 Loss: 3.322775\n",
      "\tTraining batch 110 Loss: 3.349767\n",
      "\tTraining batch 111 Loss: 3.347837\n",
      "\tTraining batch 112 Loss: 3.322492\n",
      "\tTraining batch 113 Loss: 3.324144\n",
      "\tTraining batch 114 Loss: 3.344805\n",
      "\tTraining batch 115 Loss: 3.329440\n",
      "\tTraining batch 116 Loss: 3.337692\n",
      "\tTraining batch 117 Loss: 3.330487\n",
      "\tTraining batch 118 Loss: 3.327402\n",
      "\tTraining batch 119 Loss: 3.321264\n",
      "\tTraining batch 120 Loss: 3.342884\n",
      "\tTraining batch 121 Loss: 3.344282\n",
      "\tTraining batch 122 Loss: 3.334056\n",
      "\tTraining batch 123 Loss: 3.324761\n",
      "\tTraining batch 124 Loss: 3.331384\n",
      "\tTraining batch 125 Loss: 3.344189\n",
      "\tTraining batch 126 Loss: 3.327890\n",
      "\tTraining batch 127 Loss: 3.340191\n",
      "\tTraining batch 128 Loss: 3.302786\n",
      "\tTraining batch 129 Loss: 3.351086\n",
      "\tTraining batch 130 Loss: 3.322434\n",
      "\tTraining batch 131 Loss: 3.324816\n",
      "\tTraining batch 132 Loss: 3.349786\n",
      "\tTraining batch 133 Loss: 3.338870\n",
      "\tTraining batch 134 Loss: 3.337180\n",
      "\tTraining batch 135 Loss: 3.331520\n",
      "\tTraining batch 136 Loss: 3.354543\n",
      "\tTraining batch 137 Loss: 3.331461\n",
      "\tTraining batch 138 Loss: 3.329564\n",
      "\tTraining batch 139 Loss: 3.337269\n",
      "\tTraining batch 140 Loss: 3.325540\n",
      "\tTraining batch 141 Loss: 3.325006\n",
      "\tTraining batch 142 Loss: 3.337448\n",
      "\tTraining batch 143 Loss: 3.340692\n",
      "\tTraining batch 144 Loss: 3.334739\n",
      "\tTraining batch 145 Loss: 3.336182\n",
      "\tTraining batch 146 Loss: 3.315417\n",
      "\tTraining batch 147 Loss: 3.354429\n",
      "\tTraining batch 148 Loss: 3.332004\n",
      "\tTraining batch 149 Loss: 3.329728\n",
      "\tTraining batch 150 Loss: 3.330453\n",
      "\tTraining batch 151 Loss: 3.320550\n",
      "\tTraining batch 152 Loss: 3.313139\n",
      "\tTraining batch 153 Loss: 3.333738\n",
      "\tTraining batch 154 Loss: 3.323584\n",
      "\tTraining batch 155 Loss: 3.340670\n",
      "\tTraining batch 156 Loss: 3.333192\n",
      "\tTraining batch 157 Loss: 3.326157\n",
      "\tTraining batch 158 Loss: 3.339458\n",
      "\tTraining batch 159 Loss: 3.316898\n",
      "\tTraining batch 160 Loss: 3.331073\n",
      "\tTraining batch 161 Loss: 3.323814\n",
      "\tTraining batch 162 Loss: 3.325657\n",
      "\tTraining batch 163 Loss: 3.324657\n",
      "\tTraining batch 164 Loss: 3.321643\n",
      "\tTraining batch 165 Loss: 3.330518\n",
      "\tTraining batch 166 Loss: 3.323584\n",
      "\tTraining batch 167 Loss: 3.341715\n",
      "\tTraining batch 168 Loss: 3.355432\n",
      "\tTraining batch 169 Loss: 3.353403\n",
      "\tTraining batch 170 Loss: 3.329020\n",
      "\tTraining batch 171 Loss: 3.327877\n",
      "\tTraining batch 172 Loss: 3.324873\n",
      "\tTraining batch 173 Loss: 3.347531\n",
      "\tTraining batch 174 Loss: 3.344079\n",
      "\tTraining batch 175 Loss: 3.335253\n",
      "\tTraining batch 176 Loss: 3.352485\n",
      "\tTraining batch 177 Loss: 3.351306\n",
      "\tTraining batch 178 Loss: 3.333703\n",
      "\tTraining batch 179 Loss: 3.317277\n",
      "\tTraining batch 180 Loss: 3.334573\n",
      "\tTraining batch 181 Loss: 3.332847\n",
      "\tTraining batch 182 Loss: 3.329298\n",
      "\tTraining batch 183 Loss: 3.349120\n",
      "\tTraining batch 184 Loss: 3.325939\n",
      "\tTraining batch 185 Loss: 3.355949\n",
      "\tTraining batch 186 Loss: 3.336852\n",
      "\tTraining batch 187 Loss: 3.330433\n",
      "\tTraining batch 188 Loss: 3.345115\n",
      "\tTraining batch 189 Loss: 3.327672\n",
      "\tTraining batch 190 Loss: 3.319123\n",
      "\tTraining batch 191 Loss: 3.313744\n",
      "\tTraining batch 192 Loss: 3.333153\n",
      "\tTraining batch 193 Loss: 3.336566\n",
      "\tTraining batch 194 Loss: 3.332109\n",
      "\tTraining batch 195 Loss: 3.336771\n",
      "\tTraining batch 196 Loss: 3.317353\n",
      "\tTraining batch 197 Loss: 3.319416\n",
      "\tTraining batch 198 Loss: 3.341745\n",
      "\tTraining batch 199 Loss: 3.326565\n",
      "\tTraining batch 200 Loss: 3.319589\n",
      "\tTraining batch 201 Loss: 3.340991\n",
      "\tTraining batch 202 Loss: 3.329667\n",
      "\tTraining batch 203 Loss: 3.322933\n",
      "\tTraining batch 204 Loss: 3.327211\n",
      "\tTraining batch 205 Loss: 3.349514\n",
      "\tTraining batch 206 Loss: 3.341160\n",
      "\tTraining batch 207 Loss: 3.321241\n",
      "\tTraining batch 208 Loss: 3.338736\n",
      "\tTraining batch 209 Loss: 3.354140\n",
      "\tTraining batch 210 Loss: 3.332468\n",
      "\tTraining batch 211 Loss: 3.332042\n",
      "\tTraining batch 212 Loss: 3.337829\n",
      "\tTraining batch 213 Loss: 3.320912\n",
      "\tTraining batch 214 Loss: 3.330428\n",
      "\tTraining batch 215 Loss: 3.330752\n",
      "\tTraining batch 216 Loss: 3.342454\n",
      "\tTraining batch 217 Loss: 3.336112\n",
      "\tTraining batch 218 Loss: 3.334698\n",
      "\tTraining batch 219 Loss: 3.341633\n",
      "\tTraining batch 220 Loss: 3.350151\n",
      "\tTraining batch 221 Loss: 3.325665\n",
      "\tTraining batch 222 Loss: 3.336949\n",
      "\tTraining batch 223 Loss: 3.322344\n",
      "\tTraining batch 224 Loss: 3.354656\n",
      "\tTraining batch 225 Loss: 3.323825\n",
      "\tTraining batch 226 Loss: 3.342735\n",
      "\tTraining batch 227 Loss: 3.344920\n",
      "\tTraining batch 228 Loss: 3.354146\n",
      "\tTraining batch 229 Loss: 3.326622\n",
      "\tTraining batch 230 Loss: 3.346079\n",
      "\tTraining batch 231 Loss: 3.325921\n",
      "\tTraining batch 232 Loss: 3.349639\n",
      "\tTraining batch 233 Loss: 3.340079\n",
      "\tTraining batch 234 Loss: 3.344395\n",
      "\tTraining batch 235 Loss: 3.334882\n",
      "\tTraining batch 236 Loss: 3.324530\n",
      "\tTraining batch 237 Loss: 3.337259\n",
      "\tTraining batch 238 Loss: 3.318648\n",
      "\tTraining batch 239 Loss: 3.338116\n",
      "\tTraining batch 240 Loss: 3.318562\n",
      "\tTraining batch 241 Loss: 3.321896\n",
      "\tTraining batch 242 Loss: 3.310373\n",
      "\tTraining batch 243 Loss: 3.312699\n",
      "\tTraining batch 244 Loss: 3.342731\n",
      "\tTraining batch 245 Loss: 3.339127\n",
      "\tTraining batch 246 Loss: 3.317076\n",
      "\tTraining batch 247 Loss: 3.323080\n",
      "\tTraining batch 248 Loss: 3.331955\n",
      "\tTraining batch 249 Loss: 3.346893\n",
      "\tTraining batch 250 Loss: 3.331649\n",
      "\tTraining batch 251 Loss: 3.357965\n",
      "\tTraining batch 252 Loss: 3.326750\n",
      "\tTraining batch 253 Loss: 3.353888\n",
      "\tTraining batch 254 Loss: 3.341116\n",
      "\tTraining batch 255 Loss: 3.351557\n",
      "\tTraining batch 256 Loss: 3.328202\n",
      "\tTraining batch 257 Loss: 3.345210\n",
      "\tTraining batch 258 Loss: 3.359038\n",
      "\tTraining batch 259 Loss: 3.318345\n",
      "\tTraining batch 260 Loss: 3.339365\n",
      "\tTraining batch 261 Loss: 3.327803\n",
      "\tTraining batch 262 Loss: 3.343810\n",
      "\tTraining batch 263 Loss: 3.344767\n",
      "\tTraining batch 264 Loss: 3.348514\n",
      "\tTraining batch 265 Loss: 3.335685\n",
      "\tTraining batch 266 Loss: 3.342508\n",
      "\tTraining batch 267 Loss: 3.330138\n",
      "\tTraining batch 268 Loss: 3.352558\n",
      "\tTraining batch 269 Loss: 3.354268\n",
      "\tTraining batch 270 Loss: 3.317860\n",
      "\tTraining batch 271 Loss: 3.321091\n",
      "\tTraining batch 272 Loss: 3.318929\n",
      "\tTraining batch 273 Loss: 3.326256\n",
      "\tTraining batch 274 Loss: 3.321964\n",
      "\tTraining batch 275 Loss: 3.331139\n",
      "\tTraining batch 276 Loss: 3.345332\n",
      "\tTraining batch 277 Loss: 3.330775\n",
      "\tTraining batch 278 Loss: 3.351389\n",
      "\tTraining batch 279 Loss: 3.326280\n",
      "\tTraining batch 280 Loss: 3.340743\n",
      "\tTraining batch 281 Loss: 3.329371\n",
      "\tTraining batch 282 Loss: 3.339590\n",
      "\tTraining batch 283 Loss: 3.341482\n",
      "\tTraining batch 284 Loss: 3.329573\n",
      "\tTraining batch 285 Loss: 3.330839\n",
      "\tTraining batch 286 Loss: 3.343834\n",
      "\tTraining batch 287 Loss: 3.342245\n",
      "\tTraining batch 288 Loss: 3.324314\n",
      "\tTraining batch 289 Loss: 3.348615\n",
      "\tTraining batch 290 Loss: 3.337296\n",
      "\tTraining batch 291 Loss: 3.326393\n",
      "\tTraining batch 292 Loss: 3.330549\n",
      "\tTraining batch 293 Loss: 3.352588\n",
      "\tTraining batch 294 Loss: 3.339010\n",
      "\tTraining batch 295 Loss: 3.323397\n",
      "\tTraining batch 296 Loss: 3.332285\n",
      "\tTraining batch 297 Loss: 3.334117\n",
      "\tTraining batch 298 Loss: 3.352876\n",
      "\tTraining batch 299 Loss: 3.335829\n",
      "\tTraining batch 300 Loss: 3.318161\n",
      "\tTraining batch 301 Loss: 3.336812\n",
      "\tTraining batch 302 Loss: 3.330482\n",
      "\tTraining batch 303 Loss: 3.331352\n",
      "\tTraining batch 304 Loss: 3.331104\n",
      "\tTraining batch 305 Loss: 3.330543\n",
      "\tTraining batch 306 Loss: 3.341712\n",
      "\tTraining batch 307 Loss: 3.328911\n",
      "\tTraining batch 308 Loss: 3.340334\n",
      "\tTraining batch 309 Loss: 3.332031\n",
      "\tTraining batch 310 Loss: 3.323003\n",
      "\tTraining batch 311 Loss: 3.332130\n",
      "\tTraining batch 312 Loss: 3.325711\n",
      "\tTraining batch 313 Loss: 3.341904\n",
      "\tTraining batch 314 Loss: 3.328351\n",
      "\tTraining batch 315 Loss: 3.330078\n",
      "\tTraining batch 316 Loss: 3.337062\n",
      "\tTraining batch 317 Loss: 3.331200\n",
      "\tTraining batch 318 Loss: 3.320320\n",
      "\tTraining batch 319 Loss: 3.338510\n",
      "\tTraining batch 320 Loss: 3.332752\n",
      "\tTraining batch 321 Loss: 3.331094\n",
      "\tTraining batch 322 Loss: 3.319805\n",
      "\tTraining batch 323 Loss: 3.322525\n",
      "\tTraining batch 324 Loss: 3.317323\n",
      "\tTraining batch 325 Loss: 3.335387\n",
      "\tTraining batch 326 Loss: 3.326187\n",
      "\tTraining batch 327 Loss: 3.340670\n",
      "\tTraining batch 328 Loss: 3.333266\n",
      "\tTraining batch 329 Loss: 3.340869\n",
      "\tTraining batch 330 Loss: 3.359125\n",
      "\tTraining batch 331 Loss: 3.313662\n",
      "\tTraining batch 332 Loss: 3.330416\n",
      "\tTraining batch 333 Loss: 3.332958\n",
      "\tTraining batch 334 Loss: 3.334515\n",
      "\tTraining batch 335 Loss: 3.322154\n",
      "\tTraining batch 336 Loss: 3.353348\n",
      "\tTraining batch 337 Loss: 3.331444\n",
      "\tTraining batch 338 Loss: 3.339441\n",
      "\tTraining batch 339 Loss: 3.350848\n",
      "\tTraining batch 340 Loss: 3.336478\n",
      "\tTraining batch 341 Loss: 3.346931\n",
      "\tTraining batch 342 Loss: 3.348952\n",
      "\tTraining batch 343 Loss: 3.334806\n",
      "\tTraining batch 344 Loss: 3.335067\n",
      "\tTraining batch 345 Loss: 3.329501\n",
      "\tTraining batch 346 Loss: 3.326859\n",
      "\tTraining batch 347 Loss: 3.314216\n",
      "\tTraining batch 348 Loss: 3.338018\n",
      "\tTraining batch 349 Loss: 3.333831\n",
      "\tTraining batch 350 Loss: 3.353567\n",
      "\tTraining batch 351 Loss: 3.311088\n",
      "\tTraining batch 352 Loss: 3.344757\n",
      "\tTraining batch 353 Loss: 3.338617\n",
      "\tTraining batch 354 Loss: 3.341261\n",
      "\tTraining batch 355 Loss: 3.323895\n",
      "\tTraining batch 356 Loss: 3.325672\n",
      "\tTraining batch 357 Loss: 3.330259\n",
      "\tTraining batch 358 Loss: 3.322903\n",
      "\tTraining batch 359 Loss: 3.323654\n",
      "\tTraining batch 360 Loss: 3.346393\n",
      "\tTraining batch 361 Loss: 3.329684\n",
      "\tTraining batch 362 Loss: 3.334048\n",
      "\tTraining batch 363 Loss: 3.327838\n",
      "\tTraining batch 364 Loss: 3.337318\n",
      "\tTraining batch 365 Loss: 3.317429\n",
      "\tTraining batch 366 Loss: 3.320520\n",
      "\tTraining batch 367 Loss: 3.323433\n",
      "\tTraining batch 368 Loss: 3.320593\n",
      "\tTraining batch 369 Loss: 3.331938\n",
      "\tTraining batch 370 Loss: 3.326470\n",
      "\tTraining batch 371 Loss: 3.335020\n",
      "\tTraining batch 372 Loss: 3.340161\n",
      "\tTraining batch 373 Loss: 3.336707\n",
      "\tTraining batch 374 Loss: 3.315904\n",
      "\tTraining batch 375 Loss: 3.322079\n",
      "\tTraining batch 376 Loss: 3.331298\n",
      "\tTraining batch 377 Loss: 3.326210\n",
      "\tTraining batch 378 Loss: 3.329216\n",
      "\tTraining batch 379 Loss: 3.328350\n",
      "\tTraining batch 380 Loss: 3.321006\n",
      "\tTraining batch 381 Loss: 3.343152\n",
      "\tTraining batch 382 Loss: 3.322915\n",
      "\tTraining batch 383 Loss: 3.324001\n",
      "\tTraining batch 384 Loss: 3.362388\n",
      "\tTraining batch 385 Loss: 3.340356\n",
      "\tTraining batch 386 Loss: 3.343980\n",
      "\tTraining batch 387 Loss: 3.347324\n",
      "\tTraining batch 388 Loss: 3.331759\n",
      "\tTraining batch 389 Loss: 3.362326\n",
      "\tTraining batch 390 Loss: 3.327533\n",
      "\tTraining batch 391 Loss: 3.344613\n",
      "\tTraining batch 392 Loss: 3.335135\n",
      "\tTraining batch 393 Loss: 3.333976\n",
      "\tTraining batch 394 Loss: 3.339980\n",
      "\tTraining batch 395 Loss: 3.345631\n",
      "\tTraining batch 396 Loss: 3.342083\n",
      "\tTraining batch 397 Loss: 3.339840\n",
      "\tTraining batch 398 Loss: 3.348808\n",
      "\tTraining batch 399 Loss: 3.335816\n",
      "\tTraining batch 400 Loss: 3.351973\n",
      "\tTraining batch 401 Loss: 3.320664\n",
      "\tTraining batch 402 Loss: 3.349487\n",
      "\tTraining batch 403 Loss: 3.329921\n",
      "\tTraining batch 404 Loss: 3.363496\n",
      "\tTraining batch 405 Loss: 3.339040\n",
      "\tTraining batch 406 Loss: 3.333743\n",
      "\tTraining batch 407 Loss: 3.338271\n",
      "\tTraining batch 408 Loss: 3.337271\n",
      "\tTraining batch 409 Loss: 3.332265\n",
      "\tTraining batch 410 Loss: 3.338323\n",
      "\tTraining batch 411 Loss: 3.312242\n",
      "\tTraining batch 412 Loss: 3.327826\n",
      "\tTraining batch 413 Loss: 3.329501\n",
      "\tTraining batch 414 Loss: 3.345294\n",
      "\tTraining batch 415 Loss: 3.331657\n",
      "\tTraining batch 416 Loss: 3.313213\n",
      "\tTraining batch 417 Loss: 3.319311\n",
      "\tTraining batch 418 Loss: 3.321528\n",
      "\tTraining batch 419 Loss: 3.325766\n",
      "\tTraining batch 420 Loss: 3.337023\n",
      "\tTraining batch 421 Loss: 3.359085\n",
      "\tTraining batch 422 Loss: 3.349334\n",
      "\tTraining batch 423 Loss: 3.306267\n",
      "\tTraining batch 424 Loss: 3.327199\n",
      "\tTraining batch 425 Loss: 3.338495\n",
      "\tTraining batch 426 Loss: 3.328908\n",
      "\tTraining batch 427 Loss: 3.309236\n",
      "\tTraining batch 428 Loss: 3.345309\n",
      "\tTraining batch 429 Loss: 3.333246\n",
      "\tTraining batch 430 Loss: 3.349837\n",
      "\tTraining batch 431 Loss: 3.323774\n",
      "\tTraining batch 432 Loss: 3.342994\n",
      "\tTraining batch 433 Loss: 3.326973\n",
      "\tTraining batch 434 Loss: 3.335698\n",
      "\tTraining batch 435 Loss: 3.322817\n",
      "\tTraining batch 436 Loss: 3.339931\n",
      "\tTraining batch 437 Loss: 3.352899\n",
      "\tTraining batch 438 Loss: 3.342019\n",
      "\tTraining batch 439 Loss: 3.337785\n",
      "\tTraining batch 440 Loss: 3.330214\n",
      "\tTraining batch 441 Loss: 3.345999\n",
      "\tTraining batch 442 Loss: 3.329318\n",
      "\tTraining batch 443 Loss: 3.315684\n",
      "\tTraining batch 444 Loss: 3.320201\n",
      "\tTraining batch 445 Loss: 3.322403\n",
      "\tTraining batch 446 Loss: 3.342632\n",
      "\tTraining batch 447 Loss: 3.342127\n",
      "\tTraining batch 448 Loss: 3.348411\n",
      "\tTraining batch 449 Loss: 3.312417\n",
      "\tTraining batch 450 Loss: 3.340424\n",
      "\tTraining batch 451 Loss: 3.323398\n",
      "\tTraining batch 452 Loss: 3.320190\n",
      "\tTraining batch 453 Loss: 3.333579\n",
      "\tTraining batch 454 Loss: 3.345168\n",
      "\tTraining batch 455 Loss: 3.333185\n",
      "\tTraining batch 456 Loss: 3.342753\n",
      "\tTraining batch 457 Loss: 3.309361\n",
      "\tTraining batch 458 Loss: 3.322188\n",
      "\tTraining batch 459 Loss: 3.327250\n",
      "\tTraining batch 460 Loss: 3.340186\n",
      "\tTraining batch 461 Loss: 3.334300\n",
      "\tTraining batch 462 Loss: 3.339455\n",
      "\tTraining batch 463 Loss: 3.306103\n",
      "\tTraining batch 464 Loss: 3.335244\n",
      "\tTraining batch 465 Loss: 3.336233\n",
      "\tTraining batch 466 Loss: 3.334411\n",
      "\tTraining batch 467 Loss: 3.340016\n",
      "\tTraining batch 468 Loss: 3.344943\n",
      "\tTraining batch 469 Loss: 3.328689\n",
      "\tTraining batch 470 Loss: 3.308891\n",
      "\tTraining batch 471 Loss: 3.352944\n",
      "\tTraining batch 472 Loss: 3.337543\n",
      "\tTraining batch 473 Loss: 3.333114\n",
      "\tTraining batch 474 Loss: 3.331623\n",
      "\tTraining batch 475 Loss: 3.351683\n",
      "\tTraining batch 476 Loss: 3.333611\n",
      "\tTraining batch 477 Loss: 3.334691\n",
      "\tTraining batch 478 Loss: 3.349687\n",
      "\tTraining batch 479 Loss: 3.332653\n",
      "\tTraining batch 480 Loss: 3.331926\n",
      "\tTraining batch 481 Loss: 3.344704\n",
      "\tTraining batch 482 Loss: 3.335559\n",
      "\tTraining batch 483 Loss: 3.316381\n",
      "\tTraining batch 484 Loss: 3.325697\n",
      "\tTraining batch 485 Loss: 3.344928\n",
      "\tTraining batch 486 Loss: 3.366467\n",
      "\tTraining batch 487 Loss: 3.355883\n",
      "\tTraining batch 488 Loss: 3.322334\n",
      "\tTraining batch 489 Loss: 3.336524\n",
      "\tTraining batch 490 Loss: 3.344010\n",
      "\tTraining batch 491 Loss: 3.316421\n",
      "\tTraining batch 492 Loss: 3.318490\n",
      "\tTraining batch 493 Loss: 3.318156\n",
      "\tTraining batch 494 Loss: 3.339818\n",
      "\tTraining batch 495 Loss: 3.340994\n",
      "\tTraining batch 496 Loss: 3.360576\n",
      "\tTraining batch 497 Loss: 3.330664\n",
      "\tTraining batch 498 Loss: 3.337237\n",
      "\tTraining batch 499 Loss: 3.355572\n",
      "\tTraining batch 500 Loss: 3.344987\n",
      "\tTraining batch 501 Loss: 3.347025\n",
      "\tTraining batch 502 Loss: 3.332133\n",
      "\tTraining batch 503 Loss: 3.335812\n",
      "\tTraining batch 504 Loss: 3.332317\n",
      "\tTraining batch 505 Loss: 3.323853\n",
      "\tTraining batch 506 Loss: 3.343956\n",
      "\tTraining batch 507 Loss: 3.343644\n",
      "\tTraining batch 508 Loss: 3.339127\n",
      "\tTraining batch 509 Loss: 3.321944\n",
      "\tTraining batch 510 Loss: 3.352978\n",
      "\tTraining batch 511 Loss: 3.339687\n",
      "\tTraining batch 512 Loss: 3.321051\n",
      "\tTraining batch 513 Loss: 3.351262\n",
      "\tTraining batch 514 Loss: 3.339613\n",
      "\tTraining batch 515 Loss: 3.326613\n",
      "\tTraining batch 516 Loss: 3.329066\n",
      "\tTraining batch 517 Loss: 3.322585\n",
      "\tTraining batch 518 Loss: 3.340837\n",
      "\tTraining batch 519 Loss: 3.335883\n",
      "\tTraining batch 520 Loss: 3.331787\n",
      "\tTraining batch 521 Loss: 3.330926\n",
      "\tTraining batch 522 Loss: 3.334499\n",
      "\tTraining batch 523 Loss: 3.333668\n",
      "\tTraining batch 524 Loss: 3.338389\n",
      "\tTraining batch 525 Loss: 3.341134\n",
      "\tTraining batch 526 Loss: 3.339110\n",
      "\tTraining batch 527 Loss: 3.336751\n",
      "\tTraining batch 528 Loss: 3.339647\n",
      "\tTraining batch 529 Loss: 3.337778\n",
      "\tTraining batch 530 Loss: 3.340391\n",
      "\tTraining batch 531 Loss: 3.336536\n",
      "\tTraining batch 532 Loss: 3.336968\n",
      "\tTraining batch 533 Loss: 3.347986\n",
      "\tTraining batch 534 Loss: 3.331911\n",
      "\tTraining batch 535 Loss: 3.327594\n",
      "\tTraining batch 536 Loss: 3.340657\n",
      "\tTraining batch 537 Loss: 3.331579\n",
      "\tTraining batch 538 Loss: 3.310993\n",
      "\tTraining batch 539 Loss: 3.340722\n",
      "\tTraining batch 540 Loss: 3.336517\n",
      "\tTraining batch 541 Loss: 3.356901\n",
      "\tTraining batch 542 Loss: 3.343233\n",
      "\tTraining batch 543 Loss: 3.339768\n",
      "\tTraining batch 544 Loss: 3.329585\n",
      "\tTraining batch 545 Loss: 3.352516\n",
      "\tTraining batch 546 Loss: 3.328979\n",
      "\tTraining batch 547 Loss: 3.333206\n",
      "\tTraining batch 548 Loss: 3.330534\n",
      "\tTraining batch 549 Loss: 3.338825\n",
      "\tTraining batch 550 Loss: 3.340809\n",
      "\tTraining batch 551 Loss: 3.330546\n",
      "\tTraining batch 552 Loss: 3.346746\n",
      "\tTraining batch 553 Loss: 3.323914\n",
      "\tTraining batch 554 Loss: 3.345718\n",
      "\tTraining batch 555 Loss: 3.330083\n",
      "\tTraining batch 556 Loss: 3.339643\n",
      "\tTraining batch 557 Loss: 3.337526\n",
      "\tTraining batch 558 Loss: 3.337259\n",
      "\tTraining batch 559 Loss: 3.326425\n",
      "\tTraining batch 560 Loss: 3.329967\n",
      "\tTraining batch 561 Loss: 3.315376\n",
      "\tTraining batch 562 Loss: 3.331524\n",
      "\tTraining batch 563 Loss: 3.327536\n",
      "\tTraining batch 564 Loss: 3.346250\n",
      "\tTraining batch 565 Loss: 3.325191\n",
      "\tTraining batch 566 Loss: 3.333178\n",
      "\tTraining batch 567 Loss: 3.306179\n",
      "\tTraining batch 568 Loss: 3.337755\n",
      "\tTraining batch 569 Loss: 3.329214\n",
      "\tTraining batch 570 Loss: 3.313135\n",
      "\tTraining batch 571 Loss: 3.319627\n",
      "\tTraining batch 572 Loss: 3.336108\n",
      "\tTraining batch 573 Loss: 3.334159\n",
      "\tTraining batch 574 Loss: 3.328456\n",
      "\tTraining batch 575 Loss: 3.323154\n",
      "\tTraining batch 576 Loss: 3.347050\n",
      "\tTraining batch 577 Loss: 3.324797\n",
      "\tTraining batch 578 Loss: 3.355923\n",
      "\tTraining batch 579 Loss: 3.343986\n",
      "\tTraining batch 580 Loss: 3.331926\n",
      "\tTraining batch 581 Loss: 3.315449\n",
      "\tTraining batch 582 Loss: 3.321908\n",
      "\tTraining batch 583 Loss: 3.321221\n",
      "\tTraining batch 584 Loss: 3.320466\n",
      "\tTraining batch 585 Loss: 3.316969\n",
      "\tTraining batch 586 Loss: 3.322999\n",
      "\tTraining batch 587 Loss: 3.339180\n",
      "\tTraining batch 588 Loss: 3.335157\n",
      "\tTraining batch 589 Loss: 3.327028\n",
      "\tTraining batch 590 Loss: 3.329710\n",
      "\tTraining batch 591 Loss: 3.337358\n",
      "\tTraining batch 592 Loss: 3.341452\n",
      "\tTraining batch 593 Loss: 3.330137\n",
      "\tTraining batch 594 Loss: 3.323605\n",
      "\tTraining batch 595 Loss: 3.341169\n",
      "\tTraining batch 596 Loss: 3.337905\n",
      "\tTraining batch 597 Loss: 3.343185\n",
      "\tTraining batch 598 Loss: 3.313184\n",
      "\tTraining batch 599 Loss: 3.331745\n",
      "\tTraining batch 600 Loss: 3.355581\n",
      "\tTraining batch 601 Loss: 3.344250\n",
      "\tTraining batch 602 Loss: 3.326327\n",
      "\tTraining batch 603 Loss: 3.334592\n",
      "\tTraining batch 604 Loss: 3.334111\n",
      "\tTraining batch 605 Loss: 3.325641\n",
      "\tTraining batch 606 Loss: 3.326905\n",
      "\tTraining batch 607 Loss: 3.356518\n",
      "\tTraining batch 608 Loss: 3.328699\n",
      "\tTraining batch 609 Loss: 3.352123\n",
      "\tTraining batch 610 Loss: 3.327071\n",
      "\tTraining batch 611 Loss: 3.332844\n",
      "\tTraining batch 612 Loss: 3.341879\n",
      "\tTraining batch 613 Loss: 3.310456\n",
      "\tTraining batch 614 Loss: 3.338995\n",
      "\tTraining batch 615 Loss: 3.334589\n",
      "\tTraining batch 616 Loss: 3.328328\n",
      "\tTraining batch 617 Loss: 3.348192\n",
      "\tTraining batch 618 Loss: 3.331407\n",
      "\tTraining batch 619 Loss: 3.327519\n",
      "\tTraining batch 620 Loss: 3.331616\n",
      "\tTraining batch 621 Loss: 3.338983\n",
      "\tTraining batch 622 Loss: 3.334644\n",
      "\tTraining batch 623 Loss: 3.357735\n",
      "\tTraining batch 624 Loss: 3.332358\n",
      "\tTraining batch 625 Loss: 3.324186\n",
      "\tTraining batch 626 Loss: 3.342722\n",
      "\tTraining batch 627 Loss: 3.337296\n",
      "\tTraining batch 628 Loss: 3.329862\n",
      "\tTraining batch 629 Loss: 3.372200\n",
      "\tTraining batch 630 Loss: 3.327942\n",
      "\tTraining batch 631 Loss: 3.337580\n",
      "\tTraining batch 632 Loss: 3.344065\n",
      "\tTraining batch 633 Loss: 3.333083\n",
      "\tTraining batch 634 Loss: 3.377479\n",
      "\tTraining batch 635 Loss: 3.328730\n",
      "\tTraining batch 636 Loss: 3.333553\n",
      "\tTraining batch 637 Loss: 3.347151\n",
      "\tTraining batch 638 Loss: 3.318238\n",
      "\tTraining batch 639 Loss: 3.362842\n",
      "\tTraining batch 640 Loss: 3.329826\n",
      "\tTraining batch 641 Loss: 3.333885\n",
      "\tTraining batch 642 Loss: 3.343120\n",
      "\tTraining batch 643 Loss: 3.322754\n",
      "\tTraining batch 644 Loss: 3.337227\n",
      "\tTraining batch 645 Loss: 3.336506\n",
      "\tTraining batch 646 Loss: 3.341993\n",
      "\tTraining batch 647 Loss: 3.341922\n",
      "\tTraining batch 648 Loss: 3.336534\n",
      "\tTraining batch 649 Loss: 3.324549\n",
      "\tTraining batch 650 Loss: 3.341225\n",
      "\tTraining batch 651 Loss: 3.325554\n",
      "\tTraining batch 652 Loss: 3.316561\n",
      "\tTraining batch 653 Loss: 3.352738\n",
      "\tTraining batch 654 Loss: 3.339110\n",
      "\tTraining batch 655 Loss: 3.332767\n",
      "\tTraining batch 656 Loss: 3.337682\n",
      "\tTraining batch 657 Loss: 3.325900\n",
      "\tTraining batch 658 Loss: 3.319697\n",
      "\tTraining batch 659 Loss: 3.323629\n",
      "\tTraining batch 660 Loss: 3.334844\n",
      "\tTraining batch 661 Loss: 3.328094\n",
      "\tTraining batch 662 Loss: 3.322919\n",
      "\tTraining batch 663 Loss: 3.320535\n",
      "\tTraining batch 664 Loss: 3.333971\n",
      "\tTraining batch 665 Loss: 3.328959\n",
      "\tTraining batch 666 Loss: 3.328859\n",
      "\tTraining batch 667 Loss: 3.330689\n",
      "\tTraining batch 668 Loss: 3.335820\n",
      "\tTraining batch 669 Loss: 3.350191\n",
      "\tTraining batch 670 Loss: 3.325668\n",
      "\tTraining batch 671 Loss: 3.337505\n",
      "\tTraining batch 672 Loss: 3.333064\n",
      "\tTraining batch 673 Loss: 3.331368\n",
      "\tTraining batch 674 Loss: 3.339874\n",
      "\tTraining batch 675 Loss: 3.350385\n",
      "\tTraining batch 676 Loss: 3.340418\n",
      "\tTraining batch 677 Loss: 3.330717\n",
      "\tTraining batch 678 Loss: 3.346876\n",
      "\tTraining batch 679 Loss: 3.313679\n",
      "\tTraining batch 680 Loss: 3.344541\n",
      "\tTraining batch 681 Loss: 3.362166\n",
      "\tTraining batch 682 Loss: 3.321976\n",
      "\tTraining batch 683 Loss: 3.334797\n",
      "\tTraining batch 684 Loss: 3.305750\n",
      "\tTraining batch 685 Loss: 3.329549\n",
      "\tTraining batch 686 Loss: 3.330297\n",
      "\tTraining batch 687 Loss: 3.323972\n",
      "\tTraining batch 688 Loss: 3.349046\n",
      "\tTraining batch 689 Loss: 3.337206\n",
      "\tTraining batch 690 Loss: 3.350847\n",
      "\tTraining batch 691 Loss: 3.329170\n",
      "\tTraining batch 692 Loss: 3.321378\n",
      "\tTraining batch 693 Loss: 3.331572\n",
      "\tTraining batch 694 Loss: 3.343846\n",
      "\tTraining batch 695 Loss: 3.331652\n",
      "\tTraining batch 696 Loss: 3.344773\n",
      "\tTraining batch 697 Loss: 3.321454\n",
      "\tTraining batch 698 Loss: 3.346521\n",
      "\tTraining batch 699 Loss: 3.342163\n",
      "\tTraining batch 700 Loss: 3.353218\n",
      "\tTraining batch 701 Loss: 3.344057\n",
      "\tTraining batch 702 Loss: 3.348188\n",
      "\tTraining batch 703 Loss: 3.321518\n",
      "\tTraining batch 704 Loss: 3.334618\n",
      "\tTraining batch 705 Loss: 3.315371\n",
      "\tTraining batch 706 Loss: 3.327897\n",
      "\tTraining batch 707 Loss: 3.341119\n",
      "\tTraining batch 708 Loss: 3.333555\n",
      "\tTraining batch 709 Loss: 3.330864\n",
      "\tTraining batch 710 Loss: 3.344530\n",
      "\tTraining batch 711 Loss: 3.355543\n",
      "\tTraining batch 712 Loss: 3.327213\n",
      "\tTraining batch 713 Loss: 3.336960\n",
      "\tTraining batch 714 Loss: 3.332682\n",
      "\tTraining batch 715 Loss: 3.362773\n",
      "\tTraining batch 716 Loss: 3.319431\n",
      "\tTraining batch 717 Loss: 3.341750\n",
      "\tTraining batch 718 Loss: 3.325337\n",
      "\tTraining batch 719 Loss: 3.337161\n",
      "\tTraining batch 720 Loss: 3.334438\n",
      "\tTraining batch 721 Loss: 3.344350\n",
      "\tTraining batch 722 Loss: 3.332867\n",
      "\tTraining batch 723 Loss: 3.330184\n",
      "\tTraining batch 724 Loss: 3.342835\n",
      "\tTraining batch 725 Loss: 3.347957\n",
      "\tTraining batch 726 Loss: 3.337189\n",
      "\tTraining batch 727 Loss: 3.334853\n",
      "\tTraining batch 728 Loss: 3.326503\n",
      "\tTraining batch 729 Loss: 3.336498\n",
      "\tTraining batch 730 Loss: 3.337094\n",
      "\tTraining batch 731 Loss: 3.340581\n",
      "\tTraining batch 732 Loss: 3.346277\n",
      "\tTraining batch 733 Loss: 3.327317\n",
      "\tTraining batch 734 Loss: 3.342818\n",
      "\tTraining batch 735 Loss: 3.328219\n",
      "\tTraining batch 736 Loss: 3.339660\n",
      "\tTraining batch 737 Loss: 3.318613\n",
      "\tTraining batch 738 Loss: 3.352009\n",
      "\tTraining batch 739 Loss: 3.333537\n",
      "\tTraining batch 740 Loss: 3.324654\n",
      "\tTraining batch 741 Loss: 3.335832\n",
      "\tTraining batch 742 Loss: 3.353860\n",
      "\tTraining batch 743 Loss: 3.322153\n",
      "\tTraining batch 744 Loss: 3.334134\n",
      "\tTraining batch 745 Loss: 3.323763\n",
      "\tTraining batch 746 Loss: 3.327420\n",
      "\tTraining batch 747 Loss: 3.339650\n",
      "\tTraining batch 748 Loss: 3.342294\n",
      "\tTraining batch 749 Loss: 3.343961\n",
      "\tTraining batch 750 Loss: 3.340298\n",
      "\tTraining batch 751 Loss: 3.337784\n",
      "\tTraining batch 752 Loss: 3.334711\n",
      "\tTraining batch 753 Loss: 3.341026\n",
      "\tTraining batch 754 Loss: 3.333972\n",
      "\tTraining batch 755 Loss: 3.331455\n",
      "\tTraining batch 756 Loss: 3.346678\n",
      "\tTraining batch 757 Loss: 3.352479\n",
      "\tTraining batch 758 Loss: 3.327688\n",
      "\tTraining batch 759 Loss: 3.336993\n",
      "\tTraining batch 760 Loss: 3.331859\n",
      "\tTraining batch 761 Loss: 3.340103\n",
      "\tTraining batch 762 Loss: 3.329442\n",
      "\tTraining batch 763 Loss: 3.324096\n",
      "\tTraining batch 764 Loss: 3.335640\n",
      "\tTraining batch 765 Loss: 3.342077\n",
      "\tTraining batch 766 Loss: 3.330991\n",
      "\tTraining batch 767 Loss: 3.349457\n",
      "\tTraining batch 768 Loss: 3.327347\n",
      "\tTraining batch 769 Loss: 3.343237\n",
      "\tTraining batch 770 Loss: 3.319673\n",
      "\tTraining batch 771 Loss: 3.334192\n",
      "\tTraining batch 772 Loss: 3.343977\n",
      "\tTraining batch 773 Loss: 3.345738\n",
      "\tTraining batch 774 Loss: 3.337584\n",
      "\tTraining batch 775 Loss: 3.332426\n",
      "\tTraining batch 776 Loss: 3.331228\n",
      "\tTraining batch 777 Loss: 3.341696\n",
      "\tTraining batch 778 Loss: 3.339035\n",
      "\tTraining batch 779 Loss: 3.342837\n",
      "\tTraining batch 780 Loss: 3.321577\n",
      "\tTraining batch 781 Loss: 3.325436\n",
      "\tTraining batch 782 Loss: 3.330583\n",
      "\tTraining batch 783 Loss: 3.336277\n",
      "\tTraining batch 784 Loss: 3.335693\n",
      "\tTraining batch 785 Loss: 3.330357\n",
      "\tTraining batch 786 Loss: 3.329737\n",
      "\tTraining batch 787 Loss: 3.332503\n",
      "\tTraining batch 788 Loss: 3.349374\n",
      "\tTraining batch 789 Loss: 3.330942\n",
      "\tTraining batch 790 Loss: 3.336745\n",
      "\tTraining batch 791 Loss: 3.326933\n",
      "\tTraining batch 792 Loss: 3.313323\n",
      "\tTraining batch 793 Loss: 3.337896\n",
      "\tTraining batch 794 Loss: 3.333654\n",
      "\tTraining batch 795 Loss: 3.328176\n",
      "\tTraining batch 796 Loss: 3.332661\n",
      "\tTraining batch 797 Loss: 3.340189\n",
      "\tTraining batch 798 Loss: 3.339840\n",
      "\tTraining batch 799 Loss: 3.323370\n",
      "\tTraining batch 800 Loss: 3.333353\n",
      "\tTraining batch 801 Loss: 3.327666\n",
      "\tTraining batch 802 Loss: 3.329190\n",
      "\tTraining batch 803 Loss: 3.340407\n",
      "\tTraining batch 804 Loss: 3.339738\n",
      "\tTraining batch 805 Loss: 3.322148\n",
      "\tTraining batch 806 Loss: 3.332535\n",
      "\tTraining batch 807 Loss: 3.326127\n",
      "\tTraining batch 808 Loss: 3.359297\n",
      "\tTraining batch 809 Loss: 3.346547\n",
      "\tTraining batch 810 Loss: 3.317192\n",
      "\tTraining batch 811 Loss: 3.337514\n",
      "\tTraining batch 812 Loss: 3.333324\n",
      "\tTraining batch 813 Loss: 3.343535\n",
      "\tTraining batch 814 Loss: 3.333635\n",
      "\tTraining batch 815 Loss: 3.336252\n",
      "\tTraining batch 816 Loss: 3.330246\n",
      "\tTraining batch 817 Loss: 3.344038\n",
      "\tTraining batch 818 Loss: 3.348540\n",
      "\tTraining batch 819 Loss: 3.315785\n",
      "\tTraining batch 820 Loss: 3.314676\n",
      "\tTraining batch 821 Loss: 3.344181\n",
      "\tTraining batch 822 Loss: 3.329118\n",
      "\tTraining batch 823 Loss: 3.344445\n",
      "\tTraining batch 824 Loss: 3.309429\n",
      "\tTraining batch 825 Loss: 3.338794\n",
      "\tTraining batch 826 Loss: 3.305797\n",
      "\tTraining batch 827 Loss: 3.337283\n",
      "\tTraining batch 828 Loss: 3.317583\n",
      "\tTraining batch 829 Loss: 3.335712\n",
      "\tTraining batch 830 Loss: 3.345230\n",
      "\tTraining batch 831 Loss: 3.359606\n",
      "\tTraining batch 832 Loss: 3.340077\n",
      "\tTraining batch 833 Loss: 3.329076\n",
      "\tTraining batch 834 Loss: 3.346344\n",
      "\tTraining batch 835 Loss: 3.317534\n",
      "\tTraining batch 836 Loss: 3.343383\n",
      "\tTraining batch 837 Loss: 3.353938\n",
      "\tTraining batch 838 Loss: 3.336636\n",
      "\tTraining batch 839 Loss: 3.339954\n",
      "\tTraining batch 840 Loss: 3.334112\n",
      "\tTraining batch 841 Loss: 3.340393\n",
      "\tTraining batch 842 Loss: 3.308656\n",
      "\tTraining batch 843 Loss: 3.327941\n",
      "\tTraining batch 844 Loss: 3.318040\n",
      "\tTraining batch 845 Loss: 3.343172\n",
      "\tTraining batch 846 Loss: 3.341142\n",
      "\tTraining batch 847 Loss: 3.321632\n",
      "\tTraining batch 848 Loss: 3.334015\n",
      "\tTraining batch 849 Loss: 3.331084\n",
      "\tTraining batch 850 Loss: 3.330650\n",
      "\tTraining batch 851 Loss: 3.341022\n",
      "\tTraining batch 852 Loss: 3.327338\n",
      "\tTraining batch 853 Loss: 3.328347\n",
      "\tTraining batch 854 Loss: 3.361804\n",
      "\tTraining batch 855 Loss: 3.324183\n",
      "\tTraining batch 856 Loss: 3.344817\n",
      "\tTraining batch 857 Loss: 3.320775\n",
      "\tTraining batch 858 Loss: 3.351996\n",
      "\tTraining batch 859 Loss: 3.334084\n",
      "\tTraining batch 860 Loss: 3.330749\n",
      "\tTraining batch 861 Loss: 3.342919\n",
      "\tTraining batch 862 Loss: 3.324272\n",
      "\tTraining batch 863 Loss: 3.337306\n",
      "\tTraining batch 864 Loss: 3.328146\n",
      "\tTraining batch 865 Loss: 3.338773\n",
      "\tTraining batch 866 Loss: 3.340480\n",
      "\tTraining batch 867 Loss: 3.348641\n",
      "\tTraining batch 868 Loss: 3.339242\n",
      "\tTraining batch 869 Loss: 3.335637\n",
      "\tTraining batch 870 Loss: 3.340064\n",
      "\tTraining batch 871 Loss: 3.328024\n",
      "\tTraining batch 872 Loss: 3.337616\n",
      "\tTraining batch 873 Loss: 3.345210\n",
      "\tTraining batch 874 Loss: 3.342913\n",
      "\tTraining batch 875 Loss: 3.337644\n",
      "\tTraining batch 876 Loss: 3.344692\n",
      "\tTraining batch 877 Loss: 3.342123\n",
      "\tTraining batch 878 Loss: 3.331470\n",
      "\tTraining batch 879 Loss: 3.348959\n",
      "\tTraining batch 880 Loss: 3.348591\n",
      "\tTraining batch 881 Loss: 3.339783\n",
      "\tTraining batch 882 Loss: 3.338786\n",
      "\tTraining batch 883 Loss: 3.332118\n",
      "\tTraining batch 884 Loss: 3.331669\n",
      "\tTraining batch 885 Loss: 3.342706\n",
      "\tTraining batch 886 Loss: 3.332318\n",
      "\tTraining batch 887 Loss: 3.336366\n",
      "\tTraining batch 888 Loss: 3.335537\n",
      "\tTraining batch 889 Loss: 3.342388\n",
      "\tTraining batch 890 Loss: 3.327042\n",
      "\tTraining batch 891 Loss: 3.343245\n",
      "\tTraining batch 892 Loss: 3.321114\n",
      "\tTraining batch 893 Loss: 3.329996\n",
      "\tTraining batch 894 Loss: 3.340230\n",
      "\tTraining batch 895 Loss: 3.346045\n",
      "\tTraining batch 896 Loss: 3.343818\n",
      "\tTraining batch 897 Loss: 3.339446\n",
      "\tTraining batch 898 Loss: 3.329100\n",
      "\tTraining batch 899 Loss: 3.327174\n",
      "\tTraining batch 900 Loss: 3.341608\n",
      "\tTraining batch 901 Loss: 3.322731\n",
      "\tTraining batch 902 Loss: 3.322363\n",
      "\tTraining batch 903 Loss: 3.334708\n",
      "\tTraining batch 904 Loss: 3.337037\n",
      "\tTraining batch 905 Loss: 3.322371\n",
      "\tTraining batch 906 Loss: 3.339189\n",
      "\tTraining batch 907 Loss: 3.332525\n",
      "\tTraining batch 908 Loss: 3.335776\n",
      "\tTraining batch 909 Loss: 3.326730\n",
      "\tTraining batch 910 Loss: 3.339859\n",
      "\tTraining batch 911 Loss: 3.325855\n",
      "\tTraining batch 912 Loss: 3.322271\n",
      "\tTraining batch 913 Loss: 3.333709\n",
      "\tTraining batch 914 Loss: 3.307712\n",
      "\tTraining batch 915 Loss: 3.325689\n",
      "\tTraining batch 916 Loss: 3.346546\n",
      "\tTraining batch 917 Loss: 3.340141\n",
      "\tTraining batch 918 Loss: 3.319270\n",
      "\tTraining batch 919 Loss: 3.334275\n",
      "\tTraining batch 920 Loss: 3.341950\n",
      "\tTraining batch 921 Loss: 3.343917\n",
      "\tTraining batch 922 Loss: 3.328909\n",
      "\tTraining batch 923 Loss: 3.341675\n",
      "\tTraining batch 924 Loss: 3.332186\n",
      "\tTraining batch 925 Loss: 3.342201\n",
      "\tTraining batch 926 Loss: 3.331715\n",
      "\tTraining batch 927 Loss: 3.329546\n",
      "\tTraining batch 928 Loss: 3.322269\n",
      "\tTraining batch 929 Loss: 3.320555\n",
      "\tTraining batch 930 Loss: 3.340906\n",
      "\tTraining batch 931 Loss: 3.332086\n",
      "\tTraining batch 932 Loss: 3.314461\n",
      "\tTraining batch 933 Loss: 3.328874\n",
      "\tTraining batch 934 Loss: 3.333823\n",
      "\tTraining batch 935 Loss: 3.318565\n",
      "\tTraining batch 936 Loss: 3.339921\n",
      "\tTraining batch 937 Loss: 3.355992\n",
      "\tTraining batch 938 Loss: 3.337195\n",
      "\tTraining batch 939 Loss: 3.354307\n",
      "\tTraining batch 940 Loss: 3.338098\n",
      "\tTraining batch 941 Loss: 3.319712\n",
      "\tTraining batch 942 Loss: 3.315830\n",
      "\tTraining batch 943 Loss: 3.338531\n",
      "\tTraining batch 944 Loss: 3.326802\n",
      "\tTraining batch 945 Loss: 3.323765\n",
      "\tTraining batch 946 Loss: 3.329253\n",
      "\tTraining batch 947 Loss: 3.335113\n",
      "\tTraining batch 948 Loss: 3.330383\n",
      "\tTraining batch 949 Loss: 3.360160\n",
      "\tTraining batch 950 Loss: 3.319171\n",
      "\tTraining batch 951 Loss: 3.351054\n",
      "\tTraining batch 952 Loss: 3.351589\n",
      "\tTraining batch 953 Loss: 3.349423\n",
      "\tTraining batch 954 Loss: 3.333187\n",
      "\tTraining batch 955 Loss: 3.314111\n",
      "\tTraining batch 956 Loss: 3.330871\n",
      "\tTraining batch 957 Loss: 3.334235\n",
      "\tTraining batch 958 Loss: 3.349111\n",
      "\tTraining batch 959 Loss: 3.346084\n",
      "\tTraining batch 960 Loss: 3.356390\n",
      "\tTraining batch 961 Loss: 3.331953\n",
      "\tTraining batch 962 Loss: 3.310662\n",
      "\tTraining batch 963 Loss: 3.334162\n",
      "\tTraining batch 964 Loss: 3.331278\n",
      "\tTraining batch 965 Loss: 3.312680\n",
      "\tTraining batch 966 Loss: 3.340174\n",
      "\tTraining batch 967 Loss: 3.323239\n",
      "\tTraining batch 968 Loss: 3.329907\n",
      "\tTraining batch 969 Loss: 3.314997\n",
      "\tTraining batch 970 Loss: 3.321872\n",
      "\tTraining batch 971 Loss: 3.346652\n",
      "\tTraining batch 972 Loss: 3.346555\n",
      "\tTraining batch 973 Loss: 3.319791\n",
      "\tTraining batch 974 Loss: 3.343056\n",
      "\tTraining batch 975 Loss: 3.355761\n",
      "\tTraining batch 976 Loss: 3.334628\n",
      "\tTraining batch 977 Loss: 3.331221\n",
      "\tTraining batch 978 Loss: 3.367946\n",
      "\tTraining batch 979 Loss: 3.333644\n",
      "\tTraining batch 980 Loss: 3.325651\n",
      "\tTraining batch 981 Loss: 3.340249\n",
      "\tTraining batch 982 Loss: 3.345620\n",
      "\tTraining batch 983 Loss: 3.332090\n",
      "\tTraining batch 984 Loss: 3.348891\n",
      "\tTraining batch 985 Loss: 3.319425\n",
      "\tTraining batch 986 Loss: 3.335627\n",
      "\tTraining batch 987 Loss: 3.323071\n",
      "\tTraining batch 988 Loss: 3.323740\n",
      "\tTraining batch 989 Loss: 3.337422\n",
      "\tTraining batch 990 Loss: 3.342964\n",
      "\tTraining batch 991 Loss: 3.323561\n",
      "\tTraining batch 992 Loss: 3.320811\n",
      "\tTraining batch 993 Loss: 3.333704\n",
      "\tTraining batch 994 Loss: 3.335237\n",
      "\tTraining batch 995 Loss: 3.326043\n",
      "\tTraining batch 996 Loss: 3.333552\n",
      "\tTraining batch 997 Loss: 3.327857\n",
      "\tTraining batch 998 Loss: 3.335548\n",
      "\tTraining batch 999 Loss: 3.353266\n",
      "\tTraining batch 1000 Loss: 3.341606\n",
      "\tTraining batch 1001 Loss: 3.351121\n",
      "\tTraining batch 1002 Loss: 3.300204\n",
      "\tTraining batch 1003 Loss: 3.353034\n",
      "\tTraining batch 1004 Loss: 3.349876\n",
      "\tTraining batch 1005 Loss: 3.353745\n",
      "\tTraining batch 1006 Loss: 3.329047\n",
      "\tTraining batch 1007 Loss: 3.322229\n",
      "\tTraining batch 1008 Loss: 3.330716\n",
      "\tTraining batch 1009 Loss: 3.339683\n",
      "\tTraining batch 1010 Loss: 3.347008\n",
      "\tTraining batch 1011 Loss: 3.336381\n",
      "\tTraining batch 1012 Loss: 3.327964\n",
      "\tTraining batch 1013 Loss: 3.327426\n",
      "\tTraining batch 1014 Loss: 3.345522\n",
      "\tTraining batch 1015 Loss: 3.353701\n",
      "\tTraining batch 1016 Loss: 3.320554\n",
      "\tTraining batch 1017 Loss: 3.331278\n",
      "\tTraining batch 1018 Loss: 3.345738\n",
      "\tTraining batch 1019 Loss: 3.337485\n",
      "\tTraining batch 1020 Loss: 3.352019\n",
      "\tTraining batch 1021 Loss: 3.349391\n",
      "\tTraining batch 1022 Loss: 3.320322\n",
      "\tTraining batch 1023 Loss: 3.343423\n",
      "\tTraining batch 1024 Loss: 3.314646\n",
      "\tTraining batch 1025 Loss: 3.337778\n",
      "\tTraining batch 1026 Loss: 3.345241\n",
      "\tTraining batch 1027 Loss: 3.343048\n",
      "\tTraining batch 1028 Loss: 3.342873\n",
      "\tTraining batch 1029 Loss: 3.334988\n",
      "\tTraining batch 1030 Loss: 3.325706\n",
      "\tTraining batch 1031 Loss: 3.322904\n",
      "\tTraining batch 1032 Loss: 3.325627\n",
      "\tTraining batch 1033 Loss: 3.340271\n",
      "\tTraining batch 1034 Loss: 3.311660\n",
      "\tTraining batch 1035 Loss: 3.325015\n",
      "\tTraining batch 1036 Loss: 3.320589\n",
      "\tTraining batch 1037 Loss: 3.331080\n",
      "\tTraining batch 1038 Loss: 3.322122\n",
      "\tTraining batch 1039 Loss: 3.341043\n",
      "\tTraining batch 1040 Loss: 3.329556\n",
      "\tTraining batch 1041 Loss: 3.329030\n",
      "\tTraining batch 1042 Loss: 3.336312\n",
      "\tTraining batch 1043 Loss: 3.353626\n",
      "\tTraining batch 1044 Loss: 3.334143\n",
      "\tTraining batch 1045 Loss: 3.317573\n",
      "\tTraining batch 1046 Loss: 3.346847\n",
      "\tTraining batch 1047 Loss: 3.336453\n",
      "\tTraining batch 1048 Loss: 3.339370\n",
      "\tTraining batch 1049 Loss: 3.322581\n",
      "\tTraining batch 1050 Loss: 3.323770\n",
      "\tTraining batch 1051 Loss: 3.339981\n",
      "\tTraining batch 1052 Loss: 3.332182\n",
      "\tTraining batch 1053 Loss: 3.327735\n",
      "\tTraining batch 1054 Loss: 3.321439\n",
      "\tTraining batch 1055 Loss: 3.337630\n",
      "\tTraining batch 1056 Loss: 3.335374\n",
      "\tTraining batch 1057 Loss: 3.332916\n",
      "\tTraining batch 1058 Loss: 3.350914\n",
      "\tTraining batch 1059 Loss: 3.322645\n",
      "\tTraining batch 1060 Loss: 3.336714\n",
      "\tTraining batch 1061 Loss: 3.336790\n",
      "\tTraining batch 1062 Loss: 3.321575\n",
      "\tTraining batch 1063 Loss: 3.321509\n",
      "\tTraining batch 1064 Loss: 3.334717\n",
      "\tTraining batch 1065 Loss: 3.342188\n",
      "\tTraining batch 1066 Loss: 3.347288\n",
      "\tTraining batch 1067 Loss: 3.326468\n",
      "\tTraining batch 1068 Loss: 3.318175\n",
      "\tTraining batch 1069 Loss: 3.334998\n",
      "\tTraining batch 1070 Loss: 3.345926\n",
      "\tTraining batch 1071 Loss: 3.325225\n",
      "\tTraining batch 1072 Loss: 3.327058\n",
      "\tTraining batch 1073 Loss: 3.347796\n",
      "\tTraining batch 1074 Loss: 3.336592\n",
      "\tTraining batch 1075 Loss: 3.338475\n",
      "\tTraining batch 1076 Loss: 3.346602\n",
      "\tTraining batch 1077 Loss: 3.350428\n",
      "\tTraining batch 1078 Loss: 3.359066\n",
      "\tTraining batch 1079 Loss: 3.307311\n",
      "\tTraining batch 1080 Loss: 3.318530\n",
      "\tTraining batch 1081 Loss: 3.338094\n",
      "\tTraining batch 1082 Loss: 3.331286\n",
      "\tTraining batch 1083 Loss: 3.326729\n",
      "\tTraining batch 1084 Loss: 3.345288\n",
      "\tTraining batch 1085 Loss: 3.315491\n",
      "\tTraining batch 1086 Loss: 3.338167\n",
      "\tTraining batch 1087 Loss: 3.349373\n",
      "\tTraining batch 1088 Loss: 3.329124\n",
      "\tTraining batch 1089 Loss: 3.322548\n",
      "\tTraining batch 1090 Loss: 3.351730\n",
      "\tTraining batch 1091 Loss: 3.332354\n",
      "\tTraining batch 1092 Loss: 3.358956\n",
      "\tTraining batch 1093 Loss: 3.345318\n",
      "\tTraining batch 1094 Loss: 3.353612\n",
      "\tTraining batch 1095 Loss: 3.334799\n",
      "\tTraining batch 1096 Loss: 3.331105\n",
      "\tTraining batch 1097 Loss: 3.342890\n",
      "\tTraining batch 1098 Loss: 3.353202\n",
      "\tTraining batch 1099 Loss: 3.318757\n",
      "\tTraining batch 1100 Loss: 3.337483\n",
      "\tTraining batch 1101 Loss: 3.336103\n",
      "\tTraining batch 1102 Loss: 3.340944\n",
      "\tTraining batch 1103 Loss: 3.327272\n",
      "\tTraining batch 1104 Loss: 3.325549\n",
      "\tTraining batch 1105 Loss: 3.328711\n",
      "\tTraining batch 1106 Loss: 3.338395\n",
      "\tTraining batch 1107 Loss: 3.326339\n",
      "\tTraining batch 1108 Loss: 3.338640\n",
      "\tTraining batch 1109 Loss: 3.322489\n",
      "\tTraining batch 1110 Loss: 3.321717\n",
      "\tTraining batch 1111 Loss: 3.326646\n",
      "\tTraining batch 1112 Loss: 3.326195\n",
      "\tTraining batch 1113 Loss: 3.323110\n",
      "\tTraining batch 1114 Loss: 3.322406\n",
      "\tTraining batch 1115 Loss: 3.323221\n",
      "\tTraining batch 1116 Loss: 3.338341\n",
      "\tTraining batch 1117 Loss: 3.318002\n",
      "\tTraining batch 1118 Loss: 3.346712\n",
      "\tTraining batch 1119 Loss: 3.361909\n",
      "\tTraining batch 1120 Loss: 3.350032\n",
      "\tTraining batch 1121 Loss: 3.327239\n",
      "\tTraining batch 1122 Loss: 3.333316\n",
      "\tTraining batch 1123 Loss: 3.342928\n",
      "\tTraining batch 1124 Loss: 3.340706\n",
      "\tTraining batch 1125 Loss: 3.343671\n",
      "\tTraining batch 1126 Loss: 3.331778\n",
      "\tTraining batch 1127 Loss: 3.345190\n",
      "\tTraining batch 1128 Loss: 3.341216\n",
      "\tTraining batch 1129 Loss: 3.338033\n",
      "\tTraining batch 1130 Loss: 3.332420\n",
      "\tTraining batch 1131 Loss: 3.339288\n",
      "\tTraining batch 1132 Loss: 3.326251\n",
      "\tTraining batch 1133 Loss: 3.335740\n",
      "\tTraining batch 1134 Loss: 3.325244\n",
      "\tTraining batch 1135 Loss: 3.339274\n",
      "\tTraining batch 1136 Loss: 3.312495\n",
      "\tTraining batch 1137 Loss: 3.329560\n",
      "\tTraining batch 1138 Loss: 3.329587\n",
      "\tTraining batch 1139 Loss: 3.321542\n",
      "\tTraining batch 1140 Loss: 3.334845\n",
      "\tTraining batch 1141 Loss: 3.325391\n",
      "\tTraining batch 1142 Loss: 3.328076\n",
      "\tTraining batch 1143 Loss: 3.303922\n",
      "\tTraining batch 1144 Loss: 3.320348\n",
      "\tTraining batch 1145 Loss: 3.313554\n",
      "\tTraining batch 1146 Loss: 3.321225\n",
      "\tTraining batch 1147 Loss: 3.328951\n",
      "\tTraining batch 1148 Loss: 3.331969\n",
      "\tTraining batch 1149 Loss: 3.348774\n",
      "\tTraining batch 1150 Loss: 3.325267\n",
      "\tTraining batch 1151 Loss: 3.352276\n",
      "\tTraining batch 1152 Loss: 3.331612\n",
      "\tTraining batch 1153 Loss: 3.368974\n",
      "\tTraining batch 1154 Loss: 3.362701\n",
      "\tTraining batch 1155 Loss: 3.316341\n",
      "\tTraining batch 1156 Loss: 3.336194\n",
      "\tTraining batch 1157 Loss: 3.323984\n",
      "\tTraining batch 1158 Loss: 3.315104\n",
      "\tTraining batch 1159 Loss: 3.355523\n",
      "\tTraining batch 1160 Loss: 3.311294\n",
      "\tTraining batch 1161 Loss: 3.327125\n",
      "\tTraining batch 1162 Loss: 3.345582\n",
      "\tTraining batch 1163 Loss: 3.326180\n",
      "\tTraining batch 1164 Loss: 3.328320\n",
      "\tTraining batch 1165 Loss: 3.316004\n",
      "\tTraining batch 1166 Loss: 3.354066\n",
      "\tTraining batch 1167 Loss: 3.322446\n",
      "\tTraining batch 1168 Loss: 3.341592\n",
      "\tTraining batch 1169 Loss: 3.322925\n",
      "\tTraining batch 1170 Loss: 3.325631\n",
      "\tTraining batch 1171 Loss: 3.317259\n",
      "\tTraining batch 1172 Loss: 3.331800\n",
      "\tTraining batch 1173 Loss: 3.314483\n",
      "\tTraining batch 1174 Loss: 3.329985\n",
      "\tTraining batch 1175 Loss: 3.322054\n",
      "\tTraining batch 1176 Loss: 3.339463\n",
      "\tTraining batch 1177 Loss: 3.344450\n",
      "\tTraining batch 1178 Loss: 3.339354\n",
      "\tTraining batch 1179 Loss: 3.299616\n",
      "\tTraining batch 1180 Loss: 3.348659\n",
      "\tTraining batch 1181 Loss: 3.325284\n",
      "\tTraining batch 1182 Loss: 3.334060\n",
      "\tTraining batch 1183 Loss: 3.355040\n",
      "\tTraining batch 1184 Loss: 3.318037\n",
      "\tTraining batch 1185 Loss: 3.346224\n",
      "\tTraining batch 1186 Loss: 3.345400\n",
      "\tTraining batch 1187 Loss: 3.335390\n",
      "\tTraining batch 1188 Loss: 3.340753\n",
      "\tTraining batch 1189 Loss: 3.349851\n",
      "\tTraining batch 1190 Loss: 3.338771\n",
      "\tTraining batch 1191 Loss: 3.323257\n",
      "\tTraining batch 1192 Loss: 3.348872\n",
      "\tTraining batch 1193 Loss: 3.329523\n",
      "\tTraining batch 1194 Loss: 3.334871\n",
      "\tTraining batch 1195 Loss: 3.336756\n",
      "\tTraining batch 1196 Loss: 3.329310\n",
      "\tTraining batch 1197 Loss: 3.346289\n",
      "\tTraining batch 1198 Loss: 3.354319\n",
      "\tTraining batch 1199 Loss: 3.357514\n",
      "\tTraining batch 1200 Loss: 3.358636\n",
      "\tTraining batch 1201 Loss: 3.336357\n",
      "\tTraining batch 1202 Loss: 3.343982\n",
      "\tTraining batch 1203 Loss: 3.354342\n",
      "\tTraining batch 1204 Loss: 3.340839\n",
      "\tTraining batch 1205 Loss: 3.348637\n",
      "\tTraining batch 1206 Loss: 3.337563\n",
      "\tTraining batch 1207 Loss: 3.358295\n",
      "\tTraining batch 1208 Loss: 3.317217\n",
      "\tTraining batch 1209 Loss: 3.324298\n",
      "\tTraining batch 1210 Loss: 3.355785\n",
      "\tTraining batch 1211 Loss: 3.322223\n",
      "\tTraining batch 1212 Loss: 3.344706\n",
      "\tTraining batch 1213 Loss: 3.338188\n",
      "\tTraining batch 1214 Loss: 3.333739\n",
      "\tTraining batch 1215 Loss: 3.329796\n",
      "\tTraining batch 1216 Loss: 3.331759\n",
      "\tTraining batch 1217 Loss: 3.332147\n",
      "\tTraining batch 1218 Loss: 3.322171\n",
      "\tTraining batch 1219 Loss: 3.365774\n",
      "\tTraining batch 1220 Loss: 3.329672\n",
      "\tTraining batch 1221 Loss: 3.338336\n",
      "\tTraining batch 1222 Loss: 3.330493\n",
      "\tTraining batch 1223 Loss: 3.322517\n",
      "\tTraining batch 1224 Loss: 3.328944\n",
      "\tTraining batch 1225 Loss: 3.334528\n",
      "\tTraining batch 1226 Loss: 3.317572\n",
      "\tTraining batch 1227 Loss: 3.327853\n",
      "\tTraining batch 1228 Loss: 3.340364\n",
      "\tTraining batch 1229 Loss: 3.330500\n",
      "\tTraining batch 1230 Loss: 3.335916\n",
      "\tTraining batch 1231 Loss: 3.341999\n",
      "\tTraining batch 1232 Loss: 3.351050\n",
      "\tTraining batch 1233 Loss: 3.337285\n",
      "\tTraining batch 1234 Loss: 3.329950\n",
      "\tTraining batch 1235 Loss: 3.331073\n",
      "\tTraining batch 1236 Loss: 3.344632\n",
      "\tTraining batch 1237 Loss: 3.342626\n",
      "\tTraining batch 1238 Loss: 3.354669\n",
      "\tTraining batch 1239 Loss: 3.325628\n",
      "\tTraining batch 1240 Loss: 3.343152\n",
      "\tTraining batch 1241 Loss: 3.340507\n",
      "\tTraining batch 1242 Loss: 3.339659\n",
      "\tTraining batch 1243 Loss: 3.317171\n",
      "\tTraining batch 1244 Loss: 3.349174\n",
      "\tTraining batch 1245 Loss: 3.330076\n",
      "\tTraining batch 1246 Loss: 3.329189\n",
      "\tTraining batch 1247 Loss: 3.326690\n",
      "\tTraining batch 1248 Loss: 3.331952\n",
      "\tTraining batch 1249 Loss: 3.339585\n",
      "\tTraining batch 1250 Loss: 3.342965\n",
      "\tTraining batch 1251 Loss: 3.329955\n",
      "\tTraining batch 1252 Loss: 3.337495\n",
      "\tTraining batch 1253 Loss: 3.338372\n",
      "\tTraining batch 1254 Loss: 3.325841\n",
      "\tTraining batch 1255 Loss: 3.321339\n",
      "\tTraining batch 1256 Loss: 3.337760\n",
      "\tTraining batch 1257 Loss: 3.318534\n",
      "\tTraining batch 1258 Loss: 3.344491\n",
      "\tTraining batch 1259 Loss: 3.346597\n",
      "\tTraining batch 1260 Loss: 3.333031\n",
      "\tTraining batch 1261 Loss: 3.331304\n",
      "\tTraining batch 1262 Loss: 3.334196\n",
      "\tTraining batch 1263 Loss: 3.342775\n",
      "\tTraining batch 1264 Loss: 3.338596\n",
      "\tTraining batch 1265 Loss: 3.314356\n",
      "\tTraining batch 1266 Loss: 3.315265\n",
      "\tTraining batch 1267 Loss: 3.329564\n",
      "\tTraining batch 1268 Loss: 3.350354\n",
      "\tTraining batch 1269 Loss: 3.331972\n",
      "\tTraining batch 1270 Loss: 3.354955\n",
      "\tTraining batch 1271 Loss: 3.339005\n",
      "\tTraining batch 1272 Loss: 3.331567\n",
      "\tTraining batch 1273 Loss: 3.322762\n",
      "\tTraining batch 1274 Loss: 3.367675\n",
      "\tTraining batch 1275 Loss: 3.341124\n",
      "\tTraining batch 1276 Loss: 3.333409\n",
      "\tTraining batch 1277 Loss: 3.349271\n",
      "\tTraining batch 1278 Loss: 3.337414\n",
      "\tTraining batch 1279 Loss: 3.329487\n",
      "\tTraining batch 1280 Loss: 3.328015\n",
      "\tTraining batch 1281 Loss: 3.345711\n",
      "\tTraining batch 1282 Loss: 3.340240\n",
      "\tTraining batch 1283 Loss: 3.332651\n",
      "\tTraining batch 1284 Loss: 3.331414\n",
      "\tTraining batch 1285 Loss: 3.349123\n",
      "\tTraining batch 1286 Loss: 3.344716\n",
      "\tTraining batch 1287 Loss: 3.321909\n",
      "\tTraining batch 1288 Loss: 3.339160\n",
      "\tTraining batch 1289 Loss: 3.322231\n",
      "\tTraining batch 1290 Loss: 3.337118\n",
      "\tTraining batch 1291 Loss: 3.340861\n",
      "\tTraining batch 1292 Loss: 3.326039\n",
      "\tTraining batch 1293 Loss: 3.322887\n",
      "\tTraining batch 1294 Loss: 3.352158\n",
      "\tTraining batch 1295 Loss: 3.324974\n",
      "\tTraining batch 1296 Loss: 3.322725\n",
      "\tTraining batch 1297 Loss: 3.339269\n",
      "\tTraining batch 1298 Loss: 3.339904\n",
      "\tTraining batch 1299 Loss: 3.334154\n",
      "\tTraining batch 1300 Loss: 3.324426\n",
      "\tTraining batch 1301 Loss: 3.318815\n",
      "\tTraining batch 1302 Loss: 3.326279\n",
      "\tTraining batch 1303 Loss: 3.330226\n",
      "\tTraining batch 1304 Loss: 3.335403\n",
      "\tTraining batch 1305 Loss: 3.340077\n",
      "\tTraining batch 1306 Loss: 3.342141\n",
      "\tTraining batch 1307 Loss: 3.311428\n",
      "\tTraining batch 1308 Loss: 3.331221\n",
      "\tTraining batch 1309 Loss: 3.332777\n",
      "\tTraining batch 1310 Loss: 3.310609\n",
      "\tTraining batch 1311 Loss: 3.321076\n",
      "\tTraining batch 1312 Loss: 3.336200\n",
      "\tTraining batch 1313 Loss: 3.323899\n",
      "\tTraining batch 1314 Loss: 3.339496\n",
      "\tTraining batch 1315 Loss: 3.325652\n",
      "\tTraining batch 1316 Loss: 3.310324\n",
      "\tTraining batch 1317 Loss: 3.329730\n",
      "\tTraining batch 1318 Loss: 3.360448\n",
      "\tTraining batch 1319 Loss: 3.327323\n",
      "\tTraining batch 1320 Loss: 3.350876\n",
      "\tTraining batch 1321 Loss: 3.360913\n",
      "\tTraining batch 1322 Loss: 3.313008\n",
      "\tTraining batch 1323 Loss: 3.338282\n",
      "\tTraining batch 1324 Loss: 3.343985\n",
      "\tTraining batch 1325 Loss: 3.321305\n",
      "\tTraining batch 1326 Loss: 3.313750\n",
      "\tTraining batch 1327 Loss: 3.308950\n",
      "\tTraining batch 1328 Loss: 3.319643\n",
      "\tTraining batch 1329 Loss: 3.320294\n",
      "\tTraining batch 1330 Loss: 3.336663\n",
      "\tTraining batch 1331 Loss: 3.349880\n",
      "\tTraining batch 1332 Loss: 3.333879\n",
      "\tTraining batch 1333 Loss: 3.338073\n",
      "\tTraining batch 1334 Loss: 3.339216\n",
      "\tTraining batch 1335 Loss: 3.363131\n",
      "\tTraining batch 1336 Loss: 3.337546\n",
      "\tTraining batch 1337 Loss: 3.336887\n",
      "\tTraining batch 1338 Loss: 3.334874\n",
      "\tTraining batch 1339 Loss: 3.351809\n",
      "\tTraining batch 1340 Loss: 3.325981\n",
      "\tTraining batch 1341 Loss: 3.341204\n",
      "\tTraining batch 1342 Loss: 3.339395\n",
      "\tTraining batch 1343 Loss: 3.317847\n",
      "\tTraining batch 1344 Loss: 3.333683\n",
      "\tTraining batch 1345 Loss: 3.330157\n",
      "\tTraining batch 1346 Loss: 3.327853\n",
      "\tTraining batch 1347 Loss: 3.323655\n",
      "\tTraining batch 1348 Loss: 3.324910\n",
      "\tTraining batch 1349 Loss: 3.340866\n",
      "\tTraining batch 1350 Loss: 3.346987\n",
      "\tTraining batch 1351 Loss: 3.317688\n",
      "\tTraining batch 1352 Loss: 3.345634\n",
      "\tTraining batch 1353 Loss: 3.344936\n",
      "\tTraining batch 1354 Loss: 3.353869\n",
      "\tTraining batch 1355 Loss: 3.308942\n",
      "\tTraining batch 1356 Loss: 3.345480\n",
      "\tTraining batch 1357 Loss: 3.349520\n",
      "\tTraining batch 1358 Loss: 3.363226\n",
      "\tTraining batch 1359 Loss: 3.320617\n",
      "\tTraining batch 1360 Loss: 3.342654\n",
      "\tTraining batch 1361 Loss: 3.327927\n",
      "\tTraining batch 1362 Loss: 3.319545\n",
      "\tTraining batch 1363 Loss: 3.320407\n",
      "\tTraining batch 1364 Loss: 3.336738\n",
      "\tTraining batch 1365 Loss: 3.315961\n",
      "\tTraining batch 1366 Loss: 3.339493\n",
      "\tTraining batch 1367 Loss: 3.343619\n",
      "\tTraining batch 1368 Loss: 3.341312\n",
      "\tTraining batch 1369 Loss: 3.364533\n",
      "\tTraining batch 1370 Loss: 3.340235\n",
      "\tTraining batch 1371 Loss: 3.315090\n",
      "\tTraining batch 1372 Loss: 3.348904\n",
      "\tTraining batch 1373 Loss: 3.329259\n",
      "\tTraining batch 1374 Loss: 3.346255\n",
      "\tTraining batch 1375 Loss: 3.353532\n",
      "\tTraining batch 1376 Loss: 3.313675\n",
      "\tTraining batch 1377 Loss: 3.320819\n",
      "\tTraining batch 1378 Loss: 3.311164\n",
      "\tTraining batch 1379 Loss: 3.353880\n",
      "\tTraining batch 1380 Loss: 3.341244\n",
      "\tTraining batch 1381 Loss: 3.317741\n",
      "\tTraining batch 1382 Loss: 3.317891\n",
      "\tTraining batch 1383 Loss: 3.339666\n",
      "\tTraining batch 1384 Loss: 3.324127\n",
      "\tTraining batch 1385 Loss: 3.321158\n",
      "\tTraining batch 1386 Loss: 3.319425\n",
      "\tTraining batch 1387 Loss: 3.335453\n",
      "\tTraining batch 1388 Loss: 3.363112\n",
      "\tTraining batch 1389 Loss: 3.343511\n",
      "\tTraining batch 1390 Loss: 3.351453\n",
      "\tTraining batch 1391 Loss: 3.347345\n",
      "\tTraining batch 1392 Loss: 3.329138\n",
      "\tTraining batch 1393 Loss: 3.318969\n",
      "\tTraining batch 1394 Loss: 3.320769\n",
      "\tTraining batch 1395 Loss: 3.311385\n",
      "\tTraining batch 1396 Loss: 3.336641\n",
      "\tTraining batch 1397 Loss: 3.315210\n",
      "\tTraining batch 1398 Loss: 3.332378\n",
      "\tTraining batch 1399 Loss: 3.318584\n",
      "\tTraining batch 1400 Loss: 3.342116\n",
      "\tTraining batch 1401 Loss: 3.353772\n",
      "\tTraining batch 1402 Loss: 3.347932\n",
      "\tTraining batch 1403 Loss: 3.361121\n",
      "\tTraining batch 1404 Loss: 3.338701\n",
      "\tTraining batch 1405 Loss: 3.338197\n",
      "\tTraining batch 1406 Loss: 3.339403\n",
      "\tTraining batch 1407 Loss: 3.333677\n",
      "\tTraining batch 1408 Loss: 3.324359\n",
      "\tTraining batch 1409 Loss: 3.335518\n",
      "\tTraining batch 1410 Loss: 3.325866\n",
      "\tTraining batch 1411 Loss: 3.346001\n",
      "\tTraining batch 1412 Loss: 3.328451\n",
      "\tTraining batch 1413 Loss: 3.323195\n",
      "\tTraining batch 1414 Loss: 3.350558\n",
      "\tTraining batch 1415 Loss: 3.323935\n",
      "\tTraining batch 1416 Loss: 3.333667\n",
      "\tTraining batch 1417 Loss: 3.355417\n",
      "\tTraining batch 1418 Loss: 3.326998\n",
      "\tTraining batch 1419 Loss: 3.309600\n",
      "\tTraining batch 1420 Loss: 3.340405\n",
      "\tTraining batch 1421 Loss: 3.335200\n",
      "\tTraining batch 1422 Loss: 3.340827\n",
      "\tTraining batch 1423 Loss: 3.344468\n",
      "\tTraining batch 1424 Loss: 3.332042\n",
      "\tTraining batch 1425 Loss: 3.341410\n",
      "\tTraining batch 1426 Loss: 3.339142\n",
      "\tTraining batch 1427 Loss: 3.338569\n",
      "\tTraining batch 1428 Loss: 3.329713\n",
      "\tTraining batch 1429 Loss: 3.359107\n",
      "\tTraining batch 1430 Loss: 3.337970\n",
      "\tTraining batch 1431 Loss: 3.332247\n",
      "\tTraining batch 1432 Loss: 3.332799\n",
      "\tTraining batch 1433 Loss: 3.343287\n",
      "\tTraining batch 1434 Loss: 3.333266\n",
      "\tTraining batch 1435 Loss: 3.334612\n",
      "\tTraining batch 1436 Loss: 3.323326\n",
      "\tTraining batch 1437 Loss: 3.352346\n",
      "\tTraining batch 1438 Loss: 3.324353\n",
      "\tTraining batch 1439 Loss: 3.340867\n",
      "\tTraining batch 1440 Loss: 3.325742\n",
      "\tTraining batch 1441 Loss: 3.337549\n",
      "\tTraining batch 1442 Loss: 3.339561\n",
      "\tTraining batch 1443 Loss: 3.334913\n",
      "\tTraining batch 1444 Loss: 3.340307\n",
      "\tTraining batch 1445 Loss: 3.326484\n",
      "\tTraining batch 1446 Loss: 3.345597\n",
      "\tTraining batch 1447 Loss: 3.337618\n",
      "\tTraining batch 1448 Loss: 3.349077\n",
      "\tTraining batch 1449 Loss: 3.314165\n",
      "\tTraining batch 1450 Loss: 3.337430\n",
      "\tTraining batch 1451 Loss: 3.327925\n",
      "\tTraining batch 1452 Loss: 3.324609\n",
      "\tTraining batch 1453 Loss: 3.334439\n",
      "\tTraining batch 1454 Loss: 3.344509\n",
      "\tTraining batch 1455 Loss: 3.345211\n",
      "\tTraining batch 1456 Loss: 3.327558\n",
      "\tTraining batch 1457 Loss: 3.338694\n",
      "\tTraining batch 1458 Loss: 3.333245\n",
      "\tTraining batch 1459 Loss: 3.328870\n",
      "\tTraining batch 1460 Loss: 3.335283\n",
      "\tTraining batch 1461 Loss: 3.313118\n",
      "\tTraining batch 1462 Loss: 3.334193\n",
      "\tTraining batch 1463 Loss: 3.321172\n",
      "\tTraining batch 1464 Loss: 3.318986\n",
      "\tTraining batch 1465 Loss: 3.314613\n",
      "\tTraining batch 1466 Loss: 3.339535\n",
      "\tTraining batch 1467 Loss: 3.335157\n",
      "\tTraining batch 1468 Loss: 3.328713\n",
      "\tTraining batch 1469 Loss: 3.341719\n",
      "\tTraining batch 1470 Loss: 3.327330\n",
      "\tTraining batch 1471 Loss: 3.345543\n",
      "\tTraining batch 1472 Loss: 3.337624\n",
      "\tTraining batch 1473 Loss: 3.337859\n",
      "\tTraining batch 1474 Loss: 3.316206\n",
      "\tTraining batch 1475 Loss: 3.319992\n",
      "\tTraining batch 1476 Loss: 3.327718\n",
      "\tTraining batch 1477 Loss: 3.327652\n",
      "\tTraining batch 1478 Loss: 3.334867\n",
      "\tTraining batch 1479 Loss: 3.328824\n",
      "\tTraining batch 1480 Loss: 3.334371\n",
      "\tTraining batch 1481 Loss: 3.337682\n",
      "\tTraining batch 1482 Loss: 3.341388\n",
      "\tTraining batch 1483 Loss: 3.337330\n",
      "\tTraining batch 1484 Loss: 3.328017\n",
      "\tTraining batch 1485 Loss: 3.339687\n",
      "\tTraining batch 1486 Loss: 3.352957\n",
      "\tTraining batch 1487 Loss: 3.332597\n",
      "\tTraining batch 1488 Loss: 3.340151\n",
      "\tTraining batch 1489 Loss: 3.333277\n",
      "\tTraining batch 1490 Loss: 3.338753\n",
      "\tTraining batch 1491 Loss: 3.344558\n",
      "\tTraining batch 1492 Loss: 3.340391\n",
      "\tTraining batch 1493 Loss: 3.344414\n",
      "\tTraining batch 1494 Loss: 3.336488\n",
      "\tTraining batch 1495 Loss: 3.339904\n",
      "\tTraining batch 1496 Loss: 3.327803\n",
      "\tTraining batch 1497 Loss: 3.330464\n",
      "\tTraining batch 1498 Loss: 3.357332\n",
      "\tTraining batch 1499 Loss: 3.323922\n",
      "\tTraining batch 1500 Loss: 3.346725\n",
      "\tTraining batch 1501 Loss: 3.345886\n",
      "\tTraining batch 1502 Loss: 3.325859\n",
      "\tTraining batch 1503 Loss: 3.338091\n",
      "\tTraining batch 1504 Loss: 3.326206\n",
      "\tTraining batch 1505 Loss: 3.344151\n",
      "\tTraining batch 1506 Loss: 3.353693\n",
      "\tTraining batch 1507 Loss: 3.335552\n",
      "\tTraining batch 1508 Loss: 3.333857\n",
      "\tTraining batch 1509 Loss: 3.318079\n",
      "\tTraining batch 1510 Loss: 3.303419\n",
      "\tTraining batch 1511 Loss: 3.329430\n",
      "\tTraining batch 1512 Loss: 3.328415\n",
      "\tTraining batch 1513 Loss: 3.331633\n",
      "\tTraining batch 1514 Loss: 3.357484\n",
      "\tTraining batch 1515 Loss: 3.347222\n",
      "\tTraining batch 1516 Loss: 3.330566\n",
      "\tTraining batch 1517 Loss: 3.326686\n",
      "\tTraining batch 1518 Loss: 3.335865\n",
      "\tTraining batch 1519 Loss: 3.334954\n",
      "\tTraining batch 1520 Loss: 3.345403\n",
      "\tTraining batch 1521 Loss: 3.339607\n",
      "\tTraining batch 1522 Loss: 3.345689\n",
      "\tTraining batch 1523 Loss: 3.343685\n",
      "\tTraining batch 1524 Loss: 3.315265\n",
      "\tTraining batch 1525 Loss: 3.333037\n",
      "\tTraining batch 1526 Loss: 3.344610\n",
      "\tTraining batch 1527 Loss: 3.347944\n",
      "\tTraining batch 1528 Loss: 3.326181\n",
      "\tTraining batch 1529 Loss: 3.329026\n",
      "\tTraining batch 1530 Loss: 3.322786\n",
      "\tTraining batch 1531 Loss: 3.340378\n",
      "\tTraining batch 1532 Loss: 3.312484\n",
      "\tTraining batch 1533 Loss: 3.333605\n",
      "\tTraining batch 1534 Loss: 3.325356\n",
      "\tTraining batch 1535 Loss: 3.331337\n",
      "\tTraining batch 1536 Loss: 3.326778\n",
      "\tTraining batch 1537 Loss: 3.353513\n",
      "\tTraining batch 1538 Loss: 3.325310\n",
      "\tTraining batch 1539 Loss: 3.337132\n",
      "\tTraining batch 1540 Loss: 3.331077\n",
      "\tTraining batch 1541 Loss: 3.330618\n",
      "\tTraining batch 1542 Loss: 3.335788\n",
      "\tTraining batch 1543 Loss: 3.344149\n",
      "\tTraining batch 1544 Loss: 3.330142\n",
      "\tTraining batch 1545 Loss: 3.370055\n",
      "\tTraining batch 1546 Loss: 3.320811\n",
      "\tTraining batch 1547 Loss: 3.345094\n",
      "\tTraining batch 1548 Loss: 3.320318\n",
      "\tTraining batch 1549 Loss: 3.326557\n",
      "\tTraining batch 1550 Loss: 3.348804\n",
      "\tTraining batch 1551 Loss: 3.334320\n",
      "\tTraining batch 1552 Loss: 3.344790\n",
      "\tTraining batch 1553 Loss: 3.350949\n",
      "\tTraining batch 1554 Loss: 3.337622\n",
      "\tTraining batch 1555 Loss: 3.347247\n",
      "\tTraining batch 1556 Loss: 3.342763\n",
      "\tTraining batch 1557 Loss: 3.346496\n",
      "\tTraining batch 1558 Loss: 3.350660\n",
      "\tTraining batch 1559 Loss: 3.333880\n",
      "\tTraining batch 1560 Loss: 3.331644\n",
      "\tTraining batch 1561 Loss: 3.345352\n",
      "\tTraining batch 1562 Loss: 3.316146\n",
      "\tTraining batch 1563 Loss: 3.329346\n",
      "\tTraining batch 1564 Loss: 3.334000\n",
      "\tTraining batch 1565 Loss: 3.332095\n",
      "\tTraining batch 1566 Loss: 3.329264\n",
      "\tTraining batch 1567 Loss: 3.329106\n",
      "\tTraining batch 1568 Loss: 3.330549\n",
      "\tTraining batch 1569 Loss: 3.347514\n",
      "\tTraining batch 1570 Loss: 3.361475\n",
      "\tTraining batch 1571 Loss: 3.329159\n",
      "\tTraining batch 1572 Loss: 3.348500\n",
      "\tTraining batch 1573 Loss: 3.342960\n",
      "\tTraining batch 1574 Loss: 3.333462\n",
      "\tTraining batch 1575 Loss: 3.330570\n",
      "\tTraining batch 1576 Loss: 3.331541\n",
      "\tTraining batch 1577 Loss: 3.319933\n",
      "\tTraining batch 1578 Loss: 3.355452\n",
      "\tTraining batch 1579 Loss: 3.335578\n",
      "\tTraining batch 1580 Loss: 3.328864\n",
      "\tTraining batch 1581 Loss: 3.325089\n",
      "\tTraining batch 1582 Loss: 3.346951\n",
      "\tTraining batch 1583 Loss: 3.332388\n",
      "\tTraining batch 1584 Loss: 3.332342\n",
      "\tTraining batch 1585 Loss: 3.342646\n",
      "\tTraining batch 1586 Loss: 3.346955\n",
      "\tTraining batch 1587 Loss: 3.347731\n",
      "\tTraining batch 1588 Loss: 3.326249\n",
      "\tTraining batch 1589 Loss: 3.332552\n",
      "\tTraining batch 1590 Loss: 3.328291\n",
      "\tTraining batch 1591 Loss: 3.331961\n",
      "\tTraining batch 1592 Loss: 3.340099\n",
      "\tTraining batch 1593 Loss: 3.336821\n",
      "\tTraining batch 1594 Loss: 3.320433\n",
      "\tTraining batch 1595 Loss: 3.335097\n",
      "\tTraining batch 1596 Loss: 3.337676\n",
      "\tTraining batch 1597 Loss: 3.331580\n",
      "\tTraining batch 1598 Loss: 3.326798\n",
      "\tTraining batch 1599 Loss: 3.330211\n",
      "\tTraining batch 1600 Loss: 3.340781\n",
      "\tTraining batch 1601 Loss: 3.327270\n",
      "\tTraining batch 1602 Loss: 3.322484\n",
      "\tTraining batch 1603 Loss: 3.331975\n",
      "\tTraining batch 1604 Loss: 3.319834\n",
      "\tTraining batch 1605 Loss: 3.338069\n",
      "\tTraining batch 1606 Loss: 3.321916\n",
      "\tTraining batch 1607 Loss: 3.320533\n",
      "\tTraining batch 1608 Loss: 3.335563\n",
      "\tTraining batch 1609 Loss: 3.332691\n",
      "\tTraining batch 1610 Loss: 3.340654\n",
      "\tTraining batch 1611 Loss: 3.315016\n",
      "\tTraining batch 1612 Loss: 3.337754\n",
      "\tTraining batch 1613 Loss: 3.334550\n",
      "\tTraining batch 1614 Loss: 3.320533\n",
      "\tTraining batch 1615 Loss: 3.346893\n",
      "\tTraining batch 1616 Loss: 3.343652\n",
      "\tTraining batch 1617 Loss: 3.328764\n",
      "\tTraining batch 1618 Loss: 3.319469\n",
      "\tTraining batch 1619 Loss: 3.350113\n",
      "\tTraining batch 1620 Loss: 3.347609\n",
      "\tTraining batch 1621 Loss: 3.325202\n",
      "\tTraining batch 1622 Loss: 3.341547\n",
      "\tTraining batch 1623 Loss: 3.341023\n",
      "\tTraining batch 1624 Loss: 3.342207\n",
      "\tTraining batch 1625 Loss: 3.351892\n",
      "\tTraining batch 1626 Loss: 3.351138\n",
      "\tTraining batch 1627 Loss: 3.353176\n",
      "\tTraining batch 1628 Loss: 3.334613\n",
      "\tTraining batch 1629 Loss: 3.332982\n",
      "\tTraining batch 1630 Loss: 3.344591\n",
      "\tTraining batch 1631 Loss: 3.339870\n",
      "\tTraining batch 1632 Loss: 3.333417\n",
      "\tTraining batch 1633 Loss: 3.340716\n",
      "\tTraining batch 1634 Loss: 3.348375\n",
      "\tTraining batch 1635 Loss: 3.327001\n",
      "\tTraining batch 1636 Loss: 3.329010\n",
      "\tTraining batch 1637 Loss: 3.340493\n",
      "\tTraining batch 1638 Loss: 3.332638\n",
      "\tTraining batch 1639 Loss: 3.330092\n",
      "\tTraining batch 1640 Loss: 3.329233\n",
      "\tTraining batch 1641 Loss: 3.346342\n",
      "\tTraining batch 1642 Loss: 3.348576\n",
      "\tTraining batch 1643 Loss: 3.327176\n",
      "\tTraining batch 1644 Loss: 3.329917\n",
      "\tTraining batch 1645 Loss: 3.340197\n",
      "\tTraining batch 1646 Loss: 3.330817\n",
      "\tTraining batch 1647 Loss: 3.332268\n",
      "\tTraining batch 1648 Loss: 3.325168\n",
      "\tTraining batch 1649 Loss: 3.325397\n",
      "\tTraining batch 1650 Loss: 3.328645\n",
      "\tTraining batch 1651 Loss: 3.324473\n",
      "\tTraining batch 1652 Loss: 3.341246\n",
      "\tTraining batch 1653 Loss: 3.332897\n",
      "\tTraining batch 1654 Loss: 3.349458\n",
      "\tTraining batch 1655 Loss: 3.332817\n",
      "\tTraining batch 1656 Loss: 3.328322\n",
      "\tTraining batch 1657 Loss: 3.325527\n",
      "\tTraining batch 1658 Loss: 3.323050\n",
      "\tTraining batch 1659 Loss: 3.340399\n",
      "\tTraining batch 1660 Loss: 3.332875\n",
      "\tTraining batch 1661 Loss: 3.341042\n",
      "\tTraining batch 1662 Loss: 3.307832\n",
      "\tTraining batch 1663 Loss: 3.339194\n",
      "\tTraining batch 1664 Loss: 3.364433\n",
      "\tTraining batch 1665 Loss: 3.324099\n",
      "\tTraining batch 1666 Loss: 3.332720\n",
      "\tTraining batch 1667 Loss: 3.342264\n",
      "\tTraining batch 1668 Loss: 3.314896\n",
      "\tTraining batch 1669 Loss: 3.335891\n",
      "\tTraining batch 1670 Loss: 3.331414\n",
      "\tTraining batch 1671 Loss: 3.341704\n",
      "\tTraining batch 1672 Loss: 3.356241\n",
      "\tTraining batch 1673 Loss: 3.333565\n",
      "\tTraining batch 1674 Loss: 3.318902\n",
      "\tTraining batch 1675 Loss: 3.332561\n",
      "\tTraining batch 1676 Loss: 3.351200\n",
      "\tTraining batch 1677 Loss: 3.329201\n",
      "\tTraining batch 1678 Loss: 3.329902\n",
      "\tTraining batch 1679 Loss: 3.324178\n",
      "\tTraining batch 1680 Loss: 3.334118\n",
      "\tTraining batch 1681 Loss: 3.321595\n",
      "\tTraining batch 1682 Loss: 3.333851\n",
      "\tTraining batch 1683 Loss: 3.327713\n",
      "\tTraining batch 1684 Loss: 3.334862\n",
      "\tTraining batch 1685 Loss: 3.313319\n",
      "\tTraining batch 1686 Loss: 3.341468\n",
      "\tTraining batch 1687 Loss: 3.352497\n",
      "\tTraining batch 1688 Loss: 3.320952\n",
      "\tTraining batch 1689 Loss: 3.336507\n",
      "\tTraining batch 1690 Loss: 3.339606\n",
      "\tTraining batch 1691 Loss: 3.337230\n",
      "\tTraining batch 1692 Loss: 3.301409\n",
      "\tTraining batch 1693 Loss: 3.329638\n",
      "\tTraining batch 1694 Loss: 3.327466\n",
      "\tTraining batch 1695 Loss: 3.326344\n",
      "\tTraining batch 1696 Loss: 3.337454\n",
      "\tTraining batch 1697 Loss: 3.335552\n",
      "\tTraining batch 1698 Loss: 3.341261\n",
      "\tTraining batch 1699 Loss: 3.332915\n",
      "\tTraining batch 1700 Loss: 3.334538\n",
      "\tTraining batch 1701 Loss: 3.340792\n",
      "\tTraining batch 1702 Loss: 3.363755\n",
      "\tTraining batch 1703 Loss: 3.345701\n",
      "\tTraining batch 1704 Loss: 3.316792\n",
      "\tTraining batch 1705 Loss: 3.338668\n",
      "\tTraining batch 1706 Loss: 3.332720\n",
      "\tTraining batch 1707 Loss: 3.338802\n",
      "\tTraining batch 1708 Loss: 3.336923\n",
      "\tTraining batch 1709 Loss: 3.330340\n",
      "\tTraining batch 1710 Loss: 3.339349\n",
      "\tTraining batch 1711 Loss: 3.326767\n",
      "\tTraining batch 1712 Loss: 3.344299\n",
      "\tTraining batch 1713 Loss: 3.354414\n",
      "\tTraining batch 1714 Loss: 3.322906\n",
      "\tTraining batch 1715 Loss: 3.326852\n",
      "\tTraining batch 1716 Loss: 3.312023\n",
      "\tTraining batch 1717 Loss: 3.332488\n",
      "\tTraining batch 1718 Loss: 3.349064\n",
      "\tTraining batch 1719 Loss: 3.342021\n",
      "\tTraining batch 1720 Loss: 3.341513\n",
      "\tTraining batch 1721 Loss: 3.352647\n",
      "\tTraining batch 1722 Loss: 3.334877\n",
      "\tTraining batch 1723 Loss: 3.332225\n",
      "\tTraining batch 1724 Loss: 3.336933\n",
      "\tTraining batch 1725 Loss: 3.333292\n",
      "\tTraining batch 1726 Loss: 3.314921\n",
      "\tTraining batch 1727 Loss: 3.341156\n",
      "\tTraining batch 1728 Loss: 3.321102\n",
      "\tTraining batch 1729 Loss: 3.324332\n",
      "\tTraining batch 1730 Loss: 3.335225\n",
      "\tTraining batch 1731 Loss: 3.338278\n",
      "\tTraining batch 1732 Loss: 3.321624\n",
      "\tTraining batch 1733 Loss: 3.338318\n",
      "\tTraining batch 1734 Loss: 3.317656\n",
      "\tTraining batch 1735 Loss: 3.334361\n",
      "\tTraining batch 1736 Loss: 3.341972\n",
      "\tTraining batch 1737 Loss: 3.334430\n",
      "\tTraining batch 1738 Loss: 3.347155\n",
      "\tTraining batch 1739 Loss: 3.322227\n",
      "\tTraining batch 1740 Loss: 3.334741\n",
      "\tTraining batch 1741 Loss: 3.333222\n",
      "\tTraining batch 1742 Loss: 3.343539\n",
      "\tTraining batch 1743 Loss: 3.348182\n",
      "\tTraining batch 1744 Loss: 3.313241\n",
      "\tTraining batch 1745 Loss: 3.328291\n",
      "\tTraining batch 1746 Loss: 3.356841\n",
      "\tTraining batch 1747 Loss: 3.343984\n",
      "\tTraining batch 1748 Loss: 3.365053\n",
      "\tTraining batch 1749 Loss: 3.342080\n",
      "\tTraining batch 1750 Loss: 3.335254\n",
      "\tTraining batch 1751 Loss: 3.340707\n",
      "\tTraining batch 1752 Loss: 3.334522\n",
      "\tTraining batch 1753 Loss: 3.324699\n",
      "\tTraining batch 1754 Loss: 3.347578\n",
      "\tTraining batch 1755 Loss: 3.340604\n",
      "\tTraining batch 1756 Loss: 3.337430\n",
      "\tTraining batch 1757 Loss: 3.327895\n",
      "\tTraining batch 1758 Loss: 3.353729\n",
      "\tTraining batch 1759 Loss: 3.323092\n",
      "\tTraining batch 1760 Loss: 3.326600\n",
      "\tTraining batch 1761 Loss: 3.332111\n",
      "\tTraining batch 1762 Loss: 3.351214\n",
      "\tTraining batch 1763 Loss: 3.339800\n",
      "\tTraining batch 1764 Loss: 3.332213\n",
      "\tTraining batch 1765 Loss: 3.353698\n",
      "\tTraining batch 1766 Loss: 3.335571\n",
      "\tTraining batch 1767 Loss: 3.304498\n",
      "\tTraining batch 1768 Loss: 3.317892\n",
      "\tTraining batch 1769 Loss: 3.322204\n",
      "\tTraining batch 1770 Loss: 3.335352\n",
      "\tTraining batch 1771 Loss: 3.344685\n",
      "\tTraining batch 1772 Loss: 3.330473\n",
      "\tTraining batch 1773 Loss: 3.348283\n",
      "\tTraining batch 1774 Loss: 3.343648\n",
      "\tTraining batch 1775 Loss: 3.325057\n",
      "\tTraining batch 1776 Loss: 3.323942\n",
      "\tTraining batch 1777 Loss: 3.348631\n",
      "\tTraining batch 1778 Loss: 3.340301\n",
      "\tTraining batch 1779 Loss: 3.316205\n",
      "\tTraining batch 1780 Loss: 3.350107\n",
      "\tTraining batch 1781 Loss: 3.345441\n",
      "\tTraining batch 1782 Loss: 3.336602\n",
      "\tTraining batch 1783 Loss: 3.340359\n",
      "\tTraining batch 1784 Loss: 3.311792\n",
      "\tTraining batch 1785 Loss: 3.334371\n",
      "\tTraining batch 1786 Loss: 3.340102\n",
      "\tTraining batch 1787 Loss: 3.333078\n",
      "\tTraining batch 1788 Loss: 3.345207\n",
      "\tTraining batch 1789 Loss: 3.320274\n",
      "\tTraining batch 1790 Loss: 3.335143\n",
      "\tTraining batch 1791 Loss: 3.344838\n",
      "\tTraining batch 1792 Loss: 3.331608\n",
      "\tTraining batch 1793 Loss: 3.330508\n",
      "\tTraining batch 1794 Loss: 3.325431\n",
      "\tTraining batch 1795 Loss: 3.328127\n",
      "\tTraining batch 1796 Loss: 3.323001\n",
      "\tTraining batch 1797 Loss: 3.335410\n",
      "\tTraining batch 1798 Loss: 3.338290\n",
      "\tTraining batch 1799 Loss: 3.362108\n",
      "\tTraining batch 1800 Loss: 3.328530\n",
      "\tTraining batch 1801 Loss: 3.335910\n",
      "\tTraining batch 1802 Loss: 3.344256\n",
      "\tTraining batch 1803 Loss: 3.347189\n",
      "\tTraining batch 1804 Loss: 3.338707\n",
      "\tTraining batch 1805 Loss: 3.338179\n",
      "\tTraining batch 1806 Loss: 3.325258\n",
      "\tTraining batch 1807 Loss: 3.337763\n",
      "\tTraining batch 1808 Loss: 3.330574\n",
      "\tTraining batch 1809 Loss: 3.336089\n",
      "\tTraining batch 1810 Loss: 3.345782\n",
      "\tTraining batch 1811 Loss: 3.325512\n",
      "\tTraining batch 1812 Loss: 3.343919\n",
      "\tTraining batch 1813 Loss: 3.345563\n",
      "\tTraining batch 1814 Loss: 3.324461\n",
      "\tTraining batch 1815 Loss: 3.340995\n",
      "\tTraining batch 1816 Loss: 3.356623\n",
      "\tTraining batch 1817 Loss: 3.349027\n",
      "\tTraining batch 1818 Loss: 3.337339\n",
      "\tTraining batch 1819 Loss: 3.320568\n",
      "\tTraining batch 1820 Loss: 3.339247\n",
      "\tTraining batch 1821 Loss: 3.333365\n",
      "\tTraining batch 1822 Loss: 3.331100\n",
      "\tTraining batch 1823 Loss: 3.337365\n",
      "\tTraining batch 1824 Loss: 3.328848\n",
      "\tTraining batch 1825 Loss: 3.331831\n",
      "\tTraining batch 1826 Loss: 3.335791\n",
      "\tTraining batch 1827 Loss: 3.347561\n",
      "\tTraining batch 1828 Loss: 3.326745\n",
      "\tTraining batch 1829 Loss: 3.340080\n",
      "\tTraining batch 1830 Loss: 3.339330\n",
      "\tTraining batch 1831 Loss: 3.319643\n",
      "\tTraining batch 1832 Loss: 3.348798\n",
      "\tTraining batch 1833 Loss: 3.329298\n",
      "\tTraining batch 1834 Loss: 3.343261\n",
      "\tTraining batch 1835 Loss: 3.323260\n",
      "\tTraining batch 1836 Loss: 3.339976\n",
      "\tTraining batch 1837 Loss: 3.322672\n",
      "\tTraining batch 1838 Loss: 3.355968\n",
      "Training set: Average loss: 3.358612\n",
      "Validation set: Average loss: 3.334422, Accuracy: 884/25201 (4%)\n",
      "\n",
      "Epoch:  2\n",
      "\tTraining batch 1 Loss: 3.311612\n",
      "\tTraining batch 2 Loss: 3.341319\n",
      "\tTraining batch 3 Loss: 3.306212\n",
      "\tTraining batch 4 Loss: 3.337112\n",
      "\tTraining batch 5 Loss: 3.350389\n",
      "\tTraining batch 6 Loss: 3.324020\n",
      "\tTraining batch 7 Loss: 3.336789\n",
      "\tTraining batch 8 Loss: 3.330918\n",
      "\tTraining batch 9 Loss: 3.351517\n",
      "\tTraining batch 10 Loss: 3.338046\n",
      "\tTraining batch 11 Loss: 3.326584\n",
      "\tTraining batch 12 Loss: 3.346231\n",
      "\tTraining batch 13 Loss: 3.324207\n",
      "\tTraining batch 14 Loss: 3.345054\n",
      "\tTraining batch 15 Loss: 3.321731\n",
      "\tTraining batch 16 Loss: 3.324668\n",
      "\tTraining batch 17 Loss: 3.332788\n",
      "\tTraining batch 18 Loss: 3.334624\n",
      "\tTraining batch 19 Loss: 3.341996\n",
      "\tTraining batch 20 Loss: 3.344616\n",
      "\tTraining batch 21 Loss: 3.351756\n",
      "\tTraining batch 22 Loss: 3.326744\n",
      "\tTraining batch 23 Loss: 3.340340\n",
      "\tTraining batch 24 Loss: 3.344576\n",
      "\tTraining batch 25 Loss: 3.319502\n",
      "\tTraining batch 26 Loss: 3.338128\n",
      "\tTraining batch 27 Loss: 3.337803\n",
      "\tTraining batch 28 Loss: 3.337945\n",
      "\tTraining batch 29 Loss: 3.321719\n",
      "\tTraining batch 30 Loss: 3.334108\n",
      "\tTraining batch 31 Loss: 3.332607\n",
      "\tTraining batch 32 Loss: 3.342023\n",
      "\tTraining batch 33 Loss: 3.338941\n",
      "\tTraining batch 34 Loss: 3.319940\n",
      "\tTraining batch 35 Loss: 3.365339\n",
      "\tTraining batch 36 Loss: 3.355322\n",
      "\tTraining batch 37 Loss: 3.316042\n",
      "\tTraining batch 38 Loss: 3.319499\n",
      "\tTraining batch 39 Loss: 3.351212\n",
      "\tTraining batch 40 Loss: 3.332980\n",
      "\tTraining batch 41 Loss: 3.347623\n",
      "\tTraining batch 42 Loss: 3.329943\n",
      "\tTraining batch 43 Loss: 3.336469\n",
      "\tTraining batch 44 Loss: 3.313387\n",
      "\tTraining batch 45 Loss: 3.340744\n",
      "\tTraining batch 46 Loss: 3.359908\n",
      "\tTraining batch 47 Loss: 3.333230\n",
      "\tTraining batch 48 Loss: 3.334079\n",
      "\tTraining batch 49 Loss: 3.336710\n",
      "\tTraining batch 50 Loss: 3.327067\n",
      "\tTraining batch 51 Loss: 3.335289\n",
      "\tTraining batch 52 Loss: 3.328039\n",
      "\tTraining batch 53 Loss: 3.342895\n",
      "\tTraining batch 54 Loss: 3.317573\n",
      "\tTraining batch 55 Loss: 3.330003\n",
      "\tTraining batch 56 Loss: 3.338100\n",
      "\tTraining batch 57 Loss: 3.326678\n",
      "\tTraining batch 58 Loss: 3.320951\n",
      "\tTraining batch 59 Loss: 3.328622\n",
      "\tTraining batch 60 Loss: 3.353201\n",
      "\tTraining batch 61 Loss: 3.336818\n",
      "\tTraining batch 62 Loss: 3.320207\n",
      "\tTraining batch 63 Loss: 3.326293\n",
      "\tTraining batch 64 Loss: 3.328271\n",
      "\tTraining batch 65 Loss: 3.346526\n",
      "\tTraining batch 66 Loss: 3.341916\n",
      "\tTraining batch 67 Loss: 3.327495\n",
      "\tTraining batch 68 Loss: 3.341239\n",
      "\tTraining batch 69 Loss: 3.331713\n",
      "\tTraining batch 70 Loss: 3.331028\n",
      "\tTraining batch 71 Loss: 3.329895\n",
      "\tTraining batch 72 Loss: 3.321938\n",
      "\tTraining batch 73 Loss: 3.330948\n",
      "\tTraining batch 74 Loss: 3.317000\n",
      "\tTraining batch 75 Loss: 3.354195\n",
      "\tTraining batch 76 Loss: 3.345613\n",
      "\tTraining batch 77 Loss: 3.344611\n",
      "\tTraining batch 78 Loss: 3.324651\n",
      "\tTraining batch 79 Loss: 3.335388\n",
      "\tTraining batch 80 Loss: 3.342001\n",
      "\tTraining batch 81 Loss: 3.336424\n",
      "\tTraining batch 82 Loss: 3.328475\n",
      "\tTraining batch 83 Loss: 3.349606\n",
      "\tTraining batch 84 Loss: 3.348825\n",
      "\tTraining batch 85 Loss: 3.326213\n",
      "\tTraining batch 86 Loss: 3.350324\n",
      "\tTraining batch 87 Loss: 3.333687\n",
      "\tTraining batch 88 Loss: 3.312651\n",
      "\tTraining batch 89 Loss: 3.312950\n",
      "\tTraining batch 90 Loss: 3.335262\n",
      "\tTraining batch 91 Loss: 3.326008\n",
      "\tTraining batch 92 Loss: 3.341109\n",
      "\tTraining batch 93 Loss: 3.330848\n",
      "\tTraining batch 94 Loss: 3.330656\n",
      "\tTraining batch 95 Loss: 3.329885\n",
      "\tTraining batch 96 Loss: 3.323859\n",
      "\tTraining batch 97 Loss: 3.333317\n",
      "\tTraining batch 98 Loss: 3.315973\n",
      "\tTraining batch 99 Loss: 3.361329\n",
      "\tTraining batch 100 Loss: 3.325471\n",
      "\tTraining batch 101 Loss: 3.333482\n",
      "\tTraining batch 102 Loss: 3.344793\n",
      "\tTraining batch 103 Loss: 3.327663\n",
      "\tTraining batch 104 Loss: 3.328634\n",
      "\tTraining batch 105 Loss: 3.354530\n",
      "\tTraining batch 106 Loss: 3.331114\n",
      "\tTraining batch 107 Loss: 3.330595\n",
      "\tTraining batch 108 Loss: 3.341036\n",
      "\tTraining batch 109 Loss: 3.318232\n",
      "\tTraining batch 110 Loss: 3.350687\n",
      "\tTraining batch 111 Loss: 3.344651\n",
      "\tTraining batch 112 Loss: 3.329297\n",
      "\tTraining batch 113 Loss: 3.322923\n",
      "\tTraining batch 114 Loss: 3.344165\n",
      "\tTraining batch 115 Loss: 3.322511\n",
      "\tTraining batch 116 Loss: 3.343051\n",
      "\tTraining batch 117 Loss: 3.322573\n",
      "\tTraining batch 118 Loss: 3.319627\n",
      "\tTraining batch 119 Loss: 3.324049\n",
      "\tTraining batch 120 Loss: 3.336612\n",
      "\tTraining batch 121 Loss: 3.344692\n",
      "\tTraining batch 122 Loss: 3.330873\n",
      "\tTraining batch 123 Loss: 3.316041\n",
      "\tTraining batch 124 Loss: 3.332973\n",
      "\tTraining batch 125 Loss: 3.342700\n",
      "\tTraining batch 126 Loss: 3.320030\n",
      "\tTraining batch 127 Loss: 3.342203\n",
      "\tTraining batch 128 Loss: 3.298673\n",
      "\tTraining batch 129 Loss: 3.356268\n",
      "\tTraining batch 130 Loss: 3.318424\n",
      "\tTraining batch 131 Loss: 3.323489\n",
      "\tTraining batch 132 Loss: 3.356399\n",
      "\tTraining batch 133 Loss: 3.339552\n",
      "\tTraining batch 134 Loss: 3.346755\n",
      "\tTraining batch 135 Loss: 3.334334\n",
      "\tTraining batch 136 Loss: 3.360024\n",
      "\tTraining batch 137 Loss: 3.330857\n",
      "\tTraining batch 138 Loss: 3.329272\n",
      "\tTraining batch 139 Loss: 3.336462\n",
      "\tTraining batch 140 Loss: 3.323910\n",
      "\tTraining batch 141 Loss: 3.326867\n",
      "\tTraining batch 142 Loss: 3.337029\n",
      "\tTraining batch 143 Loss: 3.338715\n",
      "\tTraining batch 144 Loss: 3.340091\n",
      "\tTraining batch 145 Loss: 3.338146\n",
      "\tTraining batch 146 Loss: 3.311533\n",
      "\tTraining batch 147 Loss: 3.358945\n",
      "\tTraining batch 148 Loss: 3.335137\n",
      "\tTraining batch 149 Loss: 3.329614\n",
      "\tTraining batch 150 Loss: 3.330491\n",
      "\tTraining batch 151 Loss: 3.325126\n",
      "\tTraining batch 152 Loss: 3.311882\n",
      "\tTraining batch 153 Loss: 3.334589\n",
      "\tTraining batch 154 Loss: 3.318295\n",
      "\tTraining batch 155 Loss: 3.343282\n",
      "\tTraining batch 156 Loss: 3.328597\n",
      "\tTraining batch 157 Loss: 3.320899\n",
      "\tTraining batch 158 Loss: 3.343228\n",
      "\tTraining batch 159 Loss: 3.317147\n",
      "\tTraining batch 160 Loss: 3.336539\n",
      "\tTraining batch 161 Loss: 3.317410\n",
      "\tTraining batch 162 Loss: 3.322259\n",
      "\tTraining batch 163 Loss: 3.321753\n",
      "\tTraining batch 164 Loss: 3.317835\n",
      "\tTraining batch 165 Loss: 3.327302\n",
      "\tTraining batch 166 Loss: 3.323418\n",
      "\tTraining batch 167 Loss: 3.341261\n",
      "\tTraining batch 168 Loss: 3.359598\n",
      "\tTraining batch 169 Loss: 3.352956\n",
      "\tTraining batch 170 Loss: 3.330839\n",
      "\tTraining batch 171 Loss: 3.326472\n",
      "\tTraining batch 172 Loss: 3.328361\n",
      "\tTraining batch 173 Loss: 3.348566\n",
      "\tTraining batch 174 Loss: 3.347706\n",
      "\tTraining batch 175 Loss: 3.337488\n",
      "\tTraining batch 176 Loss: 3.352559\n",
      "\tTraining batch 177 Loss: 3.351944\n",
      "\tTraining batch 178 Loss: 3.334737\n",
      "\tTraining batch 179 Loss: 3.319460\n",
      "\tTraining batch 180 Loss: 3.334952\n",
      "\tTraining batch 181 Loss: 3.336394\n",
      "\tTraining batch 182 Loss: 3.331239\n",
      "\tTraining batch 183 Loss: 3.346560\n",
      "\tTraining batch 184 Loss: 3.324454\n",
      "\tTraining batch 185 Loss: 3.357358\n",
      "\tTraining batch 186 Loss: 3.338524\n",
      "\tTraining batch 187 Loss: 3.331996\n",
      "\tTraining batch 188 Loss: 3.348106\n",
      "\tTraining batch 189 Loss: 3.325601\n",
      "\tTraining batch 190 Loss: 3.317936\n",
      "\tTraining batch 191 Loss: 3.311239\n",
      "\tTraining batch 192 Loss: 3.334436\n",
      "\tTraining batch 193 Loss: 3.338949\n",
      "\tTraining batch 194 Loss: 3.330007\n",
      "\tTraining batch 195 Loss: 3.339045\n",
      "\tTraining batch 196 Loss: 3.318960\n",
      "\tTraining batch 197 Loss: 3.317774\n",
      "\tTraining batch 198 Loss: 3.343954\n",
      "\tTraining batch 199 Loss: 3.326755\n",
      "\tTraining batch 200 Loss: 3.319165\n",
      "\tTraining batch 201 Loss: 3.341282\n",
      "\tTraining batch 202 Loss: 3.331055\n",
      "\tTraining batch 203 Loss: 3.323028\n",
      "\tTraining batch 204 Loss: 3.325985\n",
      "\tTraining batch 205 Loss: 3.351909\n",
      "\tTraining batch 206 Loss: 3.340211\n",
      "\tTraining batch 207 Loss: 3.323732\n",
      "\tTraining batch 208 Loss: 3.339147\n",
      "\tTraining batch 209 Loss: 3.354313\n",
      "\tTraining batch 210 Loss: 3.334175\n",
      "\tTraining batch 211 Loss: 3.333693\n",
      "\tTraining batch 212 Loss: 3.336115\n",
      "\tTraining batch 213 Loss: 3.322132\n",
      "\tTraining batch 214 Loss: 3.331095\n",
      "\tTraining batch 215 Loss: 3.333052\n",
      "\tTraining batch 216 Loss: 3.340579\n",
      "\tTraining batch 217 Loss: 3.337141\n",
      "\tTraining batch 218 Loss: 3.335765\n",
      "\tTraining batch 219 Loss: 3.344594\n",
      "\tTraining batch 220 Loss: 3.349723\n",
      "\tTraining batch 221 Loss: 3.327933\n",
      "\tTraining batch 222 Loss: 3.339165\n",
      "\tTraining batch 223 Loss: 3.325533\n",
      "\tTraining batch 224 Loss: 3.352047\n",
      "\tTraining batch 225 Loss: 3.324444\n",
      "\tTraining batch 226 Loss: 3.344374\n",
      "\tTraining batch 227 Loss: 3.343343\n",
      "\tTraining batch 228 Loss: 3.357296\n",
      "\tTraining batch 229 Loss: 3.325257\n",
      "\tTraining batch 230 Loss: 3.348014\n",
      "\tTraining batch 231 Loss: 3.325469\n",
      "\tTraining batch 232 Loss: 3.350843\n",
      "\tTraining batch 233 Loss: 3.339708\n",
      "\tTraining batch 234 Loss: 3.344174\n",
      "\tTraining batch 235 Loss: 3.337067\n",
      "\tTraining batch 236 Loss: 3.324425\n",
      "\tTraining batch 237 Loss: 3.337372\n",
      "\tTraining batch 238 Loss: 3.319924\n",
      "\tTraining batch 239 Loss: 3.338741\n",
      "\tTraining batch 240 Loss: 3.321227\n",
      "\tTraining batch 241 Loss: 3.322078\n",
      "\tTraining batch 242 Loss: 3.310407\n",
      "\tTraining batch 243 Loss: 3.311998\n",
      "\tTraining batch 244 Loss: 3.341954\n",
      "\tTraining batch 245 Loss: 3.338542\n",
      "\tTraining batch 246 Loss: 3.316834\n",
      "\tTraining batch 247 Loss: 3.323950\n",
      "\tTraining batch 248 Loss: 3.332117\n",
      "\tTraining batch 249 Loss: 3.347576\n",
      "\tTraining batch 250 Loss: 3.331001\n",
      "\tTraining batch 251 Loss: 3.357568\n",
      "\tTraining batch 252 Loss: 3.327115\n",
      "\tTraining batch 253 Loss: 3.353162\n",
      "\tTraining batch 254 Loss: 3.340574\n",
      "\tTraining batch 255 Loss: 3.351551\n",
      "\tTraining batch 256 Loss: 3.328061\n",
      "\tTraining batch 257 Loss: 3.344889\n",
      "\tTraining batch 258 Loss: 3.358297\n",
      "\tTraining batch 259 Loss: 3.318299\n",
      "\tTraining batch 260 Loss: 3.340074\n",
      "\tTraining batch 261 Loss: 3.327378\n",
      "\tTraining batch 262 Loss: 3.342785\n",
      "\tTraining batch 263 Loss: 3.346042\n",
      "\tTraining batch 264 Loss: 3.348164\n",
      "\tTraining batch 265 Loss: 3.335499\n",
      "\tTraining batch 266 Loss: 3.341730\n",
      "\tTraining batch 267 Loss: 3.329601\n",
      "\tTraining batch 268 Loss: 3.352829\n",
      "\tTraining batch 269 Loss: 3.355896\n",
      "\tTraining batch 270 Loss: 3.318445\n",
      "\tTraining batch 271 Loss: 3.320956\n",
      "\tTraining batch 272 Loss: 3.318434\n",
      "\tTraining batch 273 Loss: 3.327242\n",
      "\tTraining batch 274 Loss: 3.321415\n",
      "\tTraining batch 275 Loss: 3.330285\n",
      "\tTraining batch 276 Loss: 3.345803\n",
      "\tTraining batch 277 Loss: 3.331586\n",
      "\tTraining batch 278 Loss: 3.353019\n",
      "\tTraining batch 279 Loss: 3.325853\n",
      "\tTraining batch 280 Loss: 3.341450\n",
      "\tTraining batch 281 Loss: 3.330154\n",
      "\tTraining batch 282 Loss: 3.339150\n",
      "\tTraining batch 283 Loss: 3.342825\n",
      "\tTraining batch 284 Loss: 3.329594\n",
      "\tTraining batch 285 Loss: 3.330792\n",
      "\tTraining batch 286 Loss: 3.344230\n",
      "\tTraining batch 287 Loss: 3.342495\n",
      "\tTraining batch 288 Loss: 3.323405\n",
      "\tTraining batch 289 Loss: 3.348254\n",
      "\tTraining batch 290 Loss: 3.337555\n",
      "\tTraining batch 291 Loss: 3.326815\n",
      "\tTraining batch 292 Loss: 3.330274\n",
      "\tTraining batch 293 Loss: 3.353127\n",
      "\tTraining batch 294 Loss: 3.338419\n",
      "\tTraining batch 295 Loss: 3.323431\n",
      "\tTraining batch 296 Loss: 3.332930\n",
      "\tTraining batch 297 Loss: 3.334115\n",
      "\tTraining batch 298 Loss: 3.353984\n",
      "\tTraining batch 299 Loss: 3.336632\n",
      "\tTraining batch 300 Loss: 3.317311\n",
      "\tTraining batch 301 Loss: 3.336525\n",
      "\tTraining batch 302 Loss: 3.329451\n",
      "\tTraining batch 303 Loss: 3.330604\n",
      "\tTraining batch 304 Loss: 3.330853\n",
      "\tTraining batch 305 Loss: 3.330162\n",
      "\tTraining batch 306 Loss: 3.340292\n",
      "\tTraining batch 307 Loss: 3.329694\n",
      "\tTraining batch 308 Loss: 3.338501\n",
      "\tTraining batch 309 Loss: 3.331891\n",
      "\tTraining batch 310 Loss: 3.323369\n",
      "\tTraining batch 311 Loss: 3.331205\n",
      "\tTraining batch 312 Loss: 3.324873\n",
      "\tTraining batch 313 Loss: 3.342948\n",
      "\tTraining batch 314 Loss: 3.330451\n",
      "\tTraining batch 315 Loss: 3.330467\n",
      "\tTraining batch 316 Loss: 3.338830\n",
      "\tTraining batch 317 Loss: 3.328758\n",
      "\tTraining batch 318 Loss: 3.320449\n",
      "\tTraining batch 319 Loss: 3.337879\n",
      "\tTraining batch 320 Loss: 3.331145\n",
      "\tTraining batch 321 Loss: 3.329964\n",
      "\tTraining batch 322 Loss: 3.316653\n",
      "\tTraining batch 323 Loss: 3.320009\n",
      "\tTraining batch 324 Loss: 3.318996\n",
      "\tTraining batch 325 Loss: 3.335510\n",
      "\tTraining batch 326 Loss: 3.325586\n",
      "\tTraining batch 327 Loss: 3.338589\n",
      "\tTraining batch 328 Loss: 3.333552\n",
      "\tTraining batch 329 Loss: 3.341708\n",
      "\tTraining batch 330 Loss: 3.360819\n",
      "\tTraining batch 331 Loss: 3.313966\n",
      "\tTraining batch 332 Loss: 3.330433\n",
      "\tTraining batch 333 Loss: 3.332973\n",
      "\tTraining batch 334 Loss: 3.330568\n",
      "\tTraining batch 335 Loss: 3.323805\n",
      "\tTraining batch 336 Loss: 3.354308\n",
      "\tTraining batch 337 Loss: 3.332372\n",
      "\tTraining batch 338 Loss: 3.340683\n",
      "\tTraining batch 339 Loss: 3.353954\n",
      "\tTraining batch 340 Loss: 3.336753\n",
      "\tTraining batch 341 Loss: 3.347325\n",
      "\tTraining batch 342 Loss: 3.350979\n",
      "\tTraining batch 343 Loss: 3.334302\n",
      "\tTraining batch 344 Loss: 3.337547\n",
      "\tTraining batch 345 Loss: 3.329005\n",
      "\tTraining batch 346 Loss: 3.327509\n",
      "\tTraining batch 347 Loss: 3.315320\n",
      "\tTraining batch 348 Loss: 3.338520\n",
      "\tTraining batch 349 Loss: 3.336130\n",
      "\tTraining batch 350 Loss: 3.353932\n",
      "\tTraining batch 351 Loss: 3.312294\n",
      "\tTraining batch 352 Loss: 3.345967\n",
      "\tTraining batch 353 Loss: 3.339170\n",
      "\tTraining batch 354 Loss: 3.340554\n",
      "\tTraining batch 355 Loss: 3.324302\n",
      "\tTraining batch 356 Loss: 3.323896\n",
      "\tTraining batch 357 Loss: 3.330992\n",
      "\tTraining batch 358 Loss: 3.321930\n",
      "\tTraining batch 359 Loss: 3.323930\n",
      "\tTraining batch 360 Loss: 3.346707\n",
      "\tTraining batch 361 Loss: 3.329134\n",
      "\tTraining batch 362 Loss: 3.334850\n",
      "\tTraining batch 363 Loss: 3.327648\n",
      "\tTraining batch 364 Loss: 3.338358\n",
      "\tTraining batch 365 Loss: 3.317623\n",
      "\tTraining batch 366 Loss: 3.320780\n",
      "\tTraining batch 367 Loss: 3.322737\n",
      "\tTraining batch 368 Loss: 3.320822\n",
      "\tTraining batch 369 Loss: 3.332892\n",
      "\tTraining batch 370 Loss: 3.326757\n",
      "\tTraining batch 371 Loss: 3.336078\n",
      "\tTraining batch 372 Loss: 3.340539\n",
      "\tTraining batch 373 Loss: 3.336227\n",
      "\tTraining batch 374 Loss: 3.314958\n",
      "\tTraining batch 375 Loss: 3.322420\n",
      "\tTraining batch 376 Loss: 3.330329\n",
      "\tTraining batch 377 Loss: 3.325811\n",
      "\tTraining batch 378 Loss: 3.329571\n",
      "\tTraining batch 379 Loss: 3.328448\n",
      "\tTraining batch 380 Loss: 3.320787\n",
      "\tTraining batch 381 Loss: 3.343004\n",
      "\tTraining batch 382 Loss: 3.321916\n",
      "\tTraining batch 383 Loss: 3.323515\n",
      "\tTraining batch 384 Loss: 3.362387\n",
      "\tTraining batch 385 Loss: 3.340112\n",
      "\tTraining batch 386 Loss: 3.343992\n",
      "\tTraining batch 387 Loss: 3.348283\n",
      "\tTraining batch 388 Loss: 3.330907\n",
      "\tTraining batch 389 Loss: 3.361933\n",
      "\tTraining batch 390 Loss: 3.327857\n",
      "\tTraining batch 391 Loss: 3.345448\n",
      "\tTraining batch 392 Loss: 3.336231\n",
      "\tTraining batch 393 Loss: 3.334118\n",
      "\tTraining batch 394 Loss: 3.338438\n",
      "\tTraining batch 395 Loss: 3.344895\n",
      "\tTraining batch 396 Loss: 3.343149\n",
      "\tTraining batch 397 Loss: 3.339182\n",
      "\tTraining batch 398 Loss: 3.349804\n",
      "\tTraining batch 399 Loss: 3.336900\n",
      "\tTraining batch 400 Loss: 3.352146\n",
      "\tTraining batch 401 Loss: 3.320110\n",
      "\tTraining batch 402 Loss: 3.349961\n",
      "\tTraining batch 403 Loss: 3.329338\n",
      "\tTraining batch 404 Loss: 3.364501\n",
      "\tTraining batch 405 Loss: 3.339525\n",
      "\tTraining batch 406 Loss: 3.333347\n",
      "\tTraining batch 407 Loss: 3.338448\n",
      "\tTraining batch 408 Loss: 3.336725\n",
      "\tTraining batch 409 Loss: 3.332279\n",
      "\tTraining batch 410 Loss: 3.338180\n",
      "\tTraining batch 411 Loss: 3.311966\n",
      "\tTraining batch 412 Loss: 3.328434\n",
      "\tTraining batch 413 Loss: 3.329696\n",
      "\tTraining batch 414 Loss: 3.345496\n",
      "\tTraining batch 415 Loss: 3.332094\n",
      "\tTraining batch 416 Loss: 3.313140\n",
      "\tTraining batch 417 Loss: 3.319245\n",
      "\tTraining batch 418 Loss: 3.321003\n",
      "\tTraining batch 419 Loss: 3.325259\n",
      "\tTraining batch 420 Loss: 3.338110\n",
      "\tTraining batch 421 Loss: 3.359655\n",
      "\tTraining batch 422 Loss: 3.349268\n",
      "\tTraining batch 423 Loss: 3.306125\n",
      "\tTraining batch 424 Loss: 3.327043\n",
      "\tTraining batch 425 Loss: 3.338784\n",
      "\tTraining batch 426 Loss: 3.328327\n",
      "\tTraining batch 427 Loss: 3.309313\n",
      "\tTraining batch 428 Loss: 3.345782\n",
      "\tTraining batch 429 Loss: 3.333479\n",
      "\tTraining batch 430 Loss: 3.350753\n",
      "\tTraining batch 431 Loss: 3.323934\n",
      "\tTraining batch 432 Loss: 3.342791\n",
      "\tTraining batch 433 Loss: 3.326824\n",
      "\tTraining batch 434 Loss: 3.335666\n",
      "\tTraining batch 435 Loss: 3.322703\n",
      "\tTraining batch 436 Loss: 3.340574\n",
      "\tTraining batch 437 Loss: 3.353268\n",
      "\tTraining batch 438 Loss: 3.341971\n",
      "\tTraining batch 439 Loss: 3.337699\n",
      "\tTraining batch 440 Loss: 3.330809\n",
      "\tTraining batch 441 Loss: 3.345855\n",
      "\tTraining batch 442 Loss: 3.329463\n",
      "\tTraining batch 443 Loss: 3.315519\n",
      "\tTraining batch 444 Loss: 3.320771\n",
      "\tTraining batch 445 Loss: 3.322342\n",
      "\tTraining batch 446 Loss: 3.342550\n",
      "\tTraining batch 447 Loss: 3.342273\n",
      "\tTraining batch 448 Loss: 3.348398\n",
      "\tTraining batch 449 Loss: 3.312587\n",
      "\tTraining batch 450 Loss: 3.339961\n",
      "\tTraining batch 451 Loss: 3.323068\n",
      "\tTraining batch 452 Loss: 3.319881\n",
      "\tTraining batch 453 Loss: 3.333684\n",
      "\tTraining batch 454 Loss: 3.345452\n",
      "\tTraining batch 455 Loss: 3.333165\n",
      "\tTraining batch 456 Loss: 3.342696\n",
      "\tTraining batch 457 Loss: 3.308905\n",
      "\tTraining batch 458 Loss: 3.322182\n",
      "\tTraining batch 459 Loss: 3.326942\n",
      "\tTraining batch 460 Loss: 3.340594\n",
      "\tTraining batch 461 Loss: 3.334398\n",
      "\tTraining batch 462 Loss: 3.338903\n",
      "\tTraining batch 463 Loss: 3.305477\n",
      "\tTraining batch 464 Loss: 3.335357\n",
      "\tTraining batch 465 Loss: 3.336575\n",
      "\tTraining batch 466 Loss: 3.334744\n",
      "\tTraining batch 467 Loss: 3.339684\n",
      "\tTraining batch 468 Loss: 3.344949\n",
      "\tTraining batch 469 Loss: 3.328899\n",
      "\tTraining batch 470 Loss: 3.308344\n",
      "\tTraining batch 471 Loss: 3.352997\n",
      "\tTraining batch 472 Loss: 3.337260\n",
      "\tTraining batch 473 Loss: 3.332315\n",
      "\tTraining batch 474 Loss: 3.331809\n",
      "\tTraining batch 475 Loss: 3.352749\n",
      "\tTraining batch 476 Loss: 3.333266\n",
      "\tTraining batch 477 Loss: 3.334670\n",
      "\tTraining batch 478 Loss: 3.349788\n",
      "\tTraining batch 479 Loss: 3.332780\n",
      "\tTraining batch 480 Loss: 3.331866\n",
      "\tTraining batch 481 Loss: 3.344607\n",
      "\tTraining batch 482 Loss: 3.335163\n",
      "\tTraining batch 483 Loss: 3.315618\n",
      "\tTraining batch 484 Loss: 3.325234\n",
      "\tTraining batch 485 Loss: 3.344277\n",
      "\tTraining batch 486 Loss: 3.369318\n",
      "\tTraining batch 487 Loss: 3.357268\n",
      "\tTraining batch 488 Loss: 3.322222\n",
      "\tTraining batch 489 Loss: 3.335746\n",
      "\tTraining batch 490 Loss: 3.343854\n",
      "\tTraining batch 491 Loss: 3.315856\n",
      "\tTraining batch 492 Loss: 3.318415\n",
      "\tTraining batch 493 Loss: 3.317255\n",
      "\tTraining batch 494 Loss: 3.339721\n",
      "\tTraining batch 495 Loss: 3.342024\n",
      "\tTraining batch 496 Loss: 3.359543\n",
      "\tTraining batch 497 Loss: 3.331267\n",
      "\tTraining batch 498 Loss: 3.337311\n",
      "\tTraining batch 499 Loss: 3.356376\n",
      "\tTraining batch 500 Loss: 3.344813\n",
      "\tTraining batch 501 Loss: 3.345955\n",
      "\tTraining batch 502 Loss: 3.332209\n",
      "\tTraining batch 503 Loss: 3.337175\n",
      "\tTraining batch 504 Loss: 3.331865\n",
      "\tTraining batch 505 Loss: 3.323532\n",
      "\tTraining batch 506 Loss: 3.345354\n",
      "\tTraining batch 507 Loss: 3.344443\n",
      "\tTraining batch 508 Loss: 3.339196\n",
      "\tTraining batch 509 Loss: 3.321571\n",
      "\tTraining batch 510 Loss: 3.353423\n",
      "\tTraining batch 511 Loss: 3.339399\n",
      "\tTraining batch 512 Loss: 3.321349\n",
      "\tTraining batch 513 Loss: 3.352500\n",
      "\tTraining batch 514 Loss: 3.339102\n",
      "\tTraining batch 515 Loss: 3.326952\n",
      "\tTraining batch 516 Loss: 3.329439\n",
      "\tTraining batch 517 Loss: 3.322440\n",
      "\tTraining batch 518 Loss: 3.341692\n",
      "\tTraining batch 519 Loss: 3.335962\n",
      "\tTraining batch 520 Loss: 3.332007\n",
      "\tTraining batch 521 Loss: 3.330767\n",
      "\tTraining batch 522 Loss: 3.334017\n",
      "\tTraining batch 523 Loss: 3.334155\n",
      "\tTraining batch 524 Loss: 3.339065\n",
      "\tTraining batch 525 Loss: 3.341959\n",
      "\tTraining batch 526 Loss: 3.339132\n",
      "\tTraining batch 527 Loss: 3.336654\n",
      "\tTraining batch 528 Loss: 3.339909\n",
      "\tTraining batch 529 Loss: 3.337273\n",
      "\tTraining batch 530 Loss: 3.340994\n",
      "\tTraining batch 531 Loss: 3.336792\n",
      "\tTraining batch 532 Loss: 3.337058\n",
      "\tTraining batch 533 Loss: 3.348104\n",
      "\tTraining batch 534 Loss: 3.331621\n",
      "\tTraining batch 535 Loss: 3.327260\n",
      "\tTraining batch 536 Loss: 3.340946\n",
      "\tTraining batch 537 Loss: 3.331760\n",
      "\tTraining batch 538 Loss: 3.310888\n",
      "\tTraining batch 539 Loss: 3.341047\n",
      "\tTraining batch 540 Loss: 3.336543\n",
      "\tTraining batch 541 Loss: 3.357325\n",
      "\tTraining batch 542 Loss: 3.343305\n",
      "\tTraining batch 543 Loss: 3.339776\n",
      "\tTraining batch 544 Loss: 3.329421\n",
      "\tTraining batch 545 Loss: 3.352805\n",
      "\tTraining batch 546 Loss: 3.328626\n",
      "\tTraining batch 547 Loss: 3.333349\n",
      "\tTraining batch 548 Loss: 3.330153\n",
      "\tTraining batch 549 Loss: 3.338704\n",
      "\tTraining batch 550 Loss: 3.340577\n",
      "\tTraining batch 551 Loss: 3.330582\n",
      "\tTraining batch 552 Loss: 3.347551\n",
      "\tTraining batch 553 Loss: 3.323866\n",
      "\tTraining batch 554 Loss: 3.345963\n",
      "\tTraining batch 555 Loss: 3.330936\n",
      "\tTraining batch 556 Loss: 3.339837\n",
      "\tTraining batch 557 Loss: 3.337792\n",
      "\tTraining batch 558 Loss: 3.337177\n",
      "\tTraining batch 559 Loss: 3.325759\n",
      "\tTraining batch 560 Loss: 3.330786\n",
      "\tTraining batch 561 Loss: 3.314921\n",
      "\tTraining batch 562 Loss: 3.331387\n",
      "\tTraining batch 563 Loss: 3.327998\n",
      "\tTraining batch 564 Loss: 3.346178\n",
      "\tTraining batch 565 Loss: 3.324863\n",
      "\tTraining batch 566 Loss: 3.333301\n",
      "\tTraining batch 567 Loss: 3.305979\n",
      "\tTraining batch 568 Loss: 3.338330\n",
      "\tTraining batch 569 Loss: 3.329191\n",
      "\tTraining batch 570 Loss: 3.312957\n",
      "\tTraining batch 571 Loss: 3.319934\n",
      "\tTraining batch 572 Loss: 3.336674\n",
      "\tTraining batch 573 Loss: 3.334831\n",
      "\tTraining batch 574 Loss: 3.327942\n",
      "\tTraining batch 575 Loss: 3.323382\n",
      "\tTraining batch 576 Loss: 3.347487\n",
      "\tTraining batch 577 Loss: 3.324661\n",
      "\tTraining batch 578 Loss: 3.356230\n",
      "\tTraining batch 579 Loss: 3.343801\n",
      "\tTraining batch 580 Loss: 3.331487\n",
      "\tTraining batch 581 Loss: 3.315340\n",
      "\tTraining batch 582 Loss: 3.321679\n",
      "\tTraining batch 583 Loss: 3.321946\n",
      "\tTraining batch 584 Loss: 3.320401\n",
      "\tTraining batch 585 Loss: 3.317092\n",
      "\tTraining batch 586 Loss: 3.322827\n",
      "\tTraining batch 587 Loss: 3.339285\n",
      "\tTraining batch 588 Loss: 3.335112\n",
      "\tTraining batch 589 Loss: 3.326578\n",
      "\tTraining batch 590 Loss: 3.330292\n",
      "\tTraining batch 591 Loss: 3.336548\n",
      "\tTraining batch 592 Loss: 3.341644\n",
      "\tTraining batch 593 Loss: 3.329863\n",
      "\tTraining batch 594 Loss: 3.323937\n",
      "\tTraining batch 595 Loss: 3.341636\n",
      "\tTraining batch 596 Loss: 3.337751\n",
      "\tTraining batch 597 Loss: 3.343578\n",
      "\tTraining batch 598 Loss: 3.312804\n",
      "\tTraining batch 599 Loss: 3.331649\n",
      "\tTraining batch 600 Loss: 3.355499\n",
      "\tTraining batch 601 Loss: 3.343592\n",
      "\tTraining batch 602 Loss: 3.325721\n",
      "\tTraining batch 603 Loss: 3.333450\n",
      "\tTraining batch 604 Loss: 3.334355\n",
      "\tTraining batch 605 Loss: 3.326148\n",
      "\tTraining batch 606 Loss: 3.326801\n",
      "\tTraining batch 607 Loss: 3.356010\n",
      "\tTraining batch 608 Loss: 3.328409\n",
      "\tTraining batch 609 Loss: 3.352914\n",
      "\tTraining batch 610 Loss: 3.326424\n",
      "\tTraining batch 611 Loss: 3.333130\n",
      "\tTraining batch 612 Loss: 3.342415\n",
      "\tTraining batch 613 Loss: 3.310856\n",
      "\tTraining batch 614 Loss: 3.340906\n",
      "\tTraining batch 615 Loss: 3.335092\n",
      "\tTraining batch 616 Loss: 3.328043\n",
      "\tTraining batch 617 Loss: 3.348651\n",
      "\tTraining batch 618 Loss: 3.331811\n",
      "\tTraining batch 619 Loss: 3.327324\n",
      "\tTraining batch 620 Loss: 3.332041\n",
      "\tTraining batch 621 Loss: 3.338794\n",
      "\tTraining batch 622 Loss: 3.334137\n",
      "\tTraining batch 623 Loss: 3.357309\n",
      "\tTraining batch 624 Loss: 3.332041\n",
      "\tTraining batch 625 Loss: 3.324443\n",
      "\tTraining batch 626 Loss: 3.344810\n",
      "\tTraining batch 627 Loss: 3.337119\n",
      "\tTraining batch 628 Loss: 3.330328\n",
      "\tTraining batch 629 Loss: 3.373231\n",
      "\tTraining batch 630 Loss: 3.328167\n",
      "\tTraining batch 631 Loss: 3.338250\n",
      "\tTraining batch 632 Loss: 3.344100\n",
      "\tTraining batch 633 Loss: 3.333343\n",
      "\tTraining batch 634 Loss: 3.377968\n",
      "\tTraining batch 635 Loss: 3.328191\n",
      "\tTraining batch 636 Loss: 3.333506\n",
      "\tTraining batch 637 Loss: 3.347737\n",
      "\tTraining batch 638 Loss: 3.318085\n",
      "\tTraining batch 639 Loss: 3.363158\n",
      "\tTraining batch 640 Loss: 3.330122\n",
      "\tTraining batch 641 Loss: 3.333575\n",
      "\tTraining batch 642 Loss: 3.342801\n",
      "\tTraining batch 643 Loss: 3.323152\n",
      "\tTraining batch 644 Loss: 3.337046\n",
      "\tTraining batch 645 Loss: 3.336414\n",
      "\tTraining batch 646 Loss: 3.342360\n",
      "\tTraining batch 647 Loss: 3.341542\n",
      "\tTraining batch 648 Loss: 3.336400\n",
      "\tTraining batch 649 Loss: 3.324531\n",
      "\tTraining batch 650 Loss: 3.340903\n",
      "\tTraining batch 651 Loss: 3.325993\n",
      "\tTraining batch 652 Loss: 3.316401\n",
      "\tTraining batch 653 Loss: 3.352306\n",
      "\tTraining batch 654 Loss: 3.338914\n",
      "\tTraining batch 655 Loss: 3.332899\n",
      "\tTraining batch 656 Loss: 3.337688\n",
      "\tTraining batch 657 Loss: 3.325814\n",
      "\tTraining batch 658 Loss: 3.319430\n",
      "\tTraining batch 659 Loss: 3.323785\n",
      "\tTraining batch 660 Loss: 3.334637\n",
      "\tTraining batch 661 Loss: 3.327957\n",
      "\tTraining batch 662 Loss: 3.322434\n",
      "\tTraining batch 663 Loss: 3.320335\n",
      "\tTraining batch 664 Loss: 3.333459\n",
      "\tTraining batch 665 Loss: 3.328577\n",
      "\tTraining batch 666 Loss: 3.328645\n",
      "\tTraining batch 667 Loss: 3.331233\n",
      "\tTraining batch 668 Loss: 3.335183\n",
      "\tTraining batch 669 Loss: 3.349868\n",
      "\tTraining batch 670 Loss: 3.326071\n",
      "\tTraining batch 671 Loss: 3.337363\n",
      "\tTraining batch 672 Loss: 3.333425\n",
      "\tTraining batch 673 Loss: 3.331702\n",
      "\tTraining batch 674 Loss: 3.339475\n",
      "\tTraining batch 675 Loss: 3.350739\n",
      "\tTraining batch 676 Loss: 3.340149\n",
      "\tTraining batch 677 Loss: 3.329838\n",
      "\tTraining batch 678 Loss: 3.347099\n",
      "\tTraining batch 679 Loss: 3.313419\n",
      "\tTraining batch 680 Loss: 3.343750\n",
      "\tTraining batch 681 Loss: 3.361496\n",
      "\tTraining batch 682 Loss: 3.321908\n",
      "\tTraining batch 683 Loss: 3.334214\n",
      "\tTraining batch 684 Loss: 3.305889\n",
      "\tTraining batch 685 Loss: 3.330281\n",
      "\tTraining batch 686 Loss: 3.329209\n",
      "\tTraining batch 687 Loss: 3.322970\n",
      "\tTraining batch 688 Loss: 3.348919\n",
      "\tTraining batch 689 Loss: 3.336410\n",
      "\tTraining batch 690 Loss: 3.350857\n",
      "\tTraining batch 691 Loss: 3.329123\n",
      "\tTraining batch 692 Loss: 3.321309\n",
      "\tTraining batch 693 Loss: 3.330626\n",
      "\tTraining batch 694 Loss: 3.343569\n",
      "\tTraining batch 695 Loss: 3.330357\n",
      "\tTraining batch 696 Loss: 3.344869\n",
      "\tTraining batch 697 Loss: 3.320230\n",
      "\tTraining batch 698 Loss: 3.346646\n",
      "\tTraining batch 699 Loss: 3.341788\n",
      "\tTraining batch 700 Loss: 3.352330\n",
      "\tTraining batch 701 Loss: 3.344178\n",
      "\tTraining batch 702 Loss: 3.348502\n",
      "\tTraining batch 703 Loss: 3.321011\n",
      "\tTraining batch 704 Loss: 3.335873\n",
      "\tTraining batch 705 Loss: 3.314240\n",
      "\tTraining batch 706 Loss: 3.328241\n",
      "\tTraining batch 707 Loss: 3.341136\n",
      "\tTraining batch 708 Loss: 3.332859\n",
      "\tTraining batch 709 Loss: 3.332330\n",
      "\tTraining batch 710 Loss: 3.343106\n",
      "\tTraining batch 711 Loss: 3.356313\n",
      "\tTraining batch 712 Loss: 3.328733\n",
      "\tTraining batch 713 Loss: 3.337535\n",
      "\tTraining batch 714 Loss: 3.333040\n",
      "\tTraining batch 715 Loss: 3.363797\n",
      "\tTraining batch 716 Loss: 3.320397\n",
      "\tTraining batch 717 Loss: 3.341701\n",
      "\tTraining batch 718 Loss: 3.325428\n",
      "\tTraining batch 719 Loss: 3.336846\n",
      "\tTraining batch 720 Loss: 3.333798\n",
      "\tTraining batch 721 Loss: 3.343725\n",
      "\tTraining batch 722 Loss: 3.331882\n",
      "\tTraining batch 723 Loss: 3.330983\n",
      "\tTraining batch 724 Loss: 3.342887\n",
      "\tTraining batch 725 Loss: 3.349755\n",
      "\tTraining batch 726 Loss: 3.337039\n",
      "\tTraining batch 727 Loss: 3.336032\n",
      "\tTraining batch 728 Loss: 3.327382\n",
      "\tTraining batch 729 Loss: 3.337367\n",
      "\tTraining batch 730 Loss: 3.337097\n",
      "\tTraining batch 731 Loss: 3.342297\n",
      "\tTraining batch 732 Loss: 3.347749\n",
      "\tTraining batch 733 Loss: 3.326620\n",
      "\tTraining batch 734 Loss: 3.343286\n",
      "\tTraining batch 735 Loss: 3.327275\n",
      "\tTraining batch 736 Loss: 3.340297\n",
      "\tTraining batch 737 Loss: 3.318356\n",
      "\tTraining batch 738 Loss: 3.352208\n",
      "\tTraining batch 739 Loss: 3.332645\n",
      "\tTraining batch 740 Loss: 3.323922\n",
      "\tTraining batch 741 Loss: 3.335212\n",
      "\tTraining batch 742 Loss: 3.353496\n",
      "\tTraining batch 743 Loss: 3.321630\n",
      "\tTraining batch 744 Loss: 3.334512\n",
      "\tTraining batch 745 Loss: 3.324228\n",
      "\tTraining batch 746 Loss: 3.326890\n",
      "\tTraining batch 747 Loss: 3.338932\n",
      "\tTraining batch 748 Loss: 3.343174\n",
      "\tTraining batch 749 Loss: 3.344417\n",
      "\tTraining batch 750 Loss: 3.338952\n",
      "\tTraining batch 751 Loss: 3.337148\n",
      "\tTraining batch 752 Loss: 3.334347\n",
      "\tTraining batch 753 Loss: 3.340815\n",
      "\tTraining batch 754 Loss: 3.334104\n",
      "\tTraining batch 755 Loss: 3.331544\n",
      "\tTraining batch 756 Loss: 3.346668\n",
      "\tTraining batch 757 Loss: 3.352571\n",
      "\tTraining batch 758 Loss: 3.328660\n",
      "\tTraining batch 759 Loss: 3.336677\n",
      "\tTraining batch 760 Loss: 3.332884\n",
      "\tTraining batch 761 Loss: 3.339670\n",
      "\tTraining batch 762 Loss: 3.328670\n",
      "\tTraining batch 763 Loss: 3.323211\n",
      "\tTraining batch 764 Loss: 3.335733\n",
      "\tTraining batch 765 Loss: 3.342225\n",
      "\tTraining batch 766 Loss: 3.330831\n",
      "\tTraining batch 767 Loss: 3.349512\n",
      "\tTraining batch 768 Loss: 3.327070\n",
      "\tTraining batch 769 Loss: 3.342745\n",
      "\tTraining batch 770 Loss: 3.318413\n",
      "\tTraining batch 771 Loss: 3.334378\n",
      "\tTraining batch 772 Loss: 3.345258\n",
      "\tTraining batch 773 Loss: 3.346125\n",
      "\tTraining batch 774 Loss: 3.338753\n",
      "\tTraining batch 775 Loss: 3.332459\n",
      "\tTraining batch 776 Loss: 3.330689\n",
      "\tTraining batch 777 Loss: 3.341983\n",
      "\tTraining batch 778 Loss: 3.340209\n",
      "\tTraining batch 779 Loss: 3.343775\n",
      "\tTraining batch 780 Loss: 3.320642\n",
      "\tTraining batch 781 Loss: 3.325170\n",
      "\tTraining batch 782 Loss: 3.331225\n",
      "\tTraining batch 783 Loss: 3.336190\n",
      "\tTraining batch 784 Loss: 3.336216\n",
      "\tTraining batch 785 Loss: 3.331162\n",
      "\tTraining batch 786 Loss: 3.328651\n",
      "\tTraining batch 787 Loss: 3.331297\n",
      "\tTraining batch 788 Loss: 3.349517\n",
      "\tTraining batch 789 Loss: 3.331817\n",
      "\tTraining batch 790 Loss: 3.337269\n",
      "\tTraining batch 791 Loss: 3.326878\n",
      "\tTraining batch 792 Loss: 3.312742\n",
      "\tTraining batch 793 Loss: 3.338664\n",
      "\tTraining batch 794 Loss: 3.334116\n",
      "\tTraining batch 795 Loss: 3.328249\n",
      "\tTraining batch 796 Loss: 3.333756\n",
      "\tTraining batch 797 Loss: 3.340339\n",
      "\tTraining batch 798 Loss: 3.340071\n",
      "\tTraining batch 799 Loss: 3.323157\n",
      "\tTraining batch 800 Loss: 3.332828\n",
      "\tTraining batch 801 Loss: 3.327345\n",
      "\tTraining batch 802 Loss: 3.328710\n",
      "\tTraining batch 803 Loss: 3.341324\n",
      "\tTraining batch 804 Loss: 3.339690\n",
      "\tTraining batch 805 Loss: 3.322451\n",
      "\tTraining batch 806 Loss: 3.333036\n",
      "\tTraining batch 807 Loss: 3.326505\n",
      "\tTraining batch 808 Loss: 3.359696\n",
      "\tTraining batch 809 Loss: 3.346697\n",
      "\tTraining batch 810 Loss: 3.316579\n",
      "\tTraining batch 811 Loss: 3.337983\n",
      "\tTraining batch 812 Loss: 3.333594\n",
      "\tTraining batch 813 Loss: 3.343648\n",
      "\tTraining batch 814 Loss: 3.333272\n",
      "\tTraining batch 815 Loss: 3.335767\n",
      "\tTraining batch 816 Loss: 3.330459\n",
      "\tTraining batch 817 Loss: 3.343353\n",
      "\tTraining batch 818 Loss: 3.348058\n",
      "\tTraining batch 819 Loss: 3.315956\n",
      "\tTraining batch 820 Loss: 3.315123\n",
      "\tTraining batch 821 Loss: 3.343873\n",
      "\tTraining batch 822 Loss: 3.329800\n",
      "\tTraining batch 823 Loss: 3.344556\n",
      "\tTraining batch 824 Loss: 3.309298\n",
      "\tTraining batch 825 Loss: 3.337561\n",
      "\tTraining batch 826 Loss: 3.306096\n",
      "\tTraining batch 827 Loss: 3.337830\n",
      "\tTraining batch 828 Loss: 3.317205\n",
      "\tTraining batch 829 Loss: 3.334974\n",
      "\tTraining batch 830 Loss: 3.345551\n",
      "\tTraining batch 831 Loss: 3.360368\n",
      "\tTraining batch 832 Loss: 3.339517\n",
      "\tTraining batch 833 Loss: 3.328623\n",
      "\tTraining batch 834 Loss: 3.346527\n",
      "\tTraining batch 835 Loss: 3.317266\n",
      "\tTraining batch 836 Loss: 3.343794\n",
      "\tTraining batch 837 Loss: 3.354722\n",
      "\tTraining batch 838 Loss: 3.337028\n",
      "\tTraining batch 839 Loss: 3.339550\n",
      "\tTraining batch 840 Loss: 3.334276\n",
      "\tTraining batch 841 Loss: 3.340511\n",
      "\tTraining batch 842 Loss: 3.308821\n",
      "\tTraining batch 843 Loss: 3.328242\n",
      "\tTraining batch 844 Loss: 3.318467\n",
      "\tTraining batch 845 Loss: 3.342679\n",
      "\tTraining batch 846 Loss: 3.341692\n",
      "\tTraining batch 847 Loss: 3.321683\n",
      "\tTraining batch 848 Loss: 3.334238\n",
      "\tTraining batch 849 Loss: 3.331320\n",
      "\tTraining batch 850 Loss: 3.330746\n",
      "\tTraining batch 851 Loss: 3.340960\n",
      "\tTraining batch 852 Loss: 3.327481\n",
      "\tTraining batch 853 Loss: 3.328649\n",
      "\tTraining batch 854 Loss: 3.361840\n",
      "\tTraining batch 855 Loss: 3.324206\n",
      "\tTraining batch 856 Loss: 3.344831\n",
      "\tTraining batch 857 Loss: 3.320439\n",
      "\tTraining batch 858 Loss: 3.352171\n",
      "\tTraining batch 859 Loss: 3.333982\n",
      "\tTraining batch 860 Loss: 3.330833\n",
      "\tTraining batch 861 Loss: 3.342994\n",
      "\tTraining batch 862 Loss: 3.324195\n",
      "\tTraining batch 863 Loss: 3.337284\n",
      "\tTraining batch 864 Loss: 3.328425\n",
      "\tTraining batch 865 Loss: 3.338858\n",
      "\tTraining batch 866 Loss: 3.340877\n",
      "\tTraining batch 867 Loss: 3.348825\n",
      "\tTraining batch 868 Loss: 3.339291\n",
      "\tTraining batch 869 Loss: 3.335752\n",
      "\tTraining batch 870 Loss: 3.340304\n",
      "\tTraining batch 871 Loss: 3.328139\n",
      "\tTraining batch 872 Loss: 3.337513\n",
      "\tTraining batch 873 Loss: 3.345354\n",
      "\tTraining batch 874 Loss: 3.343007\n",
      "\tTraining batch 875 Loss: 3.337659\n",
      "\tTraining batch 876 Loss: 3.344657\n",
      "\tTraining batch 877 Loss: 3.341972\n",
      "\tTraining batch 878 Loss: 3.331558\n",
      "\tTraining batch 879 Loss: 3.348885\n",
      "\tTraining batch 880 Loss: 3.348584\n",
      "\tTraining batch 881 Loss: 3.340046\n",
      "\tTraining batch 882 Loss: 3.338637\n",
      "\tTraining batch 883 Loss: 3.332020\n",
      "\tTraining batch 884 Loss: 3.331795\n",
      "\tTraining batch 885 Loss: 3.342634\n",
      "\tTraining batch 886 Loss: 3.332670\n",
      "\tTraining batch 887 Loss: 3.336415\n",
      "\tTraining batch 888 Loss: 3.335453\n",
      "\tTraining batch 889 Loss: 3.342267\n",
      "\tTraining batch 890 Loss: 3.327157\n",
      "\tTraining batch 891 Loss: 3.343402\n",
      "\tTraining batch 892 Loss: 3.321143\n",
      "\tTraining batch 893 Loss: 3.330009\n",
      "\tTraining batch 894 Loss: 3.340207\n",
      "\tTraining batch 895 Loss: 3.346086\n",
      "\tTraining batch 896 Loss: 3.343845\n",
      "\tTraining batch 897 Loss: 3.339414\n",
      "\tTraining batch 898 Loss: 3.328930\n",
      "\tTraining batch 899 Loss: 3.327204\n",
      "\tTraining batch 900 Loss: 3.341543\n",
      "\tTraining batch 901 Loss: 3.322858\n",
      "\tTraining batch 902 Loss: 3.322201\n",
      "\tTraining batch 903 Loss: 3.334738\n",
      "\tTraining batch 904 Loss: 3.337143\n",
      "\tTraining batch 905 Loss: 3.322621\n",
      "\tTraining batch 906 Loss: 3.339324\n",
      "\tTraining batch 907 Loss: 3.332557\n",
      "\tTraining batch 908 Loss: 3.336025\n",
      "\tTraining batch 909 Loss: 3.326618\n",
      "\tTraining batch 910 Loss: 3.340132\n",
      "\tTraining batch 911 Loss: 3.325872\n",
      "\tTraining batch 912 Loss: 3.322066\n",
      "\tTraining batch 913 Loss: 3.333823\n",
      "\tTraining batch 914 Loss: 3.307535\n",
      "\tTraining batch 915 Loss: 3.325484\n",
      "\tTraining batch 916 Loss: 3.346307\n",
      "\tTraining batch 917 Loss: 3.340145\n",
      "\tTraining batch 918 Loss: 3.319443\n",
      "\tTraining batch 919 Loss: 3.334630\n",
      "\tTraining batch 920 Loss: 3.342009\n",
      "\tTraining batch 921 Loss: 3.344065\n",
      "\tTraining batch 922 Loss: 3.328975\n",
      "\tTraining batch 923 Loss: 3.341676\n",
      "\tTraining batch 924 Loss: 3.332223\n",
      "\tTraining batch 925 Loss: 3.342484\n",
      "\tTraining batch 926 Loss: 3.331437\n",
      "\tTraining batch 927 Loss: 3.329554\n",
      "\tTraining batch 928 Loss: 3.322258\n",
      "\tTraining batch 929 Loss: 3.320200\n",
      "\tTraining batch 930 Loss: 3.340826\n",
      "\tTraining batch 931 Loss: 3.331887\n",
      "\tTraining batch 932 Loss: 3.314455\n",
      "\tTraining batch 933 Loss: 3.328670\n",
      "\tTraining batch 934 Loss: 3.333820\n",
      "\tTraining batch 935 Loss: 3.318705\n",
      "\tTraining batch 936 Loss: 3.339861\n",
      "\tTraining batch 937 Loss: 3.356348\n",
      "\tTraining batch 938 Loss: 3.337196\n",
      "\tTraining batch 939 Loss: 3.354703\n",
      "\tTraining batch 940 Loss: 3.338299\n",
      "\tTraining batch 941 Loss: 3.319546\n",
      "\tTraining batch 942 Loss: 3.315812\n",
      "\tTraining batch 943 Loss: 3.338598\n",
      "\tTraining batch 944 Loss: 3.326813\n",
      "\tTraining batch 945 Loss: 3.323634\n",
      "\tTraining batch 946 Loss: 3.329208\n",
      "\tTraining batch 947 Loss: 3.335051\n",
      "\tTraining batch 948 Loss: 3.330322\n",
      "\tTraining batch 949 Loss: 3.360189\n",
      "\tTraining batch 950 Loss: 3.319078\n",
      "\tTraining batch 951 Loss: 3.350805\n",
      "\tTraining batch 952 Loss: 3.351981\n",
      "\tTraining batch 953 Loss: 3.349498\n",
      "\tTraining batch 954 Loss: 3.333329\n",
      "\tTraining batch 955 Loss: 3.313992\n",
      "\tTraining batch 956 Loss: 3.330630\n",
      "\tTraining batch 957 Loss: 3.334746\n",
      "\tTraining batch 958 Loss: 3.348963\n",
      "\tTraining batch 959 Loss: 3.346435\n",
      "\tTraining batch 960 Loss: 3.356376\n",
      "\tTraining batch 961 Loss: 3.331737\n",
      "\tTraining batch 962 Loss: 3.310397\n",
      "\tTraining batch 963 Loss: 3.334500\n",
      "\tTraining batch 964 Loss: 3.331594\n",
      "\tTraining batch 965 Loss: 3.312547\n",
      "\tTraining batch 966 Loss: 3.340099\n",
      "\tTraining batch 967 Loss: 3.323375\n",
      "\tTraining batch 968 Loss: 3.330000\n",
      "\tTraining batch 969 Loss: 3.314806\n",
      "\tTraining batch 970 Loss: 3.321753\n",
      "\tTraining batch 971 Loss: 3.346755\n",
      "\tTraining batch 972 Loss: 3.346735\n",
      "\tTraining batch 973 Loss: 3.319603\n",
      "\tTraining batch 974 Loss: 3.342952\n",
      "\tTraining batch 975 Loss: 3.355945\n",
      "\tTraining batch 976 Loss: 3.334672\n",
      "\tTraining batch 977 Loss: 3.331476\n",
      "\tTraining batch 978 Loss: 3.368149\n",
      "\tTraining batch 979 Loss: 3.333678\n",
      "\tTraining batch 980 Loss: 3.325681\n",
      "\tTraining batch 981 Loss: 3.340354\n",
      "\tTraining batch 982 Loss: 3.345359\n",
      "\tTraining batch 983 Loss: 3.331925\n",
      "\tTraining batch 984 Loss: 3.348691\n",
      "\tTraining batch 985 Loss: 3.319230\n",
      "\tTraining batch 986 Loss: 3.335525\n",
      "\tTraining batch 987 Loss: 3.323118\n",
      "\tTraining batch 988 Loss: 3.323755\n",
      "\tTraining batch 989 Loss: 3.337314\n",
      "\tTraining batch 990 Loss: 3.343195\n",
      "\tTraining batch 991 Loss: 3.323313\n",
      "\tTraining batch 992 Loss: 3.320513\n",
      "\tTraining batch 993 Loss: 3.333954\n",
      "\tTraining batch 994 Loss: 3.335413\n",
      "\tTraining batch 995 Loss: 3.326286\n",
      "\tTraining batch 996 Loss: 3.333503\n",
      "\tTraining batch 997 Loss: 3.327921\n",
      "\tTraining batch 998 Loss: 3.335260\n",
      "\tTraining batch 999 Loss: 3.353278\n",
      "\tTraining batch 1000 Loss: 3.341282\n",
      "\tTraining batch 1001 Loss: 3.351388\n",
      "\tTraining batch 1002 Loss: 3.300185\n",
      "\tTraining batch 1003 Loss: 3.352958\n",
      "\tTraining batch 1004 Loss: 3.349986\n",
      "\tTraining batch 1005 Loss: 3.353726\n",
      "\tTraining batch 1006 Loss: 3.328725\n",
      "\tTraining batch 1007 Loss: 3.322322\n",
      "\tTraining batch 1008 Loss: 3.330763\n",
      "\tTraining batch 1009 Loss: 3.339843\n",
      "\tTraining batch 1010 Loss: 3.347014\n",
      "\tTraining batch 1011 Loss: 3.336317\n",
      "\tTraining batch 1012 Loss: 3.327923\n",
      "\tTraining batch 1013 Loss: 3.327604\n",
      "\tTraining batch 1014 Loss: 3.345954\n",
      "\tTraining batch 1015 Loss: 3.354213\n",
      "\tTraining batch 1016 Loss: 3.320260\n",
      "\tTraining batch 1017 Loss: 3.331226\n",
      "\tTraining batch 1018 Loss: 3.346300\n",
      "\tTraining batch 1019 Loss: 3.337501\n",
      "\tTraining batch 1020 Loss: 3.352095\n",
      "\tTraining batch 1021 Loss: 3.349408\n",
      "\tTraining batch 1022 Loss: 3.320234\n",
      "\tTraining batch 1023 Loss: 3.343435\n",
      "\tTraining batch 1024 Loss: 3.314595\n",
      "\tTraining batch 1025 Loss: 3.337877\n",
      "\tTraining batch 1026 Loss: 3.345315\n",
      "\tTraining batch 1027 Loss: 3.343128\n",
      "\tTraining batch 1028 Loss: 3.342991\n",
      "\tTraining batch 1029 Loss: 3.335119\n",
      "\tTraining batch 1030 Loss: 3.325886\n",
      "\tTraining batch 1031 Loss: 3.322671\n",
      "\tTraining batch 1032 Loss: 3.325511\n",
      "\tTraining batch 1033 Loss: 3.340325\n",
      "\tTraining batch 1034 Loss: 3.311457\n",
      "\tTraining batch 1035 Loss: 3.325240\n",
      "\tTraining batch 1036 Loss: 3.320601\n",
      "\tTraining batch 1037 Loss: 3.331276\n",
      "\tTraining batch 1038 Loss: 3.321855\n",
      "\tTraining batch 1039 Loss: 3.341250\n",
      "\tTraining batch 1040 Loss: 3.329462\n",
      "\tTraining batch 1041 Loss: 3.329119\n",
      "\tTraining batch 1042 Loss: 3.336837\n",
      "\tTraining batch 1043 Loss: 3.353482\n",
      "\tTraining batch 1044 Loss: 3.334119\n",
      "\tTraining batch 1045 Loss: 3.317190\n",
      "\tTraining batch 1046 Loss: 3.347211\n",
      "\tTraining batch 1047 Loss: 3.336298\n",
      "\tTraining batch 1048 Loss: 3.339592\n",
      "\tTraining batch 1049 Loss: 3.322551\n",
      "\tTraining batch 1050 Loss: 3.323739\n",
      "\tTraining batch 1051 Loss: 3.339900\n",
      "\tTraining batch 1052 Loss: 3.332027\n",
      "\tTraining batch 1053 Loss: 3.327542\n",
      "\tTraining batch 1054 Loss: 3.321155\n",
      "\tTraining batch 1055 Loss: 3.337592\n",
      "\tTraining batch 1056 Loss: 3.335583\n",
      "\tTraining batch 1057 Loss: 3.333552\n",
      "\tTraining batch 1058 Loss: 3.351346\n",
      "\tTraining batch 1059 Loss: 3.322452\n",
      "\tTraining batch 1060 Loss: 3.336846\n",
      "\tTraining batch 1061 Loss: 3.337054\n",
      "\tTraining batch 1062 Loss: 3.321321\n",
      "\tTraining batch 1063 Loss: 3.321356\n",
      "\tTraining batch 1064 Loss: 3.334793\n",
      "\tTraining batch 1065 Loss: 3.342319\n",
      "\tTraining batch 1066 Loss: 3.347585\n",
      "\tTraining batch 1067 Loss: 3.326577\n",
      "\tTraining batch 1068 Loss: 3.318265\n",
      "\tTraining batch 1069 Loss: 3.335090\n",
      "\tTraining batch 1070 Loss: 3.345972\n",
      "\tTraining batch 1071 Loss: 3.325321\n",
      "\tTraining batch 1072 Loss: 3.327133\n",
      "\tTraining batch 1073 Loss: 3.347732\n",
      "\tTraining batch 1074 Loss: 3.336535\n",
      "\tTraining batch 1075 Loss: 3.338493\n",
      "\tTraining batch 1076 Loss: 3.346676\n",
      "\tTraining batch 1077 Loss: 3.350428\n",
      "\tTraining batch 1078 Loss: 3.359091\n",
      "\tTraining batch 1079 Loss: 3.307390\n",
      "\tTraining batch 1080 Loss: 3.318479\n",
      "\tTraining batch 1081 Loss: 3.338095\n",
      "\tTraining batch 1082 Loss: 3.331391\n",
      "\tTraining batch 1083 Loss: 3.326644\n",
      "\tTraining batch 1084 Loss: 3.345399\n",
      "\tTraining batch 1085 Loss: 3.315382\n",
      "\tTraining batch 1086 Loss: 3.338269\n",
      "\tTraining batch 1087 Loss: 3.349394\n",
      "\tTraining batch 1088 Loss: 3.329150\n",
      "\tTraining batch 1089 Loss: 3.322336\n",
      "\tTraining batch 1090 Loss: 3.351865\n",
      "\tTraining batch 1091 Loss: 3.332322\n",
      "\tTraining batch 1092 Loss: 3.359065\n",
      "\tTraining batch 1093 Loss: 3.345392\n",
      "\tTraining batch 1094 Loss: 3.353694\n",
      "\tTraining batch 1095 Loss: 3.334930\n",
      "\tTraining batch 1096 Loss: 3.331180\n",
      "\tTraining batch 1097 Loss: 3.343011\n",
      "\tTraining batch 1098 Loss: 3.353175\n",
      "\tTraining batch 1099 Loss: 3.318792\n",
      "\tTraining batch 1100 Loss: 3.337506\n",
      "\tTraining batch 1101 Loss: 3.336227\n",
      "\tTraining batch 1102 Loss: 3.340908\n",
      "\tTraining batch 1103 Loss: 3.327314\n",
      "\tTraining batch 1104 Loss: 3.325572\n",
      "\tTraining batch 1105 Loss: 3.328614\n",
      "\tTraining batch 1106 Loss: 3.338356\n",
      "\tTraining batch 1107 Loss: 3.326250\n",
      "\tTraining batch 1108 Loss: 3.338581\n",
      "\tTraining batch 1109 Loss: 3.322527\n",
      "\tTraining batch 1110 Loss: 3.321639\n",
      "\tTraining batch 1111 Loss: 3.326694\n",
      "\tTraining batch 1112 Loss: 3.326042\n",
      "\tTraining batch 1113 Loss: 3.323208\n",
      "\tTraining batch 1114 Loss: 3.322103\n",
      "\tTraining batch 1115 Loss: 3.323283\n",
      "\tTraining batch 1116 Loss: 3.338245\n",
      "\tTraining batch 1117 Loss: 3.317713\n",
      "\tTraining batch 1118 Loss: 3.346915\n",
      "\tTraining batch 1119 Loss: 3.361509\n",
      "\tTraining batch 1120 Loss: 3.350019\n",
      "\tTraining batch 1121 Loss: 3.327246\n",
      "\tTraining batch 1122 Loss: 3.333614\n",
      "\tTraining batch 1123 Loss: 3.343284\n",
      "\tTraining batch 1124 Loss: 3.341032\n",
      "\tTraining batch 1125 Loss: 3.343575\n",
      "\tTraining batch 1126 Loss: 3.331550\n",
      "\tTraining batch 1127 Loss: 3.344880\n",
      "\tTraining batch 1128 Loss: 3.341151\n",
      "\tTraining batch 1129 Loss: 3.337467\n",
      "\tTraining batch 1130 Loss: 3.332802\n",
      "\tTraining batch 1131 Loss: 3.339725\n",
      "\tTraining batch 1132 Loss: 3.326332\n",
      "\tTraining batch 1133 Loss: 3.335932\n",
      "\tTraining batch 1134 Loss: 3.325446\n",
      "\tTraining batch 1135 Loss: 3.339657\n",
      "\tTraining batch 1136 Loss: 3.312780\n",
      "\tTraining batch 1137 Loss: 3.329509\n",
      "\tTraining batch 1138 Loss: 3.329643\n",
      "\tTraining batch 1139 Loss: 3.321388\n",
      "\tTraining batch 1140 Loss: 3.335244\n",
      "\tTraining batch 1141 Loss: 3.325727\n",
      "\tTraining batch 1142 Loss: 3.328197\n",
      "\tTraining batch 1143 Loss: 3.304042\n",
      "\tTraining batch 1144 Loss: 3.320269\n",
      "\tTraining batch 1145 Loss: 3.313357\n",
      "\tTraining batch 1146 Loss: 3.321322\n",
      "\tTraining batch 1147 Loss: 3.328610\n",
      "\tTraining batch 1148 Loss: 3.331824\n",
      "\tTraining batch 1149 Loss: 3.349022\n",
      "\tTraining batch 1150 Loss: 3.325152\n",
      "\tTraining batch 1151 Loss: 3.352506\n",
      "\tTraining batch 1152 Loss: 3.331622\n",
      "\tTraining batch 1153 Loss: 3.368930\n",
      "\tTraining batch 1154 Loss: 3.362651\n",
      "\tTraining batch 1155 Loss: 3.316460\n",
      "\tTraining batch 1156 Loss: 3.336137\n",
      "\tTraining batch 1157 Loss: 3.323826\n",
      "\tTraining batch 1158 Loss: 3.314629\n",
      "\tTraining batch 1159 Loss: 3.355922\n",
      "\tTraining batch 1160 Loss: 3.311547\n",
      "\tTraining batch 1161 Loss: 3.327389\n",
      "\tTraining batch 1162 Loss: 3.346092\n",
      "\tTraining batch 1163 Loss: 3.325999\n",
      "\tTraining batch 1164 Loss: 3.328573\n",
      "\tTraining batch 1165 Loss: 3.315867\n",
      "\tTraining batch 1166 Loss: 3.354304\n",
      "\tTraining batch 1167 Loss: 3.322471\n",
      "\tTraining batch 1168 Loss: 3.341192\n",
      "\tTraining batch 1169 Loss: 3.322865\n",
      "\tTraining batch 1170 Loss: 3.325386\n",
      "\tTraining batch 1171 Loss: 3.317373\n",
      "\tTraining batch 1172 Loss: 3.331601\n",
      "\tTraining batch 1173 Loss: 3.314221\n",
      "\tTraining batch 1174 Loss: 3.329964\n",
      "\tTraining batch 1175 Loss: 3.322224\n",
      "\tTraining batch 1176 Loss: 3.338821\n",
      "\tTraining batch 1177 Loss: 3.344575\n",
      "\tTraining batch 1178 Loss: 3.339238\n",
      "\tTraining batch 1179 Loss: 3.299117\n",
      "\tTraining batch 1180 Loss: 3.348520\n",
      "\tTraining batch 1181 Loss: 3.325615\n",
      "\tTraining batch 1182 Loss: 3.334390\n",
      "\tTraining batch 1183 Loss: 3.354861\n",
      "\tTraining batch 1184 Loss: 3.318525\n",
      "\tTraining batch 1185 Loss: 3.346235\n",
      "\tTraining batch 1186 Loss: 3.345714\n",
      "\tTraining batch 1187 Loss: 3.335402\n",
      "\tTraining batch 1188 Loss: 3.340970\n",
      "\tTraining batch 1189 Loss: 3.350219\n",
      "\tTraining batch 1190 Loss: 3.339137\n",
      "\tTraining batch 1191 Loss: 3.322912\n",
      "\tTraining batch 1192 Loss: 3.349196\n",
      "\tTraining batch 1193 Loss: 3.329720\n",
      "\tTraining batch 1194 Loss: 3.335073\n",
      "\tTraining batch 1195 Loss: 3.337141\n",
      "\tTraining batch 1196 Loss: 3.329722\n",
      "\tTraining batch 1197 Loss: 3.346225\n",
      "\tTraining batch 1198 Loss: 3.354347\n",
      "\tTraining batch 1199 Loss: 3.357552\n",
      "\tTraining batch 1200 Loss: 3.358897\n",
      "\tTraining batch 1201 Loss: 3.336625\n",
      "\tTraining batch 1202 Loss: 3.344217\n",
      "\tTraining batch 1203 Loss: 3.354477\n",
      "\tTraining batch 1204 Loss: 3.340842\n",
      "\tTraining batch 1205 Loss: 3.348648\n",
      "\tTraining batch 1206 Loss: 3.337520\n",
      "\tTraining batch 1207 Loss: 3.358341\n",
      "\tTraining batch 1208 Loss: 3.317153\n",
      "\tTraining batch 1209 Loss: 3.324294\n",
      "\tTraining batch 1210 Loss: 3.355622\n",
      "\tTraining batch 1211 Loss: 3.322243\n",
      "\tTraining batch 1212 Loss: 3.344731\n",
      "\tTraining batch 1213 Loss: 3.338234\n",
      "\tTraining batch 1214 Loss: 3.333544\n",
      "\tTraining batch 1215 Loss: 3.329772\n",
      "\tTraining batch 1216 Loss: 3.331789\n",
      "\tTraining batch 1217 Loss: 3.332229\n",
      "\tTraining batch 1218 Loss: 3.322004\n",
      "\tTraining batch 1219 Loss: 3.365838\n",
      "\tTraining batch 1220 Loss: 3.329550\n",
      "\tTraining batch 1221 Loss: 3.338176\n",
      "\tTraining batch 1222 Loss: 3.330538\n",
      "\tTraining batch 1223 Loss: 3.322597\n",
      "\tTraining batch 1224 Loss: 3.328935\n",
      "\tTraining batch 1225 Loss: 3.334337\n",
      "\tTraining batch 1226 Loss: 3.317707\n",
      "\tTraining batch 1227 Loss: 3.327954\n",
      "\tTraining batch 1228 Loss: 3.340384\n",
      "\tTraining batch 1229 Loss: 3.330569\n",
      "\tTraining batch 1230 Loss: 3.335850\n",
      "\tTraining batch 1231 Loss: 3.341961\n",
      "\tTraining batch 1232 Loss: 3.351123\n",
      "\tTraining batch 1233 Loss: 3.337308\n",
      "\tTraining batch 1234 Loss: 3.330180\n",
      "\tTraining batch 1235 Loss: 3.331312\n",
      "\tTraining batch 1236 Loss: 3.344742\n",
      "\tTraining batch 1237 Loss: 3.342929\n",
      "\tTraining batch 1238 Loss: 3.354634\n",
      "\tTraining batch 1239 Loss: 3.325731\n",
      "\tTraining batch 1240 Loss: 3.343084\n",
      "\tTraining batch 1241 Loss: 3.340553\n",
      "\tTraining batch 1242 Loss: 3.339555\n",
      "\tTraining batch 1243 Loss: 3.317054\n",
      "\tTraining batch 1244 Loss: 3.349122\n",
      "\tTraining batch 1245 Loss: 3.330151\n",
      "\tTraining batch 1246 Loss: 3.329195\n",
      "\tTraining batch 1247 Loss: 3.326671\n",
      "\tTraining batch 1248 Loss: 3.331905\n",
      "\tTraining batch 1249 Loss: 3.339733\n",
      "\tTraining batch 1250 Loss: 3.342869\n",
      "\tTraining batch 1251 Loss: 3.329954\n",
      "\tTraining batch 1252 Loss: 3.337624\n",
      "\tTraining batch 1253 Loss: 3.338333\n",
      "\tTraining batch 1254 Loss: 3.325840\n",
      "\tTraining batch 1255 Loss: 3.321380\n",
      "\tTraining batch 1256 Loss: 3.337797\n",
      "\tTraining batch 1257 Loss: 3.318413\n",
      "\tTraining batch 1258 Loss: 3.344530\n",
      "\tTraining batch 1259 Loss: 3.346582\n",
      "\tTraining batch 1260 Loss: 3.332857\n",
      "\tTraining batch 1261 Loss: 3.331123\n",
      "\tTraining batch 1262 Loss: 3.334232\n",
      "\tTraining batch 1263 Loss: 3.342699\n",
      "\tTraining batch 1264 Loss: 3.338504\n",
      "\tTraining batch 1265 Loss: 3.314250\n",
      "\tTraining batch 1266 Loss: 3.315173\n",
      "\tTraining batch 1267 Loss: 3.329331\n",
      "\tTraining batch 1268 Loss: 3.350055\n",
      "\tTraining batch 1269 Loss: 3.331838\n",
      "\tTraining batch 1270 Loss: 3.354634\n",
      "\tTraining batch 1271 Loss: 3.339210\n",
      "\tTraining batch 1272 Loss: 3.332020\n",
      "\tTraining batch 1273 Loss: 3.322945\n",
      "\tTraining batch 1274 Loss: 3.368240\n",
      "\tTraining batch 1275 Loss: 3.341697\n",
      "\tTraining batch 1276 Loss: 3.333111\n",
      "\tTraining batch 1277 Loss: 3.349521\n",
      "\tTraining batch 1278 Loss: 3.337169\n",
      "\tTraining batch 1279 Loss: 3.329150\n",
      "\tTraining batch 1280 Loss: 3.328065\n",
      "\tTraining batch 1281 Loss: 3.345502\n",
      "\tTraining batch 1282 Loss: 3.339998\n",
      "\tTraining batch 1283 Loss: 3.333180\n",
      "\tTraining batch 1284 Loss: 3.331676\n",
      "\tTraining batch 1285 Loss: 3.349695\n",
      "\tTraining batch 1286 Loss: 3.345207\n",
      "\tTraining batch 1287 Loss: 3.321642\n",
      "\tTraining batch 1288 Loss: 3.339351\n",
      "\tTraining batch 1289 Loss: 3.322135\n",
      "\tTraining batch 1290 Loss: 3.337070\n",
      "\tTraining batch 1291 Loss: 3.340777\n",
      "\tTraining batch 1292 Loss: 3.325810\n",
      "\tTraining batch 1293 Loss: 3.322793\n",
      "\tTraining batch 1294 Loss: 3.352123\n",
      "\tTraining batch 1295 Loss: 3.324728\n",
      "\tTraining batch 1296 Loss: 3.323215\n",
      "\tTraining batch 1297 Loss: 3.339192\n",
      "\tTraining batch 1298 Loss: 3.339764\n",
      "\tTraining batch 1299 Loss: 3.334261\n",
      "\tTraining batch 1300 Loss: 3.324446\n",
      "\tTraining batch 1301 Loss: 3.319124\n",
      "\tTraining batch 1302 Loss: 3.326336\n",
      "\tTraining batch 1303 Loss: 3.330791\n",
      "\tTraining batch 1304 Loss: 3.335220\n",
      "\tTraining batch 1305 Loss: 3.340362\n",
      "\tTraining batch 1306 Loss: 3.342235\n",
      "\tTraining batch 1307 Loss: 3.311498\n",
      "\tTraining batch 1308 Loss: 3.331191\n",
      "\tTraining batch 1309 Loss: 3.332633\n",
      "\tTraining batch 1310 Loss: 3.310671\n",
      "\tTraining batch 1311 Loss: 3.321068\n",
      "\tTraining batch 1312 Loss: 3.336226\n",
      "\tTraining batch 1313 Loss: 3.323762\n",
      "\tTraining batch 1314 Loss: 3.339216\n",
      "\tTraining batch 1315 Loss: 3.325601\n",
      "\tTraining batch 1316 Loss: 3.310597\n",
      "\tTraining batch 1317 Loss: 3.329843\n",
      "\tTraining batch 1318 Loss: 3.360593\n",
      "\tTraining batch 1319 Loss: 3.327510\n",
      "\tTraining batch 1320 Loss: 3.350525\n",
      "\tTraining batch 1321 Loss: 3.360970\n",
      "\tTraining batch 1322 Loss: 3.313069\n",
      "\tTraining batch 1323 Loss: 3.338626\n",
      "\tTraining batch 1324 Loss: 3.343928\n",
      "\tTraining batch 1325 Loss: 3.321282\n",
      "\tTraining batch 1326 Loss: 3.313538\n",
      "\tTraining batch 1327 Loss: 3.308995\n",
      "\tTraining batch 1328 Loss: 3.319865\n",
      "\tTraining batch 1329 Loss: 3.319984\n",
      "\tTraining batch 1330 Loss: 3.336547\n",
      "\tTraining batch 1331 Loss: 3.349819\n",
      "\tTraining batch 1332 Loss: 3.333843\n",
      "\tTraining batch 1333 Loss: 3.338056\n",
      "\tTraining batch 1334 Loss: 3.339378\n",
      "\tTraining batch 1335 Loss: 3.363183\n",
      "\tTraining batch 1336 Loss: 3.337801\n",
      "\tTraining batch 1337 Loss: 3.337037\n",
      "\tTraining batch 1338 Loss: 3.334830\n",
      "\tTraining batch 1339 Loss: 3.351772\n",
      "\tTraining batch 1340 Loss: 3.326071\n",
      "\tTraining batch 1341 Loss: 3.341053\n",
      "\tTraining batch 1342 Loss: 3.339247\n",
      "\tTraining batch 1343 Loss: 3.317810\n",
      "\tTraining batch 1344 Loss: 3.333619\n",
      "\tTraining batch 1345 Loss: 3.330238\n",
      "\tTraining batch 1346 Loss: 3.327840\n",
      "\tTraining batch 1347 Loss: 3.323739\n",
      "\tTraining batch 1348 Loss: 3.324976\n",
      "\tTraining batch 1349 Loss: 3.340887\n",
      "\tTraining batch 1350 Loss: 3.346950\n",
      "\tTraining batch 1351 Loss: 3.317612\n",
      "\tTraining batch 1352 Loss: 3.345767\n",
      "\tTraining batch 1353 Loss: 3.344866\n",
      "\tTraining batch 1354 Loss: 3.353968\n",
      "\tTraining batch 1355 Loss: 3.308960\n",
      "\tTraining batch 1356 Loss: 3.345474\n",
      "\tTraining batch 1357 Loss: 3.349605\n",
      "\tTraining batch 1358 Loss: 3.363321\n",
      "\tTraining batch 1359 Loss: 3.320799\n",
      "\tTraining batch 1360 Loss: 3.342701\n",
      "\tTraining batch 1361 Loss: 3.327878\n",
      "\tTraining batch 1362 Loss: 3.319614\n",
      "\tTraining batch 1363 Loss: 3.320444\n",
      "\tTraining batch 1364 Loss: 3.336792\n",
      "\tTraining batch 1365 Loss: 3.316161\n",
      "\tTraining batch 1366 Loss: 3.339577\n",
      "\tTraining batch 1367 Loss: 3.343506\n",
      "\tTraining batch 1368 Loss: 3.341324\n",
      "\tTraining batch 1369 Loss: 3.364589\n",
      "\tTraining batch 1370 Loss: 3.340386\n",
      "\tTraining batch 1371 Loss: 3.315134\n",
      "\tTraining batch 1372 Loss: 3.348921\n",
      "\tTraining batch 1373 Loss: 3.329333\n",
      "\tTraining batch 1374 Loss: 3.346151\n",
      "\tTraining batch 1375 Loss: 3.353599\n",
      "\tTraining batch 1376 Loss: 3.313666\n",
      "\tTraining batch 1377 Loss: 3.320832\n",
      "\tTraining batch 1378 Loss: 3.311222\n",
      "\tTraining batch 1379 Loss: 3.353939\n",
      "\tTraining batch 1380 Loss: 3.341164\n",
      "\tTraining batch 1381 Loss: 3.317800\n",
      "\tTraining batch 1382 Loss: 3.317992\n",
      "\tTraining batch 1383 Loss: 3.339652\n",
      "\tTraining batch 1384 Loss: 3.324133\n",
      "\tTraining batch 1385 Loss: 3.321214\n",
      "\tTraining batch 1386 Loss: 3.319358\n",
      "\tTraining batch 1387 Loss: 3.335556\n",
      "\tTraining batch 1388 Loss: 3.363039\n",
      "\tTraining batch 1389 Loss: 3.343456\n",
      "\tTraining batch 1390 Loss: 3.351302\n",
      "\tTraining batch 1391 Loss: 3.347304\n",
      "\tTraining batch 1392 Loss: 3.329212\n",
      "\tTraining batch 1393 Loss: 3.319057\n",
      "\tTraining batch 1394 Loss: 3.320832\n",
      "\tTraining batch 1395 Loss: 3.311498\n",
      "\tTraining batch 1396 Loss: 3.336647\n",
      "\tTraining batch 1397 Loss: 3.315204\n",
      "\tTraining batch 1398 Loss: 3.332467\n",
      "\tTraining batch 1399 Loss: 3.318496\n",
      "\tTraining batch 1400 Loss: 3.342165\n",
      "\tTraining batch 1401 Loss: 3.353895\n",
      "\tTraining batch 1402 Loss: 3.347788\n",
      "\tTraining batch 1403 Loss: 3.361003\n",
      "\tTraining batch 1404 Loss: 3.338621\n",
      "\tTraining batch 1405 Loss: 3.338143\n",
      "\tTraining batch 1406 Loss: 3.339379\n",
      "\tTraining batch 1407 Loss: 3.333770\n",
      "\tTraining batch 1408 Loss: 3.324246\n",
      "\tTraining batch 1409 Loss: 3.335517\n",
      "\tTraining batch 1410 Loss: 3.325870\n",
      "\tTraining batch 1411 Loss: 3.346022\n",
      "\tTraining batch 1412 Loss: 3.328475\n",
      "\tTraining batch 1413 Loss: 3.323155\n",
      "\tTraining batch 1414 Loss: 3.350615\n",
      "\tTraining batch 1415 Loss: 3.324112\n",
      "\tTraining batch 1416 Loss: 3.333704\n",
      "\tTraining batch 1417 Loss: 3.355402\n",
      "\tTraining batch 1418 Loss: 3.326986\n",
      "\tTraining batch 1419 Loss: 3.309684\n",
      "\tTraining batch 1420 Loss: 3.340403\n",
      "\tTraining batch 1421 Loss: 3.335333\n",
      "\tTraining batch 1422 Loss: 3.340837\n",
      "\tTraining batch 1423 Loss: 3.344269\n",
      "\tTraining batch 1424 Loss: 3.332053\n",
      "\tTraining batch 1425 Loss: 3.341301\n",
      "\tTraining batch 1426 Loss: 3.339205\n",
      "\tTraining batch 1427 Loss: 3.338691\n",
      "\tTraining batch 1428 Loss: 3.329761\n",
      "\tTraining batch 1429 Loss: 3.359241\n",
      "\tTraining batch 1430 Loss: 3.337979\n",
      "\tTraining batch 1431 Loss: 3.332232\n",
      "\tTraining batch 1432 Loss: 3.333020\n",
      "\tTraining batch 1433 Loss: 3.343195\n",
      "\tTraining batch 1434 Loss: 3.333478\n",
      "\tTraining batch 1435 Loss: 3.334639\n",
      "\tTraining batch 1436 Loss: 3.323224\n",
      "\tTraining batch 1437 Loss: 3.352590\n",
      "\tTraining batch 1438 Loss: 3.324128\n",
      "\tTraining batch 1439 Loss: 3.340839\n",
      "\tTraining batch 1440 Loss: 3.325889\n",
      "\tTraining batch 1441 Loss: 3.337690\n",
      "\tTraining batch 1442 Loss: 3.339504\n",
      "\tTraining batch 1443 Loss: 3.334929\n",
      "\tTraining batch 1444 Loss: 3.340364\n",
      "\tTraining batch 1445 Loss: 3.326658\n",
      "\tTraining batch 1446 Loss: 3.345783\n",
      "\tTraining batch 1447 Loss: 3.337558\n",
      "\tTraining batch 1448 Loss: 3.349155\n",
      "\tTraining batch 1449 Loss: 3.314054\n",
      "\tTraining batch 1450 Loss: 3.337513\n",
      "\tTraining batch 1451 Loss: 3.327821\n",
      "\tTraining batch 1452 Loss: 3.324467\n",
      "\tTraining batch 1453 Loss: 3.334487\n",
      "\tTraining batch 1454 Loss: 3.344520\n",
      "\tTraining batch 1455 Loss: 3.345272\n",
      "\tTraining batch 1456 Loss: 3.327504\n",
      "\tTraining batch 1457 Loss: 3.338765\n",
      "\tTraining batch 1458 Loss: 3.333357\n",
      "\tTraining batch 1459 Loss: 3.328748\n",
      "\tTraining batch 1460 Loss: 3.335377\n",
      "\tTraining batch 1461 Loss: 3.313161\n",
      "\tTraining batch 1462 Loss: 3.334210\n",
      "\tTraining batch 1463 Loss: 3.321125\n",
      "\tTraining batch 1464 Loss: 3.319033\n",
      "\tTraining batch 1465 Loss: 3.314631\n",
      "\tTraining batch 1466 Loss: 3.339478\n",
      "\tTraining batch 1467 Loss: 3.335281\n",
      "\tTraining batch 1468 Loss: 3.328762\n",
      "\tTraining batch 1469 Loss: 3.341703\n",
      "\tTraining batch 1470 Loss: 3.327327\n",
      "\tTraining batch 1471 Loss: 3.345539\n",
      "\tTraining batch 1472 Loss: 3.337723\n",
      "\tTraining batch 1473 Loss: 3.337859\n",
      "\tTraining batch 1474 Loss: 3.316322\n",
      "\tTraining batch 1475 Loss: 3.319960\n",
      "\tTraining batch 1476 Loss: 3.327851\n",
      "\tTraining batch 1477 Loss: 3.327636\n",
      "\tTraining batch 1478 Loss: 3.334913\n",
      "\tTraining batch 1479 Loss: 3.328746\n",
      "\tTraining batch 1480 Loss: 3.334342\n",
      "\tTraining batch 1481 Loss: 3.337746\n",
      "\tTraining batch 1482 Loss: 3.341507\n",
      "\tTraining batch 1483 Loss: 3.337326\n",
      "\tTraining batch 1484 Loss: 3.327940\n",
      "\tTraining batch 1485 Loss: 3.339594\n",
      "\tTraining batch 1486 Loss: 3.352985\n",
      "\tTraining batch 1487 Loss: 3.332639\n",
      "\tTraining batch 1488 Loss: 3.340179\n",
      "\tTraining batch 1489 Loss: 3.333184\n",
      "\tTraining batch 1490 Loss: 3.338760\n",
      "\tTraining batch 1491 Loss: 3.344574\n",
      "\tTraining batch 1492 Loss: 3.340242\n",
      "\tTraining batch 1493 Loss: 3.344389\n",
      "\tTraining batch 1494 Loss: 3.336597\n",
      "\tTraining batch 1495 Loss: 3.339896\n",
      "\tTraining batch 1496 Loss: 3.327749\n",
      "\tTraining batch 1497 Loss: 3.330446\n",
      "\tTraining batch 1498 Loss: 3.357401\n",
      "\tTraining batch 1499 Loss: 3.323962\n",
      "\tTraining batch 1500 Loss: 3.346750\n",
      "\tTraining batch 1501 Loss: 3.345846\n",
      "\tTraining batch 1502 Loss: 3.325855\n",
      "\tTraining batch 1503 Loss: 3.338123\n",
      "\tTraining batch 1504 Loss: 3.325763\n",
      "\tTraining batch 1505 Loss: 3.344258\n",
      "\tTraining batch 1506 Loss: 3.353927\n",
      "\tTraining batch 1507 Loss: 3.335699\n",
      "\tTraining batch 1508 Loss: 3.334010\n",
      "\tTraining batch 1509 Loss: 3.318076\n",
      "\tTraining batch 1510 Loss: 3.303626\n",
      "\tTraining batch 1511 Loss: 3.329038\n",
      "\tTraining batch 1512 Loss: 3.328413\n",
      "\tTraining batch 1513 Loss: 3.331669\n",
      "\tTraining batch 1514 Loss: 3.357471\n",
      "\tTraining batch 1515 Loss: 3.347208\n",
      "\tTraining batch 1516 Loss: 3.330632\n",
      "\tTraining batch 1517 Loss: 3.326619\n",
      "\tTraining batch 1518 Loss: 3.335777\n",
      "\tTraining batch 1519 Loss: 3.334954\n",
      "\tTraining batch 1520 Loss: 3.345347\n",
      "\tTraining batch 1521 Loss: 3.339608\n",
      "\tTraining batch 1522 Loss: 3.345469\n",
      "\tTraining batch 1523 Loss: 3.343778\n",
      "\tTraining batch 1524 Loss: 3.315274\n",
      "\tTraining batch 1525 Loss: 3.332907\n",
      "\tTraining batch 1526 Loss: 3.344830\n",
      "\tTraining batch 1527 Loss: 3.348042\n",
      "\tTraining batch 1528 Loss: 3.326196\n",
      "\tTraining batch 1529 Loss: 3.328928\n",
      "\tTraining batch 1530 Loss: 3.322625\n",
      "\tTraining batch 1531 Loss: 3.340506\n",
      "\tTraining batch 1532 Loss: 3.312179\n",
      "\tTraining batch 1533 Loss: 3.333730\n",
      "\tTraining batch 1534 Loss: 3.325329\n",
      "\tTraining batch 1535 Loss: 3.331418\n",
      "\tTraining batch 1536 Loss: 3.326828\n",
      "\tTraining batch 1537 Loss: 3.353595\n",
      "\tTraining batch 1538 Loss: 3.325274\n",
      "\tTraining batch 1539 Loss: 3.337163\n",
      "\tTraining batch 1540 Loss: 3.331142\n",
      "\tTraining batch 1541 Loss: 3.330209\n",
      "\tTraining batch 1542 Loss: 3.335643\n",
      "\tTraining batch 1543 Loss: 3.344075\n",
      "\tTraining batch 1544 Loss: 3.330360\n",
      "\tTraining batch 1545 Loss: 3.370117\n",
      "\tTraining batch 1546 Loss: 3.321023\n",
      "\tTraining batch 1547 Loss: 3.345287\n",
      "\tTraining batch 1548 Loss: 3.320036\n",
      "\tTraining batch 1549 Loss: 3.326468\n",
      "\tTraining batch 1550 Loss: 3.348894\n",
      "\tTraining batch 1551 Loss: 3.334390\n",
      "\tTraining batch 1552 Loss: 3.344670\n",
      "\tTraining batch 1553 Loss: 3.350965\n",
      "\tTraining batch 1554 Loss: 3.337630\n",
      "\tTraining batch 1555 Loss: 3.347146\n",
      "\tTraining batch 1556 Loss: 3.342766\n",
      "\tTraining batch 1557 Loss: 3.346723\n",
      "\tTraining batch 1558 Loss: 3.350722\n",
      "\tTraining batch 1559 Loss: 3.333931\n",
      "\tTraining batch 1560 Loss: 3.331642\n",
      "\tTraining batch 1561 Loss: 3.345274\n",
      "\tTraining batch 1562 Loss: 3.316158\n",
      "\tTraining batch 1563 Loss: 3.329358\n",
      "\tTraining batch 1564 Loss: 3.333994\n",
      "\tTraining batch 1565 Loss: 3.332149\n",
      "\tTraining batch 1566 Loss: 3.329274\n",
      "\tTraining batch 1567 Loss: 3.329078\n",
      "\tTraining batch 1568 Loss: 3.330656\n",
      "\tTraining batch 1569 Loss: 3.347540\n",
      "\tTraining batch 1570 Loss: 3.361598\n",
      "\tTraining batch 1571 Loss: 3.329132\n",
      "\tTraining batch 1572 Loss: 3.348487\n",
      "\tTraining batch 1573 Loss: 3.342993\n",
      "\tTraining batch 1574 Loss: 3.333458\n",
      "\tTraining batch 1575 Loss: 3.330632\n",
      "\tTraining batch 1576 Loss: 3.331526\n",
      "\tTraining batch 1577 Loss: 3.319969\n",
      "\tTraining batch 1578 Loss: 3.355486\n",
      "\tTraining batch 1579 Loss: 3.335665\n",
      "\tTraining batch 1580 Loss: 3.328832\n",
      "\tTraining batch 1581 Loss: 3.325037\n",
      "\tTraining batch 1582 Loss: 3.346958\n",
      "\tTraining batch 1583 Loss: 3.332410\n",
      "\tTraining batch 1584 Loss: 3.332316\n",
      "\tTraining batch 1585 Loss: 3.342669\n",
      "\tTraining batch 1586 Loss: 3.346977\n",
      "\tTraining batch 1587 Loss: 3.347775\n",
      "\tTraining batch 1588 Loss: 3.326230\n",
      "\tTraining batch 1589 Loss: 3.332552\n",
      "\tTraining batch 1590 Loss: 3.328357\n",
      "\tTraining batch 1591 Loss: 3.331954\n",
      "\tTraining batch 1592 Loss: 3.340110\n",
      "\tTraining batch 1593 Loss: 3.336809\n",
      "\tTraining batch 1594 Loss: 3.320414\n",
      "\tTraining batch 1595 Loss: 3.335058\n",
      "\tTraining batch 1596 Loss: 3.337577\n",
      "\tTraining batch 1597 Loss: 3.331530\n",
      "\tTraining batch 1598 Loss: 3.326750\n",
      "\tTraining batch 1599 Loss: 3.330208\n",
      "\tTraining batch 1600 Loss: 3.340800\n",
      "\tTraining batch 1601 Loss: 3.327317\n",
      "\tTraining batch 1602 Loss: 3.322523\n",
      "\tTraining batch 1603 Loss: 3.332029\n",
      "\tTraining batch 1604 Loss: 3.320033\n",
      "\tTraining batch 1605 Loss: 3.338095\n",
      "\tTraining batch 1606 Loss: 3.321888\n",
      "\tTraining batch 1607 Loss: 3.320689\n",
      "\tTraining batch 1608 Loss: 3.335607\n",
      "\tTraining batch 1609 Loss: 3.332644\n",
      "\tTraining batch 1610 Loss: 3.340686\n",
      "\tTraining batch 1611 Loss: 3.315009\n",
      "\tTraining batch 1612 Loss: 3.337698\n",
      "\tTraining batch 1613 Loss: 3.334471\n",
      "\tTraining batch 1614 Loss: 3.320482\n",
      "\tTraining batch 1615 Loss: 3.346807\n",
      "\tTraining batch 1616 Loss: 3.343660\n",
      "\tTraining batch 1617 Loss: 3.328691\n",
      "\tTraining batch 1618 Loss: 3.319410\n",
      "\tTraining batch 1619 Loss: 3.350279\n",
      "\tTraining batch 1620 Loss: 3.347606\n",
      "\tTraining batch 1621 Loss: 3.325393\n",
      "\tTraining batch 1622 Loss: 3.341471\n",
      "\tTraining batch 1623 Loss: 3.341007\n",
      "\tTraining batch 1624 Loss: 3.342211\n",
      "\tTraining batch 1625 Loss: 3.351945\n",
      "\tTraining batch 1626 Loss: 3.351182\n",
      "\tTraining batch 1627 Loss: 3.353307\n",
      "\tTraining batch 1628 Loss: 3.334756\n",
      "\tTraining batch 1629 Loss: 3.333010\n",
      "\tTraining batch 1630 Loss: 3.344588\n",
      "\tTraining batch 1631 Loss: 3.339804\n",
      "\tTraining batch 1632 Loss: 3.333413\n",
      "\tTraining batch 1633 Loss: 3.340681\n",
      "\tTraining batch 1634 Loss: 3.348332\n",
      "\tTraining batch 1635 Loss: 3.327042\n",
      "\tTraining batch 1636 Loss: 3.328974\n",
      "\tTraining batch 1637 Loss: 3.340581\n",
      "\tTraining batch 1638 Loss: 3.332566\n",
      "\tTraining batch 1639 Loss: 3.330124\n",
      "\tTraining batch 1640 Loss: 3.329234\n",
      "\tTraining batch 1641 Loss: 3.346397\n",
      "\tTraining batch 1642 Loss: 3.348608\n",
      "\tTraining batch 1643 Loss: 3.327154\n",
      "\tTraining batch 1644 Loss: 3.329878\n",
      "\tTraining batch 1645 Loss: 3.340185\n",
      "\tTraining batch 1646 Loss: 3.330878\n",
      "\tTraining batch 1647 Loss: 3.332287\n",
      "\tTraining batch 1648 Loss: 3.325153\n",
      "\tTraining batch 1649 Loss: 3.325387\n",
      "\tTraining batch 1650 Loss: 3.328660\n",
      "\tTraining batch 1651 Loss: 3.324439\n",
      "\tTraining batch 1652 Loss: 3.341215\n",
      "\tTraining batch 1653 Loss: 3.332786\n",
      "\tTraining batch 1654 Loss: 3.349414\n",
      "\tTraining batch 1655 Loss: 3.332834\n",
      "\tTraining batch 1656 Loss: 3.328325\n",
      "\tTraining batch 1657 Loss: 3.325507\n",
      "\tTraining batch 1658 Loss: 3.323010\n",
      "\tTraining batch 1659 Loss: 3.340389\n",
      "\tTraining batch 1660 Loss: 3.333078\n",
      "\tTraining batch 1661 Loss: 3.341112\n",
      "\tTraining batch 1662 Loss: 3.307740\n",
      "\tTraining batch 1663 Loss: 3.339321\n",
      "\tTraining batch 1664 Loss: 3.364478\n",
      "\tTraining batch 1665 Loss: 3.323940\n",
      "\tTraining batch 1666 Loss: 3.332654\n",
      "\tTraining batch 1667 Loss: 3.342283\n",
      "\tTraining batch 1668 Loss: 3.314829\n",
      "\tTraining batch 1669 Loss: 3.335747\n",
      "\tTraining batch 1670 Loss: 3.331619\n",
      "\tTraining batch 1671 Loss: 3.341714\n",
      "\tTraining batch 1672 Loss: 3.356225\n",
      "\tTraining batch 1673 Loss: 3.333684\n",
      "\tTraining batch 1674 Loss: 3.318705\n",
      "\tTraining batch 1675 Loss: 3.332609\n",
      "\tTraining batch 1676 Loss: 3.351185\n",
      "\tTraining batch 1677 Loss: 3.329281\n",
      "\tTraining batch 1678 Loss: 3.329994\n",
      "\tTraining batch 1679 Loss: 3.324182\n",
      "\tTraining batch 1680 Loss: 3.333965\n",
      "\tTraining batch 1681 Loss: 3.321617\n",
      "\tTraining batch 1682 Loss: 3.333645\n",
      "\tTraining batch 1683 Loss: 3.327830\n",
      "\tTraining batch 1684 Loss: 3.334900\n",
      "\tTraining batch 1685 Loss: 3.313206\n",
      "\tTraining batch 1686 Loss: 3.341506\n",
      "\tTraining batch 1687 Loss: 3.352684\n",
      "\tTraining batch 1688 Loss: 3.320865\n",
      "\tTraining batch 1689 Loss: 3.336442\n",
      "\tTraining batch 1690 Loss: 3.339569\n",
      "\tTraining batch 1691 Loss: 3.337231\n",
      "\tTraining batch 1692 Loss: 3.301345\n",
      "\tTraining batch 1693 Loss: 3.329749\n",
      "\tTraining batch 1694 Loss: 3.327474\n",
      "\tTraining batch 1695 Loss: 3.326702\n",
      "\tTraining batch 1696 Loss: 3.337595\n",
      "\tTraining batch 1697 Loss: 3.335418\n",
      "\tTraining batch 1698 Loss: 3.341339\n",
      "\tTraining batch 1699 Loss: 3.332739\n",
      "\tTraining batch 1700 Loss: 3.334492\n",
      "\tTraining batch 1701 Loss: 3.340731\n",
      "\tTraining batch 1702 Loss: 3.363954\n",
      "\tTraining batch 1703 Loss: 3.345708\n",
      "\tTraining batch 1704 Loss: 3.316721\n",
      "\tTraining batch 1705 Loss: 3.338655\n",
      "\tTraining batch 1706 Loss: 3.332620\n",
      "\tTraining batch 1707 Loss: 3.338779\n",
      "\tTraining batch 1708 Loss: 3.336923\n",
      "\tTraining batch 1709 Loss: 3.330357\n",
      "\tTraining batch 1710 Loss: 3.339386\n",
      "\tTraining batch 1711 Loss: 3.326815\n",
      "\tTraining batch 1712 Loss: 3.344320\n",
      "\tTraining batch 1713 Loss: 3.354455\n",
      "\tTraining batch 1714 Loss: 3.322911\n",
      "\tTraining batch 1715 Loss: 3.326955\n",
      "\tTraining batch 1716 Loss: 3.311867\n",
      "\tTraining batch 1717 Loss: 3.332438\n",
      "\tTraining batch 1718 Loss: 3.349156\n",
      "\tTraining batch 1719 Loss: 3.341967\n",
      "\tTraining batch 1720 Loss: 3.341487\n",
      "\tTraining batch 1721 Loss: 3.352635\n",
      "\tTraining batch 1722 Loss: 3.334748\n",
      "\tTraining batch 1723 Loss: 3.332153\n",
      "\tTraining batch 1724 Loss: 3.336871\n",
      "\tTraining batch 1725 Loss: 3.333296\n",
      "\tTraining batch 1726 Loss: 3.314902\n",
      "\tTraining batch 1727 Loss: 3.341128\n",
      "\tTraining batch 1728 Loss: 3.321220\n",
      "\tTraining batch 1729 Loss: 3.324332\n",
      "\tTraining batch 1730 Loss: 3.335588\n",
      "\tTraining batch 1731 Loss: 3.338235\n",
      "\tTraining batch 1732 Loss: 3.321729\n",
      "\tTraining batch 1733 Loss: 3.338446\n",
      "\tTraining batch 1734 Loss: 3.317667\n",
      "\tTraining batch 1735 Loss: 3.334612\n",
      "\tTraining batch 1736 Loss: 3.342027\n",
      "\tTraining batch 1737 Loss: 3.334479\n",
      "\tTraining batch 1738 Loss: 3.347101\n",
      "\tTraining batch 1739 Loss: 3.322258\n",
      "\tTraining batch 1740 Loss: 3.334719\n",
      "\tTraining batch 1741 Loss: 3.333255\n",
      "\tTraining batch 1742 Loss: 3.343479\n",
      "\tTraining batch 1743 Loss: 3.348225\n",
      "\tTraining batch 1744 Loss: 3.313275\n",
      "\tTraining batch 1745 Loss: 3.328288\n",
      "\tTraining batch 1746 Loss: 3.356842\n",
      "\tTraining batch 1747 Loss: 3.343982\n",
      "\tTraining batch 1748 Loss: 3.365056\n",
      "\tTraining batch 1749 Loss: 3.342081\n",
      "\tTraining batch 1750 Loss: 3.335249\n",
      "\tTraining batch 1751 Loss: 3.340688\n",
      "\tTraining batch 1752 Loss: 3.334504\n",
      "\tTraining batch 1753 Loss: 3.324807\n",
      "\tTraining batch 1754 Loss: 3.347600\n",
      "\tTraining batch 1755 Loss: 3.340652\n",
      "\tTraining batch 1756 Loss: 3.337438\n",
      "\tTraining batch 1757 Loss: 3.327926\n",
      "\tTraining batch 1758 Loss: 3.353668\n",
      "\tTraining batch 1759 Loss: 3.323078\n",
      "\tTraining batch 1760 Loss: 3.326628\n",
      "\tTraining batch 1761 Loss: 3.332185\n",
      "\tTraining batch 1762 Loss: 3.351202\n",
      "\tTraining batch 1763 Loss: 3.339737\n",
      "\tTraining batch 1764 Loss: 3.332200\n",
      "\tTraining batch 1765 Loss: 3.353740\n",
      "\tTraining batch 1766 Loss: 3.335573\n",
      "\tTraining batch 1767 Loss: 3.304523\n",
      "\tTraining batch 1768 Loss: 3.317880\n",
      "\tTraining batch 1769 Loss: 3.322244\n",
      "\tTraining batch 1770 Loss: 3.335313\n",
      "\tTraining batch 1771 Loss: 3.344743\n",
      "\tTraining batch 1772 Loss: 3.330592\n",
      "\tTraining batch 1773 Loss: 3.348260\n",
      "\tTraining batch 1774 Loss: 3.343659\n",
      "\tTraining batch 1775 Loss: 3.325040\n",
      "\tTraining batch 1776 Loss: 3.323944\n",
      "\tTraining batch 1777 Loss: 3.348620\n",
      "\tTraining batch 1778 Loss: 3.340336\n",
      "\tTraining batch 1779 Loss: 3.316193\n",
      "\tTraining batch 1780 Loss: 3.350110\n",
      "\tTraining batch 1781 Loss: 3.345415\n",
      "\tTraining batch 1782 Loss: 3.336634\n",
      "\tTraining batch 1783 Loss: 3.340320\n",
      "\tTraining batch 1784 Loss: 3.311767\n",
      "\tTraining batch 1785 Loss: 3.334396\n",
      "\tTraining batch 1786 Loss: 3.340152\n",
      "\tTraining batch 1787 Loss: 3.333062\n",
      "\tTraining batch 1788 Loss: 3.345219\n",
      "\tTraining batch 1789 Loss: 3.320273\n",
      "\tTraining batch 1790 Loss: 3.335209\n",
      "\tTraining batch 1791 Loss: 3.344909\n",
      "\tTraining batch 1792 Loss: 3.331566\n",
      "\tTraining batch 1793 Loss: 3.330471\n",
      "\tTraining batch 1794 Loss: 3.325379\n",
      "\tTraining batch 1795 Loss: 3.328080\n",
      "\tTraining batch 1796 Loss: 3.322950\n",
      "\tTraining batch 1797 Loss: 3.335532\n",
      "\tTraining batch 1798 Loss: 3.338278\n",
      "\tTraining batch 1799 Loss: 3.362047\n",
      "\tTraining batch 1800 Loss: 3.328611\n",
      "\tTraining batch 1801 Loss: 3.335855\n",
      "\tTraining batch 1802 Loss: 3.344279\n",
      "\tTraining batch 1803 Loss: 3.347218\n",
      "\tTraining batch 1804 Loss: 3.338773\n",
      "\tTraining batch 1805 Loss: 3.338137\n",
      "\tTraining batch 1806 Loss: 3.325216\n",
      "\tTraining batch 1807 Loss: 3.337801\n",
      "\tTraining batch 1808 Loss: 3.330471\n",
      "\tTraining batch 1809 Loss: 3.336108\n",
      "\tTraining batch 1810 Loss: 3.345724\n",
      "\tTraining batch 1811 Loss: 3.325480\n",
      "\tTraining batch 1812 Loss: 3.344091\n",
      "\tTraining batch 1813 Loss: 3.345634\n",
      "\tTraining batch 1814 Loss: 3.324425\n",
      "\tTraining batch 1815 Loss: 3.340906\n",
      "\tTraining batch 1816 Loss: 3.356632\n",
      "\tTraining batch 1817 Loss: 3.349014\n",
      "\tTraining batch 1818 Loss: 3.337308\n",
      "\tTraining batch 1819 Loss: 3.320462\n",
      "\tTraining batch 1820 Loss: 3.339361\n",
      "\tTraining batch 1821 Loss: 3.333404\n",
      "\tTraining batch 1822 Loss: 3.330998\n",
      "\tTraining batch 1823 Loss: 3.337415\n",
      "\tTraining batch 1824 Loss: 3.328793\n",
      "\tTraining batch 1825 Loss: 3.331753\n",
      "\tTraining batch 1826 Loss: 3.335959\n",
      "\tTraining batch 1827 Loss: 3.347533\n",
      "\tTraining batch 1828 Loss: 3.326610\n",
      "\tTraining batch 1829 Loss: 3.340095\n",
      "\tTraining batch 1830 Loss: 3.339388\n",
      "\tTraining batch 1831 Loss: 3.319798\n",
      "\tTraining batch 1832 Loss: 3.349067\n",
      "\tTraining batch 1833 Loss: 3.329333\n",
      "\tTraining batch 1834 Loss: 3.343204\n",
      "\tTraining batch 1835 Loss: 3.323238\n",
      "\tTraining batch 1836 Loss: 3.339959\n",
      "\tTraining batch 1837 Loss: 3.322652\n",
      "\tTraining batch 1838 Loss: 3.355934\n",
      "Training set: Average loss: 3.334620\n",
      "Validation set: Average loss: 3.334421, Accuracy: 884/25201 (4%)\n",
      "\n",
      "Epoch:  3\n",
      "\tTraining batch 1 Loss: 3.311710\n",
      "\tTraining batch 2 Loss: 3.341417\n",
      "\tTraining batch 3 Loss: 3.306092\n",
      "\tTraining batch 4 Loss: 3.337140\n",
      "\tTraining batch 5 Loss: 3.350358\n",
      "\tTraining batch 6 Loss: 3.323738\n",
      "\tTraining batch 7 Loss: 3.336832\n",
      "\tTraining batch 8 Loss: 3.330956\n",
      "\tTraining batch 9 Loss: 3.351397\n",
      "\tTraining batch 10 Loss: 3.338183\n",
      "\tTraining batch 11 Loss: 3.326545\n",
      "\tTraining batch 12 Loss: 3.346300\n",
      "\tTraining batch 13 Loss: 3.324310\n",
      "\tTraining batch 14 Loss: 3.345027\n",
      "\tTraining batch 15 Loss: 3.321791\n",
      "\tTraining batch 16 Loss: 3.324538\n",
      "\tTraining batch 17 Loss: 3.332967\n",
      "\tTraining batch 18 Loss: 3.334648\n",
      "\tTraining batch 19 Loss: 3.342128\n",
      "\tTraining batch 20 Loss: 3.344653\n",
      "\tTraining batch 21 Loss: 3.351961\n",
      "\tTraining batch 22 Loss: 3.326602\n",
      "\tTraining batch 23 Loss: 3.340424\n",
      "\tTraining batch 24 Loss: 3.344303\n",
      "\tTraining batch 25 Loss: 3.319521\n",
      "\tTraining batch 26 Loss: 3.338317\n",
      "\tTraining batch 27 Loss: 3.337793\n",
      "\tTraining batch 28 Loss: 3.337763\n",
      "\tTraining batch 29 Loss: 3.321769\n",
      "\tTraining batch 30 Loss: 3.334140\n",
      "\tTraining batch 31 Loss: 3.332703\n",
      "\tTraining batch 32 Loss: 3.342176\n",
      "\tTraining batch 33 Loss: 3.338813\n",
      "\tTraining batch 34 Loss: 3.320023\n",
      "\tTraining batch 35 Loss: 3.365609\n",
      "\tTraining batch 36 Loss: 3.355363\n",
      "\tTraining batch 37 Loss: 3.316063\n",
      "\tTraining batch 38 Loss: 3.319564\n",
      "\tTraining batch 39 Loss: 3.351246\n",
      "\tTraining batch 40 Loss: 3.333161\n",
      "\tTraining batch 41 Loss: 3.347565\n",
      "\tTraining batch 42 Loss: 3.329931\n",
      "\tTraining batch 43 Loss: 3.336159\n",
      "\tTraining batch 44 Loss: 3.313384\n",
      "\tTraining batch 45 Loss: 3.340892\n",
      "\tTraining batch 46 Loss: 3.359919\n",
      "\tTraining batch 47 Loss: 3.333248\n",
      "\tTraining batch 48 Loss: 3.334085\n",
      "\tTraining batch 49 Loss: 3.336861\n",
      "\tTraining batch 50 Loss: 3.326978\n",
      "\tTraining batch 51 Loss: 3.335339\n",
      "\tTraining batch 52 Loss: 3.327956\n",
      "\tTraining batch 53 Loss: 3.343036\n",
      "\tTraining batch 54 Loss: 3.317476\n",
      "\tTraining batch 55 Loss: 3.330033\n",
      "\tTraining batch 56 Loss: 3.338104\n",
      "\tTraining batch 57 Loss: 3.326705\n",
      "\tTraining batch 58 Loss: 3.320900\n",
      "\tTraining batch 59 Loss: 3.328580\n",
      "\tTraining batch 60 Loss: 3.353030\n",
      "\tTraining batch 61 Loss: 3.336915\n",
      "\tTraining batch 62 Loss: 3.320141\n",
      "\tTraining batch 63 Loss: 3.326327\n",
      "\tTraining batch 64 Loss: 3.328206\n",
      "\tTraining batch 65 Loss: 3.346572\n",
      "\tTraining batch 66 Loss: 3.342054\n",
      "\tTraining batch 67 Loss: 3.327605\n",
      "\tTraining batch 68 Loss: 3.341271\n",
      "\tTraining batch 69 Loss: 3.331654\n",
      "\tTraining batch 70 Loss: 3.331021\n",
      "\tTraining batch 71 Loss: 3.329827\n",
      "\tTraining batch 72 Loss: 3.321909\n",
      "\tTraining batch 73 Loss: 3.330956\n",
      "\tTraining batch 74 Loss: 3.317002\n",
      "\tTraining batch 75 Loss: 3.354293\n",
      "\tTraining batch 76 Loss: 3.345724\n",
      "\tTraining batch 77 Loss: 3.344647\n",
      "\tTraining batch 78 Loss: 3.324763\n",
      "\tTraining batch 79 Loss: 3.335487\n",
      "\tTraining batch 80 Loss: 3.342031\n",
      "\tTraining batch 81 Loss: 3.336333\n",
      "\tTraining batch 82 Loss: 3.328501\n",
      "\tTraining batch 83 Loss: 3.349692\n",
      "\tTraining batch 84 Loss: 3.348940\n",
      "\tTraining batch 85 Loss: 3.326374\n",
      "\tTraining batch 86 Loss: 3.350457\n",
      "\tTraining batch 87 Loss: 3.333565\n",
      "\tTraining batch 88 Loss: 3.312498\n",
      "\tTraining batch 89 Loss: 3.312975\n",
      "\tTraining batch 90 Loss: 3.335290\n",
      "\tTraining batch 91 Loss: 3.325791\n",
      "\tTraining batch 92 Loss: 3.341042\n",
      "\tTraining batch 93 Loss: 3.330869\n",
      "\tTraining batch 94 Loss: 3.330772\n",
      "\tTraining batch 95 Loss: 3.329816\n",
      "\tTraining batch 96 Loss: 3.323946\n",
      "\tTraining batch 97 Loss: 3.333380\n",
      "\tTraining batch 98 Loss: 3.316047\n",
      "\tTraining batch 99 Loss: 3.361513\n",
      "\tTraining batch 100 Loss: 3.325593\n",
      "\tTraining batch 101 Loss: 3.333578\n",
      "\tTraining batch 102 Loss: 3.344810\n",
      "\tTraining batch 103 Loss: 3.327766\n",
      "\tTraining batch 104 Loss: 3.328633\n",
      "\tTraining batch 105 Loss: 3.354550\n",
      "\tTraining batch 106 Loss: 3.331075\n",
      "\tTraining batch 107 Loss: 3.330542\n",
      "\tTraining batch 108 Loss: 3.340955\n",
      "\tTraining batch 109 Loss: 3.318195\n",
      "\tTraining batch 110 Loss: 3.350753\n",
      "\tTraining batch 111 Loss: 3.344716\n",
      "\tTraining batch 112 Loss: 3.329283\n",
      "\tTraining batch 113 Loss: 3.322965\n",
      "\tTraining batch 114 Loss: 3.344217\n",
      "\tTraining batch 115 Loss: 3.322362\n",
      "\tTraining batch 116 Loss: 3.343106\n",
      "\tTraining batch 117 Loss: 3.322596\n",
      "\tTraining batch 118 Loss: 3.319554\n",
      "\tTraining batch 119 Loss: 3.324039\n",
      "\tTraining batch 120 Loss: 3.336615\n",
      "\tTraining batch 121 Loss: 3.344771\n",
      "\tTraining batch 122 Loss: 3.330834\n",
      "\tTraining batch 123 Loss: 3.315912\n",
      "\tTraining batch 124 Loss: 3.333015\n",
      "\tTraining batch 125 Loss: 3.342643\n",
      "\tTraining batch 126 Loss: 3.319967\n",
      "\tTraining batch 127 Loss: 3.342275\n",
      "\tTraining batch 128 Loss: 3.298647\n",
      "\tTraining batch 129 Loss: 3.356438\n",
      "\tTraining batch 130 Loss: 3.318336\n",
      "\tTraining batch 131 Loss: 3.323505\n",
      "\tTraining batch 132 Loss: 3.356476\n",
      "\tTraining batch 133 Loss: 3.339583\n",
      "\tTraining batch 134 Loss: 3.346808\n",
      "\tTraining batch 135 Loss: 3.334470\n",
      "\tTraining batch 136 Loss: 3.360006\n",
      "\tTraining batch 137 Loss: 3.330821\n",
      "\tTraining batch 138 Loss: 3.329427\n",
      "\tTraining batch 139 Loss: 3.336355\n",
      "\tTraining batch 140 Loss: 3.323890\n",
      "\tTraining batch 141 Loss: 3.326874\n",
      "\tTraining batch 142 Loss: 3.337022\n",
      "\tTraining batch 143 Loss: 3.338738\n",
      "\tTraining batch 144 Loss: 3.340106\n",
      "\tTraining batch 145 Loss: 3.338270\n",
      "\tTraining batch 146 Loss: 3.311491\n",
      "\tTraining batch 147 Loss: 3.358986\n",
      "\tTraining batch 148 Loss: 3.335164\n",
      "\tTraining batch 149 Loss: 3.329609\n",
      "\tTraining batch 150 Loss: 3.330486\n",
      "\tTraining batch 151 Loss: 3.325135\n",
      "\tTraining batch 152 Loss: 3.311874\n",
      "\tTraining batch 153 Loss: 3.334576\n",
      "\tTraining batch 154 Loss: 3.318278\n",
      "\tTraining batch 155 Loss: 3.343279\n",
      "\tTraining batch 156 Loss: 3.328615\n",
      "\tTraining batch 157 Loss: 3.320884\n",
      "\tTraining batch 158 Loss: 3.343221\n",
      "\tTraining batch 159 Loss: 3.317151\n",
      "\tTraining batch 160 Loss: 3.336545\n",
      "\tTraining batch 161 Loss: 3.317515\n",
      "\tTraining batch 162 Loss: 3.322281\n",
      "\tTraining batch 163 Loss: 3.321793\n",
      "\tTraining batch 164 Loss: 3.317848\n",
      "\tTraining batch 165 Loss: 3.327278\n",
      "\tTraining batch 166 Loss: 3.323423\n",
      "\tTraining batch 167 Loss: 3.341235\n",
      "\tTraining batch 168 Loss: 3.359578\n",
      "\tTraining batch 169 Loss: 3.352954\n",
      "\tTraining batch 170 Loss: 3.330865\n",
      "\tTraining batch 171 Loss: 3.326450\n",
      "\tTraining batch 172 Loss: 3.328339\n",
      "\tTraining batch 173 Loss: 3.348666\n",
      "\tTraining batch 174 Loss: 3.347752\n",
      "\tTraining batch 175 Loss: 3.337450\n",
      "\tTraining batch 176 Loss: 3.352515\n",
      "\tTraining batch 177 Loss: 3.351884\n",
      "\tTraining batch 178 Loss: 3.334727\n",
      "\tTraining batch 179 Loss: 3.319518\n",
      "\tTraining batch 180 Loss: 3.334894\n",
      "\tTraining batch 181 Loss: 3.336587\n",
      "\tTraining batch 182 Loss: 3.331392\n",
      "\tTraining batch 183 Loss: 3.346542\n",
      "\tTraining batch 184 Loss: 3.324339\n",
      "\tTraining batch 185 Loss: 3.357362\n",
      "\tTraining batch 186 Loss: 3.338471\n",
      "\tTraining batch 187 Loss: 3.332034\n",
      "\tTraining batch 188 Loss: 3.348276\n",
      "\tTraining batch 189 Loss: 3.325563\n",
      "\tTraining batch 190 Loss: 3.317963\n",
      "\tTraining batch 191 Loss: 3.311240\n",
      "\tTraining batch 192 Loss: 3.334585\n",
      "\tTraining batch 193 Loss: 3.339086\n",
      "\tTraining batch 194 Loss: 3.329942\n",
      "\tTraining batch 195 Loss: 3.339101\n",
      "\tTraining batch 196 Loss: 3.318922\n",
      "\tTraining batch 197 Loss: 3.317786\n",
      "\tTraining batch 198 Loss: 3.344034\n",
      "\tTraining batch 199 Loss: 3.326768\n",
      "\tTraining batch 200 Loss: 3.319197\n",
      "\tTraining batch 201 Loss: 3.341382\n",
      "\tTraining batch 202 Loss: 3.331159\n",
      "\tTraining batch 203 Loss: 3.323035\n",
      "\tTraining batch 204 Loss: 3.326032\n",
      "\tTraining batch 205 Loss: 3.351867\n",
      "\tTraining batch 206 Loss: 3.340209\n",
      "\tTraining batch 207 Loss: 3.323705\n",
      "\tTraining batch 208 Loss: 3.339167\n",
      "\tTraining batch 209 Loss: 3.354361\n",
      "\tTraining batch 210 Loss: 3.334137\n",
      "\tTraining batch 211 Loss: 3.333699\n",
      "\tTraining batch 212 Loss: 3.336163\n",
      "\tTraining batch 213 Loss: 3.322148\n",
      "\tTraining batch 214 Loss: 3.331101\n",
      "\tTraining batch 215 Loss: 3.333062\n",
      "\tTraining batch 216 Loss: 3.340567\n",
      "\tTraining batch 217 Loss: 3.337183\n",
      "\tTraining batch 218 Loss: 3.335759\n",
      "\tTraining batch 219 Loss: 3.344584\n",
      "\tTraining batch 220 Loss: 3.349673\n",
      "\tTraining batch 221 Loss: 3.327925\n",
      "\tTraining batch 222 Loss: 3.339127\n",
      "\tTraining batch 223 Loss: 3.325511\n",
      "\tTraining batch 224 Loss: 3.352101\n",
      "\tTraining batch 225 Loss: 3.324521\n",
      "\tTraining batch 226 Loss: 3.344350\n",
      "\tTraining batch 227 Loss: 3.343327\n",
      "\tTraining batch 228 Loss: 3.357279\n",
      "\tTraining batch 229 Loss: 3.325240\n",
      "\tTraining batch 230 Loss: 3.348017\n",
      "\tTraining batch 231 Loss: 3.325426\n",
      "\tTraining batch 232 Loss: 3.350847\n",
      "\tTraining batch 233 Loss: 3.339752\n",
      "\tTraining batch 234 Loss: 3.344263\n",
      "\tTraining batch 235 Loss: 3.337008\n",
      "\tTraining batch 236 Loss: 3.324438\n",
      "\tTraining batch 237 Loss: 3.337381\n",
      "\tTraining batch 238 Loss: 3.319897\n",
      "\tTraining batch 239 Loss: 3.338685\n",
      "\tTraining batch 240 Loss: 3.321170\n",
      "\tTraining batch 241 Loss: 3.322111\n",
      "\tTraining batch 242 Loss: 3.310357\n",
      "\tTraining batch 243 Loss: 3.311985\n",
      "\tTraining batch 244 Loss: 3.341907\n",
      "\tTraining batch 245 Loss: 3.338586\n",
      "\tTraining batch 246 Loss: 3.316780\n",
      "\tTraining batch 247 Loss: 3.323997\n",
      "\tTraining batch 248 Loss: 3.332148\n",
      "\tTraining batch 249 Loss: 3.347518\n",
      "\tTraining batch 250 Loss: 3.331061\n",
      "\tTraining batch 251 Loss: 3.357515\n",
      "\tTraining batch 252 Loss: 3.327117\n",
      "\tTraining batch 253 Loss: 3.353185\n",
      "\tTraining batch 254 Loss: 3.340592\n",
      "\tTraining batch 255 Loss: 3.351456\n",
      "\tTraining batch 256 Loss: 3.328026\n",
      "\tTraining batch 257 Loss: 3.344911\n",
      "\tTraining batch 258 Loss: 3.358341\n",
      "\tTraining batch 259 Loss: 3.318262\n",
      "\tTraining batch 260 Loss: 3.340018\n",
      "\tTraining batch 261 Loss: 3.327380\n",
      "\tTraining batch 262 Loss: 3.342740\n",
      "\tTraining batch 263 Loss: 3.346340\n",
      "\tTraining batch 264 Loss: 3.348167\n",
      "\tTraining batch 265 Loss: 3.335438\n",
      "\tTraining batch 266 Loss: 3.341699\n",
      "\tTraining batch 267 Loss: 3.329513\n",
      "\tTraining batch 268 Loss: 3.352734\n",
      "\tTraining batch 269 Loss: 3.356070\n",
      "\tTraining batch 270 Loss: 3.318522\n",
      "\tTraining batch 271 Loss: 3.320866\n",
      "\tTraining batch 272 Loss: 3.318347\n",
      "\tTraining batch 273 Loss: 3.327416\n",
      "\tTraining batch 274 Loss: 3.321295\n",
      "\tTraining batch 275 Loss: 3.330182\n",
      "\tTraining batch 276 Loss: 3.345714\n",
      "\tTraining batch 277 Loss: 3.331771\n",
      "\tTraining batch 278 Loss: 3.353281\n",
      "\tTraining batch 279 Loss: 3.325886\n",
      "\tTraining batch 280 Loss: 3.341436\n",
      "\tTraining batch 281 Loss: 3.330141\n",
      "\tTraining batch 282 Loss: 3.339182\n",
      "\tTraining batch 283 Loss: 3.343036\n",
      "\tTraining batch 284 Loss: 3.329577\n",
      "\tTraining batch 285 Loss: 3.330851\n",
      "\tTraining batch 286 Loss: 3.344377\n",
      "\tTraining batch 287 Loss: 3.342513\n",
      "\tTraining batch 288 Loss: 3.323349\n",
      "\tTraining batch 289 Loss: 3.348192\n",
      "\tTraining batch 290 Loss: 3.337589\n",
      "\tTraining batch 291 Loss: 3.326893\n",
      "\tTraining batch 292 Loss: 3.330349\n",
      "\tTraining batch 293 Loss: 3.353148\n",
      "\tTraining batch 294 Loss: 3.338413\n",
      "\tTraining batch 295 Loss: 3.323453\n",
      "\tTraining batch 296 Loss: 3.332917\n",
      "\tTraining batch 297 Loss: 3.334128\n",
      "\tTraining batch 298 Loss: 3.353966\n",
      "\tTraining batch 299 Loss: 3.336632\n",
      "\tTraining batch 300 Loss: 3.317318\n",
      "\tTraining batch 301 Loss: 3.336528\n",
      "\tTraining batch 302 Loss: 3.329458\n",
      "\tTraining batch 303 Loss: 3.330637\n",
      "\tTraining batch 304 Loss: 3.330861\n",
      "\tTraining batch 305 Loss: 3.330158\n",
      "\tTraining batch 306 Loss: 3.340272\n",
      "\tTraining batch 307 Loss: 3.329713\n",
      "\tTraining batch 308 Loss: 3.338428\n",
      "\tTraining batch 309 Loss: 3.331865\n",
      "\tTraining batch 310 Loss: 3.323413\n",
      "\tTraining batch 311 Loss: 3.331150\n",
      "\tTraining batch 312 Loss: 3.324791\n",
      "\tTraining batch 313 Loss: 3.342992\n",
      "\tTraining batch 314 Loss: 3.330542\n",
      "\tTraining batch 315 Loss: 3.330489\n",
      "\tTraining batch 316 Loss: 3.338921\n",
      "\tTraining batch 317 Loss: 3.328668\n",
      "\tTraining batch 318 Loss: 3.320468\n",
      "\tTraining batch 319 Loss: 3.337845\n",
      "\tTraining batch 320 Loss: 3.331049\n",
      "\tTraining batch 321 Loss: 3.329899\n",
      "\tTraining batch 322 Loss: 3.316422\n",
      "\tTraining batch 323 Loss: 3.319870\n",
      "\tTraining batch 324 Loss: 3.319094\n",
      "\tTraining batch 325 Loss: 3.335491\n",
      "\tTraining batch 326 Loss: 3.325521\n",
      "\tTraining batch 327 Loss: 3.338456\n",
      "\tTraining batch 328 Loss: 3.333631\n",
      "\tTraining batch 329 Loss: 3.341724\n",
      "\tTraining batch 330 Loss: 3.360934\n",
      "\tTraining batch 331 Loss: 3.313992\n",
      "\tTraining batch 332 Loss: 3.330458\n",
      "\tTraining batch 333 Loss: 3.332971\n",
      "\tTraining batch 334 Loss: 3.330320\n",
      "\tTraining batch 335 Loss: 3.323888\n",
      "\tTraining batch 336 Loss: 3.354407\n",
      "\tTraining batch 337 Loss: 3.332435\n",
      "\tTraining batch 338 Loss: 3.340775\n",
      "\tTraining batch 339 Loss: 3.354164\n",
      "\tTraining batch 340 Loss: 3.336766\n",
      "\tTraining batch 341 Loss: 3.347370\n",
      "\tTraining batch 342 Loss: 3.351153\n",
      "\tTraining batch 343 Loss: 3.334278\n",
      "\tTraining batch 344 Loss: 3.337709\n",
      "\tTraining batch 345 Loss: 3.328973\n",
      "\tTraining batch 346 Loss: 3.327549\n",
      "\tTraining batch 347 Loss: 3.315415\n",
      "\tTraining batch 348 Loss: 3.338512\n",
      "\tTraining batch 349 Loss: 3.336243\n",
      "\tTraining batch 350 Loss: 3.353958\n",
      "\tTraining batch 351 Loss: 3.312358\n",
      "\tTraining batch 352 Loss: 3.346035\n",
      "\tTraining batch 353 Loss: 3.339210\n",
      "\tTraining batch 354 Loss: 3.340528\n",
      "\tTraining batch 355 Loss: 3.324329\n",
      "\tTraining batch 356 Loss: 3.323811\n",
      "\tTraining batch 357 Loss: 3.331029\n",
      "\tTraining batch 358 Loss: 3.321913\n",
      "\tTraining batch 359 Loss: 3.323953\n",
      "\tTraining batch 360 Loss: 3.346716\n",
      "\tTraining batch 361 Loss: 3.329109\n",
      "\tTraining batch 362 Loss: 3.334864\n",
      "\tTraining batch 363 Loss: 3.327653\n",
      "\tTraining batch 364 Loss: 3.338386\n",
      "\tTraining batch 365 Loss: 3.317641\n",
      "\tTraining batch 366 Loss: 3.320784\n",
      "\tTraining batch 367 Loss: 3.322719\n",
      "\tTraining batch 368 Loss: 3.320816\n",
      "\tTraining batch 369 Loss: 3.332903\n",
      "\tTraining batch 370 Loss: 3.326762\n",
      "\tTraining batch 371 Loss: 3.336086\n",
      "\tTraining batch 372 Loss: 3.340575\n",
      "\tTraining batch 373 Loss: 3.336232\n",
      "\tTraining batch 374 Loss: 3.314964\n",
      "\tTraining batch 375 Loss: 3.322426\n",
      "\tTraining batch 376 Loss: 3.330301\n",
      "\tTraining batch 377 Loss: 3.325837\n",
      "\tTraining batch 378 Loss: 3.329569\n",
      "\tTraining batch 379 Loss: 3.328503\n",
      "\tTraining batch 380 Loss: 3.320807\n",
      "\tTraining batch 381 Loss: 3.342969\n",
      "\tTraining batch 382 Loss: 3.321883\n",
      "\tTraining batch 383 Loss: 3.323520\n",
      "\tTraining batch 384 Loss: 3.362396\n",
      "\tTraining batch 385 Loss: 3.340115\n",
      "\tTraining batch 386 Loss: 3.343976\n",
      "\tTraining batch 387 Loss: 3.348296\n",
      "\tTraining batch 388 Loss: 3.330867\n",
      "\tTraining batch 389 Loss: 3.361929\n",
      "\tTraining batch 390 Loss: 3.327850\n",
      "\tTraining batch 391 Loss: 3.345500\n",
      "\tTraining batch 392 Loss: 3.336288\n",
      "\tTraining batch 393 Loss: 3.334116\n",
      "\tTraining batch 394 Loss: 3.338346\n",
      "\tTraining batch 395 Loss: 3.344868\n",
      "\tTraining batch 396 Loss: 3.343175\n",
      "\tTraining batch 397 Loss: 3.339154\n",
      "\tTraining batch 398 Loss: 3.349835\n",
      "\tTraining batch 399 Loss: 3.336936\n",
      "\tTraining batch 400 Loss: 3.352206\n",
      "\tTraining batch 401 Loss: 3.320056\n",
      "\tTraining batch 402 Loss: 3.349989\n",
      "\tTraining batch 403 Loss: 3.329311\n",
      "\tTraining batch 404 Loss: 3.364576\n",
      "\tTraining batch 405 Loss: 3.339544\n",
      "\tTraining batch 406 Loss: 3.333319\n",
      "\tTraining batch 407 Loss: 3.338454\n",
      "\tTraining batch 408 Loss: 3.336668\n",
      "\tTraining batch 409 Loss: 3.332274\n",
      "\tTraining batch 410 Loss: 3.338204\n",
      "\tTraining batch 411 Loss: 3.312015\n",
      "\tTraining batch 412 Loss: 3.328454\n",
      "\tTraining batch 413 Loss: 3.329693\n",
      "\tTraining batch 414 Loss: 3.345529\n",
      "\tTraining batch 415 Loss: 3.332137\n",
      "\tTraining batch 416 Loss: 3.313133\n",
      "\tTraining batch 417 Loss: 3.319216\n",
      "\tTraining batch 418 Loss: 3.320957\n",
      "\tTraining batch 419 Loss: 3.325214\n",
      "\tTraining batch 420 Loss: 3.338175\n",
      "\tTraining batch 421 Loss: 3.359676\n",
      "\tTraining batch 422 Loss: 3.349266\n",
      "\tTraining batch 423 Loss: 3.306118\n",
      "\tTraining batch 424 Loss: 3.327002\n",
      "\tTraining batch 425 Loss: 3.338793\n",
      "\tTraining batch 426 Loss: 3.328286\n",
      "\tTraining batch 427 Loss: 3.309286\n",
      "\tTraining batch 428 Loss: 3.345824\n",
      "\tTraining batch 429 Loss: 3.333516\n",
      "\tTraining batch 430 Loss: 3.350825\n",
      "\tTraining batch 431 Loss: 3.323942\n",
      "\tTraining batch 432 Loss: 3.342779\n",
      "\tTraining batch 433 Loss: 3.326860\n",
      "\tTraining batch 434 Loss: 3.335657\n",
      "\tTraining batch 435 Loss: 3.322700\n",
      "\tTraining batch 436 Loss: 3.340596\n",
      "\tTraining batch 437 Loss: 3.353254\n",
      "\tTraining batch 438 Loss: 3.341992\n",
      "\tTraining batch 439 Loss: 3.337695\n",
      "\tTraining batch 440 Loss: 3.330858\n",
      "\tTraining batch 441 Loss: 3.345816\n",
      "\tTraining batch 442 Loss: 3.329454\n",
      "\tTraining batch 443 Loss: 3.315479\n",
      "\tTraining batch 444 Loss: 3.320880\n",
      "\tTraining batch 445 Loss: 3.322334\n",
      "\tTraining batch 446 Loss: 3.342534\n",
      "\tTraining batch 447 Loss: 3.342302\n",
      "\tTraining batch 448 Loss: 3.348377\n",
      "\tTraining batch 449 Loss: 3.312625\n",
      "\tTraining batch 450 Loss: 3.339913\n",
      "\tTraining batch 451 Loss: 3.323027\n",
      "\tTraining batch 452 Loss: 3.319903\n",
      "\tTraining batch 453 Loss: 3.333710\n",
      "\tTraining batch 454 Loss: 3.345469\n",
      "\tTraining batch 455 Loss: 3.333172\n",
      "\tTraining batch 456 Loss: 3.342695\n",
      "\tTraining batch 457 Loss: 3.308885\n",
      "\tTraining batch 458 Loss: 3.322187\n",
      "\tTraining batch 459 Loss: 3.326957\n",
      "\tTraining batch 460 Loss: 3.340601\n",
      "\tTraining batch 461 Loss: 3.334390\n",
      "\tTraining batch 462 Loss: 3.338861\n",
      "\tTraining batch 463 Loss: 3.305446\n",
      "\tTraining batch 464 Loss: 3.335359\n",
      "\tTraining batch 465 Loss: 3.336591\n",
      "\tTraining batch 466 Loss: 3.334761\n",
      "\tTraining batch 467 Loss: 3.339645\n",
      "\tTraining batch 468 Loss: 3.344951\n",
      "\tTraining batch 469 Loss: 3.328927\n",
      "\tTraining batch 470 Loss: 3.308294\n",
      "\tTraining batch 471 Loss: 3.352998\n",
      "\tTraining batch 472 Loss: 3.337243\n",
      "\tTraining batch 473 Loss: 3.332278\n",
      "\tTraining batch 474 Loss: 3.331817\n",
      "\tTraining batch 475 Loss: 3.352803\n",
      "\tTraining batch 476 Loss: 3.333220\n",
      "\tTraining batch 477 Loss: 3.334640\n",
      "\tTraining batch 478 Loss: 3.349789\n",
      "\tTraining batch 479 Loss: 3.332782\n",
      "\tTraining batch 480 Loss: 3.331830\n",
      "\tTraining batch 481 Loss: 3.344570\n",
      "\tTraining batch 482 Loss: 3.335109\n",
      "\tTraining batch 483 Loss: 3.315543\n",
      "\tTraining batch 484 Loss: 3.325174\n",
      "\tTraining batch 485 Loss: 3.344194\n",
      "\tTraining batch 486 Loss: 3.369635\n",
      "\tTraining batch 487 Loss: 3.357417\n",
      "\tTraining batch 488 Loss: 3.322224\n",
      "\tTraining batch 489 Loss: 3.335669\n",
      "\tTraining batch 490 Loss: 3.343793\n",
      "\tTraining batch 491 Loss: 3.315773\n",
      "\tTraining batch 492 Loss: 3.318403\n",
      "\tTraining batch 493 Loss: 3.317170\n",
      "\tTraining batch 494 Loss: 3.339722\n",
      "\tTraining batch 495 Loss: 3.342112\n",
      "\tTraining batch 496 Loss: 3.359426\n",
      "\tTraining batch 497 Loss: 3.331335\n",
      "\tTraining batch 498 Loss: 3.337323\n",
      "\tTraining batch 499 Loss: 3.356462\n",
      "\tTraining batch 500 Loss: 3.344788\n",
      "\tTraining batch 501 Loss: 3.345846\n",
      "\tTraining batch 502 Loss: 3.332225\n",
      "\tTraining batch 503 Loss: 3.337322\n",
      "\tTraining batch 504 Loss: 3.331819\n",
      "\tTraining batch 505 Loss: 3.323523\n",
      "\tTraining batch 506 Loss: 3.345513\n",
      "\tTraining batch 507 Loss: 3.344509\n",
      "\tTraining batch 508 Loss: 3.339226\n",
      "\tTraining batch 509 Loss: 3.321554\n",
      "\tTraining batch 510 Loss: 3.353467\n",
      "\tTraining batch 511 Loss: 3.339405\n",
      "\tTraining batch 512 Loss: 3.321393\n",
      "\tTraining batch 513 Loss: 3.352631\n",
      "\tTraining batch 514 Loss: 3.339054\n",
      "\tTraining batch 515 Loss: 3.327008\n",
      "\tTraining batch 516 Loss: 3.329481\n",
      "\tTraining batch 517 Loss: 3.322439\n",
      "\tTraining batch 518 Loss: 3.341767\n",
      "\tTraining batch 519 Loss: 3.335965\n",
      "\tTraining batch 520 Loss: 3.332012\n",
      "\tTraining batch 521 Loss: 3.330737\n",
      "\tTraining batch 522 Loss: 3.333983\n",
      "\tTraining batch 523 Loss: 3.334181\n",
      "\tTraining batch 524 Loss: 3.339120\n",
      "\tTraining batch 525 Loss: 3.342021\n",
      "\tTraining batch 526 Loss: 3.339136\n",
      "\tTraining batch 527 Loss: 3.336643\n",
      "\tTraining batch 528 Loss: 3.339937\n",
      "\tTraining batch 529 Loss: 3.337244\n",
      "\tTraining batch 530 Loss: 3.341057\n",
      "\tTraining batch 531 Loss: 3.336809\n",
      "\tTraining batch 532 Loss: 3.337063\n",
      "\tTraining batch 533 Loss: 3.348126\n",
      "\tTraining batch 534 Loss: 3.331603\n",
      "\tTraining batch 535 Loss: 3.327238\n",
      "\tTraining batch 536 Loss: 3.340964\n",
      "\tTraining batch 537 Loss: 3.331772\n",
      "\tTraining batch 538 Loss: 3.310885\n",
      "\tTraining batch 539 Loss: 3.341071\n",
      "\tTraining batch 540 Loss: 3.336541\n",
      "\tTraining batch 541 Loss: 3.357347\n",
      "\tTraining batch 542 Loss: 3.343312\n",
      "\tTraining batch 543 Loss: 3.339775\n",
      "\tTraining batch 544 Loss: 3.329418\n",
      "\tTraining batch 545 Loss: 3.352804\n",
      "\tTraining batch 546 Loss: 3.328603\n",
      "\tTraining batch 547 Loss: 3.333359\n",
      "\tTraining batch 548 Loss: 3.330127\n",
      "\tTraining batch 549 Loss: 3.338673\n",
      "\tTraining batch 550 Loss: 3.340535\n",
      "\tTraining batch 551 Loss: 3.330584\n",
      "\tTraining batch 552 Loss: 3.347629\n",
      "\tTraining batch 553 Loss: 3.323866\n",
      "\tTraining batch 554 Loss: 3.345980\n",
      "\tTraining batch 555 Loss: 3.331020\n",
      "\tTraining batch 556 Loss: 3.339851\n",
      "\tTraining batch 557 Loss: 3.337814\n",
      "\tTraining batch 558 Loss: 3.337156\n",
      "\tTraining batch 559 Loss: 3.325697\n",
      "\tTraining batch 560 Loss: 3.330866\n",
      "\tTraining batch 561 Loss: 3.314874\n",
      "\tTraining batch 562 Loss: 3.331359\n",
      "\tTraining batch 563 Loss: 3.328031\n",
      "\tTraining batch 564 Loss: 3.346177\n",
      "\tTraining batch 565 Loss: 3.324825\n",
      "\tTraining batch 566 Loss: 3.333313\n",
      "\tTraining batch 567 Loss: 3.305969\n",
      "\tTraining batch 568 Loss: 3.338396\n",
      "\tTraining batch 569 Loss: 3.329197\n",
      "\tTraining batch 570 Loss: 3.312952\n",
      "\tTraining batch 571 Loss: 3.319990\n",
      "\tTraining batch 572 Loss: 3.336735\n",
      "\tTraining batch 573 Loss: 3.334901\n",
      "\tTraining batch 574 Loss: 3.327894\n",
      "\tTraining batch 575 Loss: 3.323398\n",
      "\tTraining batch 576 Loss: 3.347536\n",
      "\tTraining batch 577 Loss: 3.324655\n",
      "\tTraining batch 578 Loss: 3.356250\n",
      "\tTraining batch 579 Loss: 3.343809\n",
      "\tTraining batch 580 Loss: 3.331434\n",
      "\tTraining batch 581 Loss: 3.315348\n",
      "\tTraining batch 582 Loss: 3.321665\n",
      "\tTraining batch 583 Loss: 3.322006\n",
      "\tTraining batch 584 Loss: 3.320406\n",
      "\tTraining batch 585 Loss: 3.317106\n",
      "\tTraining batch 586 Loss: 3.322821\n",
      "\tTraining batch 587 Loss: 3.339298\n",
      "\tTraining batch 588 Loss: 3.335111\n",
      "\tTraining batch 589 Loss: 3.326546\n",
      "\tTraining batch 590 Loss: 3.330340\n",
      "\tTraining batch 591 Loss: 3.336486\n",
      "\tTraining batch 592 Loss: 3.341653\n",
      "\tTraining batch 593 Loss: 3.329835\n",
      "\tTraining batch 594 Loss: 3.323969\n",
      "\tTraining batch 595 Loss: 3.341665\n",
      "\tTraining batch 596 Loss: 3.337748\n",
      "\tTraining batch 597 Loss: 3.343601\n",
      "\tTraining batch 598 Loss: 3.312774\n",
      "\tTraining batch 599 Loss: 3.331635\n",
      "\tTraining batch 600 Loss: 3.355482\n",
      "\tTraining batch 601 Loss: 3.343530\n",
      "\tTraining batch 602 Loss: 3.325672\n",
      "\tTraining batch 603 Loss: 3.333347\n",
      "\tTraining batch 604 Loss: 3.334387\n",
      "\tTraining batch 605 Loss: 3.326205\n",
      "\tTraining batch 606 Loss: 3.326779\n",
      "\tTraining batch 607 Loss: 3.355953\n",
      "\tTraining batch 608 Loss: 3.328385\n",
      "\tTraining batch 609 Loss: 3.352976\n",
      "\tTraining batch 610 Loss: 3.326370\n",
      "\tTraining batch 611 Loss: 3.333131\n",
      "\tTraining batch 612 Loss: 3.342458\n",
      "\tTraining batch 613 Loss: 3.310907\n",
      "\tTraining batch 614 Loss: 3.341123\n",
      "\tTraining batch 615 Loss: 3.335131\n",
      "\tTraining batch 616 Loss: 3.327999\n",
      "\tTraining batch 617 Loss: 3.348685\n",
      "\tTraining batch 618 Loss: 3.331862\n",
      "\tTraining batch 619 Loss: 3.327306\n",
      "\tTraining batch 620 Loss: 3.332076\n",
      "\tTraining batch 621 Loss: 3.338761\n",
      "\tTraining batch 622 Loss: 3.334078\n",
      "\tTraining batch 623 Loss: 3.357256\n",
      "\tTraining batch 624 Loss: 3.332020\n",
      "\tTraining batch 625 Loss: 3.324462\n",
      "\tTraining batch 626 Loss: 3.345031\n",
      "\tTraining batch 627 Loss: 3.337106\n",
      "\tTraining batch 628 Loss: 3.330386\n",
      "\tTraining batch 629 Loss: 3.373346\n",
      "\tTraining batch 630 Loss: 3.328183\n",
      "\tTraining batch 631 Loss: 3.338314\n",
      "\tTraining batch 632 Loss: 3.344101\n",
      "\tTraining batch 633 Loss: 3.333357\n",
      "\tTraining batch 634 Loss: 3.378013\n",
      "\tTraining batch 635 Loss: 3.328137\n",
      "\tTraining batch 636 Loss: 3.333501\n",
      "\tTraining batch 637 Loss: 3.347795\n",
      "\tTraining batch 638 Loss: 3.318084\n",
      "\tTraining batch 639 Loss: 3.363181\n",
      "\tTraining batch 640 Loss: 3.330156\n",
      "\tTraining batch 641 Loss: 3.333536\n",
      "\tTraining batch 642 Loss: 3.342773\n",
      "\tTraining batch 643 Loss: 3.323201\n",
      "\tTraining batch 644 Loss: 3.337021\n",
      "\tTraining batch 645 Loss: 3.336412\n",
      "\tTraining batch 646 Loss: 3.342382\n",
      "\tTraining batch 647 Loss: 3.341501\n",
      "\tTraining batch 648 Loss: 3.336385\n",
      "\tTraining batch 649 Loss: 3.324528\n",
      "\tTraining batch 650 Loss: 3.340868\n",
      "\tTraining batch 651 Loss: 3.326034\n",
      "\tTraining batch 652 Loss: 3.316376\n",
      "\tTraining batch 653 Loss: 3.352260\n",
      "\tTraining batch 654 Loss: 3.338884\n",
      "\tTraining batch 655 Loss: 3.332907\n",
      "\tTraining batch 656 Loss: 3.337687\n",
      "\tTraining batch 657 Loss: 3.325805\n",
      "\tTraining batch 658 Loss: 3.319406\n",
      "\tTraining batch 659 Loss: 3.323809\n",
      "\tTraining batch 660 Loss: 3.334608\n",
      "\tTraining batch 661 Loss: 3.327944\n",
      "\tTraining batch 662 Loss: 3.322380\n",
      "\tTraining batch 663 Loss: 3.320314\n",
      "\tTraining batch 664 Loss: 3.333415\n",
      "\tTraining batch 665 Loss: 3.328542\n",
      "\tTraining batch 666 Loss: 3.328617\n",
      "\tTraining batch 667 Loss: 3.331297\n",
      "\tTraining batch 668 Loss: 3.335115\n",
      "\tTraining batch 669 Loss: 3.349834\n",
      "\tTraining batch 670 Loss: 3.326126\n",
      "\tTraining batch 671 Loss: 3.337343\n",
      "\tTraining batch 672 Loss: 3.333468\n",
      "\tTraining batch 673 Loss: 3.331728\n",
      "\tTraining batch 674 Loss: 3.339443\n",
      "\tTraining batch 675 Loss: 3.350782\n",
      "\tTraining batch 676 Loss: 3.340120\n",
      "\tTraining batch 677 Loss: 3.329743\n",
      "\tTraining batch 678 Loss: 3.347122\n",
      "\tTraining batch 679 Loss: 3.313390\n",
      "\tTraining batch 680 Loss: 3.343670\n",
      "\tTraining batch 681 Loss: 3.361426\n",
      "\tTraining batch 682 Loss: 3.321901\n",
      "\tTraining batch 683 Loss: 3.334160\n",
      "\tTraining batch 684 Loss: 3.305897\n",
      "\tTraining batch 685 Loss: 3.330354\n",
      "\tTraining batch 686 Loss: 3.329097\n",
      "\tTraining batch 687 Loss: 3.322864\n",
      "\tTraining batch 688 Loss: 3.348910\n",
      "\tTraining batch 689 Loss: 3.336328\n",
      "\tTraining batch 690 Loss: 3.350857\n",
      "\tTraining batch 691 Loss: 3.329118\n",
      "\tTraining batch 692 Loss: 3.321303\n",
      "\tTraining batch 693 Loss: 3.330523\n",
      "\tTraining batch 694 Loss: 3.343545\n",
      "\tTraining batch 695 Loss: 3.330226\n",
      "\tTraining batch 696 Loss: 3.344878\n",
      "\tTraining batch 697 Loss: 3.320105\n",
      "\tTraining batch 698 Loss: 3.346654\n",
      "\tTraining batch 699 Loss: 3.341748\n",
      "\tTraining batch 700 Loss: 3.352240\n",
      "\tTraining batch 701 Loss: 3.344189\n",
      "\tTraining batch 702 Loss: 3.348537\n",
      "\tTraining batch 703 Loss: 3.320962\n",
      "\tTraining batch 704 Loss: 3.336005\n",
      "\tTraining batch 705 Loss: 3.314129\n",
      "\tTraining batch 706 Loss: 3.328279\n",
      "\tTraining batch 707 Loss: 3.341139\n",
      "\tTraining batch 708 Loss: 3.332785\n",
      "\tTraining batch 709 Loss: 3.332483\n",
      "\tTraining batch 710 Loss: 3.342964\n",
      "\tTraining batch 711 Loss: 3.356389\n",
      "\tTraining batch 712 Loss: 3.328892\n",
      "\tTraining batch 713 Loss: 3.337597\n",
      "\tTraining batch 714 Loss: 3.333076\n",
      "\tTraining batch 715 Loss: 3.363904\n",
      "\tTraining batch 716 Loss: 3.320498\n",
      "\tTraining batch 717 Loss: 3.341700\n",
      "\tTraining batch 718 Loss: 3.325435\n",
      "\tTraining batch 719 Loss: 3.336802\n",
      "\tTraining batch 720 Loss: 3.333728\n",
      "\tTraining batch 721 Loss: 3.343649\n",
      "\tTraining batch 722 Loss: 3.331792\n",
      "\tTraining batch 723 Loss: 3.331059\n",
      "\tTraining batch 724 Loss: 3.342892\n",
      "\tTraining batch 725 Loss: 3.349948\n",
      "\tTraining batch 726 Loss: 3.337018\n",
      "\tTraining batch 727 Loss: 3.336148\n",
      "\tTraining batch 728 Loss: 3.327483\n",
      "\tTraining batch 729 Loss: 3.337455\n",
      "\tTraining batch 730 Loss: 3.337099\n",
      "\tTraining batch 731 Loss: 3.342480\n",
      "\tTraining batch 732 Loss: 3.347911\n",
      "\tTraining batch 733 Loss: 3.326554\n",
      "\tTraining batch 734 Loss: 3.343324\n",
      "\tTraining batch 735 Loss: 3.327188\n",
      "\tTraining batch 736 Loss: 3.340361\n",
      "\tTraining batch 737 Loss: 3.318340\n",
      "\tTraining batch 738 Loss: 3.352238\n",
      "\tTraining batch 739 Loss: 3.332541\n",
      "\tTraining batch 740 Loss: 3.323831\n",
      "\tTraining batch 741 Loss: 3.335142\n",
      "\tTraining batch 742 Loss: 3.353431\n",
      "\tTraining batch 743 Loss: 3.321580\n",
      "\tTraining batch 744 Loss: 3.334553\n",
      "\tTraining batch 745 Loss: 3.324276\n",
      "\tTraining batch 746 Loss: 3.326843\n",
      "\tTraining batch 747 Loss: 3.338850\n",
      "\tTraining batch 748 Loss: 3.343247\n",
      "\tTraining batch 749 Loss: 3.344476\n",
      "\tTraining batch 750 Loss: 3.338800\n",
      "\tTraining batch 751 Loss: 3.337091\n",
      "\tTraining batch 752 Loss: 3.334313\n",
      "\tTraining batch 753 Loss: 3.340768\n",
      "\tTraining batch 754 Loss: 3.334119\n",
      "\tTraining batch 755 Loss: 3.331555\n",
      "\tTraining batch 756 Loss: 3.346671\n",
      "\tTraining batch 757 Loss: 3.352567\n",
      "\tTraining batch 758 Loss: 3.328774\n",
      "\tTraining batch 759 Loss: 3.336649\n",
      "\tTraining batch 760 Loss: 3.333003\n",
      "\tTraining batch 761 Loss: 3.339615\n",
      "\tTraining batch 762 Loss: 3.328582\n",
      "\tTraining batch 763 Loss: 3.323097\n",
      "\tTraining batch 764 Loss: 3.335745\n",
      "\tTraining batch 765 Loss: 3.342242\n",
      "\tTraining batch 766 Loss: 3.330819\n",
      "\tTraining batch 767 Loss: 3.349485\n",
      "\tTraining batch 768 Loss: 3.327047\n",
      "\tTraining batch 769 Loss: 3.342685\n",
      "\tTraining batch 770 Loss: 3.318262\n",
      "\tTraining batch 771 Loss: 3.334422\n",
      "\tTraining batch 772 Loss: 3.345402\n",
      "\tTraining batch 773 Loss: 3.346167\n",
      "\tTraining batch 774 Loss: 3.338888\n",
      "\tTraining batch 775 Loss: 3.332447\n",
      "\tTraining batch 776 Loss: 3.330624\n",
      "\tTraining batch 777 Loss: 3.342036\n",
      "\tTraining batch 778 Loss: 3.340343\n",
      "\tTraining batch 779 Loss: 3.343885\n",
      "\tTraining batch 780 Loss: 3.320542\n",
      "\tTraining batch 781 Loss: 3.325150\n",
      "\tTraining batch 782 Loss: 3.331287\n",
      "\tTraining batch 783 Loss: 3.336187\n",
      "\tTraining batch 784 Loss: 3.336268\n",
      "\tTraining batch 785 Loss: 3.331239\n",
      "\tTraining batch 786 Loss: 3.328524\n",
      "\tTraining batch 787 Loss: 3.331160\n",
      "\tTraining batch 788 Loss: 3.349535\n",
      "\tTraining batch 789 Loss: 3.331916\n",
      "\tTraining batch 790 Loss: 3.337337\n",
      "\tTraining batch 791 Loss: 3.326877\n",
      "\tTraining batch 792 Loss: 3.312680\n",
      "\tTraining batch 793 Loss: 3.338732\n",
      "\tTraining batch 794 Loss: 3.334160\n",
      "\tTraining batch 795 Loss: 3.328260\n",
      "\tTraining batch 796 Loss: 3.333866\n",
      "\tTraining batch 797 Loss: 3.340355\n",
      "\tTraining batch 798 Loss: 3.340112\n",
      "\tTraining batch 799 Loss: 3.323145\n",
      "\tTraining batch 800 Loss: 3.332777\n",
      "\tTraining batch 801 Loss: 3.327307\n",
      "\tTraining batch 802 Loss: 3.328663\n",
      "\tTraining batch 803 Loss: 3.341407\n",
      "\tTraining batch 804 Loss: 3.339692\n",
      "\tTraining batch 805 Loss: 3.322490\n",
      "\tTraining batch 806 Loss: 3.333096\n",
      "\tTraining batch 807 Loss: 3.326540\n",
      "\tTraining batch 808 Loss: 3.359725\n",
      "\tTraining batch 809 Loss: 3.346717\n",
      "\tTraining batch 810 Loss: 3.316519\n",
      "\tTraining batch 811 Loss: 3.338017\n",
      "\tTraining batch 812 Loss: 3.333625\n",
      "\tTraining batch 813 Loss: 3.343663\n",
      "\tTraining batch 814 Loss: 3.333233\n",
      "\tTraining batch 815 Loss: 3.335718\n",
      "\tTraining batch 816 Loss: 3.330488\n",
      "\tTraining batch 817 Loss: 3.343289\n",
      "\tTraining batch 818 Loss: 3.348004\n",
      "\tTraining batch 819 Loss: 3.315981\n",
      "\tTraining batch 820 Loss: 3.315169\n",
      "\tTraining batch 821 Loss: 3.343851\n",
      "\tTraining batch 822 Loss: 3.329864\n",
      "\tTraining batch 823 Loss: 3.344568\n",
      "\tTraining batch 824 Loss: 3.309291\n",
      "\tTraining batch 825 Loss: 3.337435\n",
      "\tTraining batch 826 Loss: 3.306129\n",
      "\tTraining batch 827 Loss: 3.337881\n",
      "\tTraining batch 828 Loss: 3.317176\n",
      "\tTraining batch 829 Loss: 3.334903\n",
      "\tTraining batch 830 Loss: 3.345589\n",
      "\tTraining batch 831 Loss: 3.360439\n",
      "\tTraining batch 832 Loss: 3.339465\n",
      "\tTraining batch 833 Loss: 3.328574\n",
      "\tTraining batch 834 Loss: 3.346542\n",
      "\tTraining batch 835 Loss: 3.317237\n",
      "\tTraining batch 836 Loss: 3.343831\n",
      "\tTraining batch 837 Loss: 3.354787\n",
      "\tTraining batch 838 Loss: 3.337072\n",
      "\tTraining batch 839 Loss: 3.339512\n",
      "\tTraining batch 840 Loss: 3.334290\n",
      "\tTraining batch 841 Loss: 3.340522\n",
      "\tTraining batch 842 Loss: 3.308825\n",
      "\tTraining batch 843 Loss: 3.328279\n",
      "\tTraining batch 844 Loss: 3.318515\n",
      "\tTraining batch 845 Loss: 3.342635\n",
      "\tTraining batch 846 Loss: 3.341741\n",
      "\tTraining batch 847 Loss: 3.321692\n",
      "\tTraining batch 848 Loss: 3.334267\n",
      "\tTraining batch 849 Loss: 3.331346\n",
      "\tTraining batch 850 Loss: 3.330752\n",
      "\tTraining batch 851 Loss: 3.340958\n",
      "\tTraining batch 852 Loss: 3.327495\n",
      "\tTraining batch 853 Loss: 3.328680\n",
      "\tTraining batch 854 Loss: 3.361842\n",
      "\tTraining batch 855 Loss: 3.324209\n",
      "\tTraining batch 856 Loss: 3.344830\n",
      "\tTraining batch 857 Loss: 3.320418\n",
      "\tTraining batch 858 Loss: 3.352184\n",
      "\tTraining batch 859 Loss: 3.333977\n",
      "\tTraining batch 860 Loss: 3.330844\n",
      "\tTraining batch 861 Loss: 3.342999\n",
      "\tTraining batch 862 Loss: 3.324191\n",
      "\tTraining batch 863 Loss: 3.337285\n",
      "\tTraining batch 864 Loss: 3.328437\n",
      "\tTraining batch 865 Loss: 3.338862\n",
      "\tTraining batch 866 Loss: 3.340899\n",
      "\tTraining batch 867 Loss: 3.348844\n",
      "\tTraining batch 868 Loss: 3.339291\n",
      "\tTraining batch 869 Loss: 3.335768\n",
      "\tTraining batch 870 Loss: 3.340328\n",
      "\tTraining batch 871 Loss: 3.328156\n",
      "\tTraining batch 872 Loss: 3.337508\n",
      "\tTraining batch 873 Loss: 3.345362\n",
      "\tTraining batch 874 Loss: 3.343016\n",
      "\tTraining batch 875 Loss: 3.337661\n",
      "\tTraining batch 876 Loss: 3.344648\n",
      "\tTraining batch 877 Loss: 3.341959\n",
      "\tTraining batch 878 Loss: 3.331571\n",
      "\tTraining batch 879 Loss: 3.348884\n",
      "\tTraining batch 880 Loss: 3.348578\n",
      "\tTraining batch 881 Loss: 3.340096\n",
      "\tTraining batch 882 Loss: 3.338609\n",
      "\tTraining batch 883 Loss: 3.331997\n",
      "\tTraining batch 884 Loss: 3.331820\n",
      "\tTraining batch 885 Loss: 3.342622\n",
      "\tTraining batch 886 Loss: 3.332725\n",
      "\tTraining batch 887 Loss: 3.336427\n",
      "\tTraining batch 888 Loss: 3.335440\n",
      "\tTraining batch 889 Loss: 3.342252\n",
      "\tTraining batch 890 Loss: 3.327183\n",
      "\tTraining batch 891 Loss: 3.343443\n",
      "\tTraining batch 892 Loss: 3.321148\n",
      "\tTraining batch 893 Loss: 3.330002\n",
      "\tTraining batch 894 Loss: 3.340199\n",
      "\tTraining batch 895 Loss: 3.346077\n",
      "\tTraining batch 896 Loss: 3.343836\n",
      "\tTraining batch 897 Loss: 3.339403\n",
      "\tTraining batch 898 Loss: 3.328906\n",
      "\tTraining batch 899 Loss: 3.327216\n",
      "\tTraining batch 900 Loss: 3.341536\n",
      "\tTraining batch 901 Loss: 3.322865\n",
      "\tTraining batch 902 Loss: 3.322172\n",
      "\tTraining batch 903 Loss: 3.334750\n",
      "\tTraining batch 904 Loss: 3.337168\n",
      "\tTraining batch 905 Loss: 3.322655\n",
      "\tTraining batch 906 Loss: 3.339343\n",
      "\tTraining batch 907 Loss: 3.332560\n",
      "\tTraining batch 908 Loss: 3.336052\n",
      "\tTraining batch 909 Loss: 3.326611\n",
      "\tTraining batch 910 Loss: 3.340161\n",
      "\tTraining batch 911 Loss: 3.325880\n",
      "\tTraining batch 912 Loss: 3.322043\n",
      "\tTraining batch 913 Loss: 3.333833\n",
      "\tTraining batch 914 Loss: 3.307516\n",
      "\tTraining batch 915 Loss: 3.325462\n",
      "\tTraining batch 916 Loss: 3.346289\n",
      "\tTraining batch 917 Loss: 3.340145\n",
      "\tTraining batch 918 Loss: 3.319464\n",
      "\tTraining batch 919 Loss: 3.334669\n",
      "\tTraining batch 920 Loss: 3.342013\n",
      "\tTraining batch 921 Loss: 3.344077\n",
      "\tTraining batch 922 Loss: 3.328982\n",
      "\tTraining batch 923 Loss: 3.341673\n",
      "\tTraining batch 924 Loss: 3.332228\n",
      "\tTraining batch 925 Loss: 3.342514\n",
      "\tTraining batch 926 Loss: 3.331411\n",
      "\tTraining batch 927 Loss: 3.329553\n",
      "\tTraining batch 928 Loss: 3.322252\n",
      "\tTraining batch 929 Loss: 3.320155\n",
      "\tTraining batch 930 Loss: 3.340825\n",
      "\tTraining batch 931 Loss: 3.331869\n",
      "\tTraining batch 932 Loss: 3.314450\n",
      "\tTraining batch 933 Loss: 3.328643\n",
      "\tTraining batch 934 Loss: 3.333820\n",
      "\tTraining batch 935 Loss: 3.318731\n",
      "\tTraining batch 936 Loss: 3.339851\n",
      "\tTraining batch 937 Loss: 3.356387\n",
      "\tTraining batch 938 Loss: 3.337193\n",
      "\tTraining batch 939 Loss: 3.354751\n",
      "\tTraining batch 940 Loss: 3.338330\n",
      "\tTraining batch 941 Loss: 3.319528\n",
      "\tTraining batch 942 Loss: 3.315812\n",
      "\tTraining batch 943 Loss: 3.338607\n",
      "\tTraining batch 944 Loss: 3.326818\n",
      "\tTraining batch 945 Loss: 3.323618\n",
      "\tTraining batch 946 Loss: 3.329206\n",
      "\tTraining batch 947 Loss: 3.335039\n",
      "\tTraining batch 948 Loss: 3.330316\n",
      "\tTraining batch 949 Loss: 3.360192\n",
      "\tTraining batch 950 Loss: 3.319062\n",
      "\tTraining batch 951 Loss: 3.350775\n",
      "\tTraining batch 952 Loss: 3.352031\n",
      "\tTraining batch 953 Loss: 3.349505\n",
      "\tTraining batch 954 Loss: 3.333342\n",
      "\tTraining batch 955 Loss: 3.313981\n",
      "\tTraining batch 956 Loss: 3.330607\n",
      "\tTraining batch 957 Loss: 3.334808\n",
      "\tTraining batch 958 Loss: 3.348943\n",
      "\tTraining batch 959 Loss: 3.346475\n",
      "\tTraining batch 960 Loss: 3.356368\n",
      "\tTraining batch 961 Loss: 3.331710\n",
      "\tTraining batch 962 Loss: 3.310370\n",
      "\tTraining batch 963 Loss: 3.334545\n",
      "\tTraining batch 964 Loss: 3.331636\n",
      "\tTraining batch 965 Loss: 3.312533\n",
      "\tTraining batch 966 Loss: 3.340087\n",
      "\tTraining batch 967 Loss: 3.323383\n",
      "\tTraining batch 968 Loss: 3.330013\n",
      "\tTraining batch 969 Loss: 3.314786\n",
      "\tTraining batch 970 Loss: 3.321739\n",
      "\tTraining batch 971 Loss: 3.346762\n",
      "\tTraining batch 972 Loss: 3.346760\n",
      "\tTraining batch 973 Loss: 3.319582\n",
      "\tTraining batch 974 Loss: 3.342937\n",
      "\tTraining batch 975 Loss: 3.355966\n",
      "\tTraining batch 976 Loss: 3.334671\n",
      "\tTraining batch 977 Loss: 3.331507\n",
      "\tTraining batch 978 Loss: 3.368172\n",
      "\tTraining batch 979 Loss: 3.333680\n",
      "\tTraining batch 980 Loss: 3.325686\n",
      "\tTraining batch 981 Loss: 3.340372\n",
      "\tTraining batch 982 Loss: 3.345331\n",
      "\tTraining batch 983 Loss: 3.331911\n",
      "\tTraining batch 984 Loss: 3.348667\n",
      "\tTraining batch 985 Loss: 3.319212\n",
      "\tTraining batch 986 Loss: 3.335511\n",
      "\tTraining batch 987 Loss: 3.323123\n",
      "\tTraining batch 988 Loss: 3.323757\n",
      "\tTraining batch 989 Loss: 3.337301\n",
      "\tTraining batch 990 Loss: 3.343220\n",
      "\tTraining batch 991 Loss: 3.323287\n",
      "\tTraining batch 992 Loss: 3.320481\n",
      "\tTraining batch 993 Loss: 3.333982\n",
      "\tTraining batch 994 Loss: 3.335434\n",
      "\tTraining batch 995 Loss: 3.326314\n",
      "\tTraining batch 996 Loss: 3.333498\n",
      "\tTraining batch 997 Loss: 3.327928\n",
      "\tTraining batch 998 Loss: 3.335228\n",
      "\tTraining batch 999 Loss: 3.353279\n",
      "\tTraining batch 1000 Loss: 3.341245\n",
      "\tTraining batch 1001 Loss: 3.351418\n",
      "\tTraining batch 1002 Loss: 3.300184\n",
      "\tTraining batch 1003 Loss: 3.352951\n",
      "\tTraining batch 1004 Loss: 3.350001\n",
      "\tTraining batch 1005 Loss: 3.353724\n",
      "\tTraining batch 1006 Loss: 3.328690\n",
      "\tTraining batch 1007 Loss: 3.322331\n",
      "\tTraining batch 1008 Loss: 3.330770\n",
      "\tTraining batch 1009 Loss: 3.339860\n",
      "\tTraining batch 1010 Loss: 3.347017\n",
      "\tTraining batch 1011 Loss: 3.336313\n",
      "\tTraining batch 1012 Loss: 3.327921\n",
      "\tTraining batch 1013 Loss: 3.327624\n",
      "\tTraining batch 1014 Loss: 3.346003\n",
      "\tTraining batch 1015 Loss: 3.354269\n",
      "\tTraining batch 1016 Loss: 3.320228\n",
      "\tTraining batch 1017 Loss: 3.331220\n",
      "\tTraining batch 1018 Loss: 3.346361\n",
      "\tTraining batch 1019 Loss: 3.337502\n",
      "\tTraining batch 1020 Loss: 3.352102\n",
      "\tTraining batch 1021 Loss: 3.349407\n",
      "\tTraining batch 1022 Loss: 3.320225\n",
      "\tTraining batch 1023 Loss: 3.343433\n",
      "\tTraining batch 1024 Loss: 3.314587\n",
      "\tTraining batch 1025 Loss: 3.337891\n",
      "\tTraining batch 1026 Loss: 3.345318\n",
      "\tTraining batch 1027 Loss: 3.343140\n",
      "\tTraining batch 1028 Loss: 3.343008\n",
      "\tTraining batch 1029 Loss: 3.335138\n",
      "\tTraining batch 1030 Loss: 3.325910\n",
      "\tTraining batch 1031 Loss: 3.322642\n",
      "\tTraining batch 1032 Loss: 3.325493\n",
      "\tTraining batch 1033 Loss: 3.340338\n",
      "\tTraining batch 1034 Loss: 3.311431\n",
      "\tTraining batch 1035 Loss: 3.325269\n",
      "\tTraining batch 1036 Loss: 3.320603\n",
      "\tTraining batch 1037 Loss: 3.331302\n",
      "\tTraining batch 1038 Loss: 3.321823\n",
      "\tTraining batch 1039 Loss: 3.341275\n",
      "\tTraining batch 1040 Loss: 3.329454\n",
      "\tTraining batch 1041 Loss: 3.329134\n",
      "\tTraining batch 1042 Loss: 3.336901\n",
      "\tTraining batch 1043 Loss: 3.353462\n",
      "\tTraining batch 1044 Loss: 3.334116\n",
      "\tTraining batch 1045 Loss: 3.317146\n",
      "\tTraining batch 1046 Loss: 3.347257\n",
      "\tTraining batch 1047 Loss: 3.336277\n",
      "\tTraining batch 1048 Loss: 3.339623\n",
      "\tTraining batch 1049 Loss: 3.322549\n",
      "\tTraining batch 1050 Loss: 3.323735\n",
      "\tTraining batch 1051 Loss: 3.339891\n",
      "\tTraining batch 1052 Loss: 3.332007\n",
      "\tTraining batch 1053 Loss: 3.327518\n",
      "\tTraining batch 1054 Loss: 3.321123\n",
      "\tTraining batch 1055 Loss: 3.337588\n",
      "\tTraining batch 1056 Loss: 3.335608\n",
      "\tTraining batch 1057 Loss: 3.333623\n",
      "\tTraining batch 1058 Loss: 3.351396\n",
      "\tTraining batch 1059 Loss: 3.322432\n",
      "\tTraining batch 1060 Loss: 3.336858\n",
      "\tTraining batch 1061 Loss: 3.337083\n",
      "\tTraining batch 1062 Loss: 3.321294\n",
      "\tTraining batch 1063 Loss: 3.321337\n",
      "\tTraining batch 1064 Loss: 3.334800\n",
      "\tTraining batch 1065 Loss: 3.342333\n",
      "\tTraining batch 1066 Loss: 3.347622\n",
      "\tTraining batch 1067 Loss: 3.326591\n",
      "\tTraining batch 1068 Loss: 3.318280\n",
      "\tTraining batch 1069 Loss: 3.335101\n",
      "\tTraining batch 1070 Loss: 3.345978\n",
      "\tTraining batch 1071 Loss: 3.325332\n",
      "\tTraining batch 1072 Loss: 3.327138\n",
      "\tTraining batch 1073 Loss: 3.347726\n",
      "\tTraining batch 1074 Loss: 3.336529\n",
      "\tTraining batch 1075 Loss: 3.338495\n",
      "\tTraining batch 1076 Loss: 3.346683\n",
      "\tTraining batch 1077 Loss: 3.350426\n",
      "\tTraining batch 1078 Loss: 3.359091\n",
      "\tTraining batch 1079 Loss: 3.307400\n",
      "\tTraining batch 1080 Loss: 3.318472\n",
      "\tTraining batch 1081 Loss: 3.338096\n",
      "\tTraining batch 1082 Loss: 3.331405\n",
      "\tTraining batch 1083 Loss: 3.326637\n",
      "\tTraining batch 1084 Loss: 3.345407\n",
      "\tTraining batch 1085 Loss: 3.315367\n",
      "\tTraining batch 1086 Loss: 3.338282\n",
      "\tTraining batch 1087 Loss: 3.349402\n",
      "\tTraining batch 1088 Loss: 3.329156\n",
      "\tTraining batch 1089 Loss: 3.322306\n",
      "\tTraining batch 1090 Loss: 3.351884\n",
      "\tTraining batch 1091 Loss: 3.332317\n",
      "\tTraining batch 1092 Loss: 3.359077\n",
      "\tTraining batch 1093 Loss: 3.345406\n",
      "\tTraining batch 1094 Loss: 3.353705\n",
      "\tTraining batch 1095 Loss: 3.334948\n",
      "\tTraining batch 1096 Loss: 3.331192\n",
      "\tTraining batch 1097 Loss: 3.343029\n",
      "\tTraining batch 1098 Loss: 3.353170\n",
      "\tTraining batch 1099 Loss: 3.318796\n",
      "\tTraining batch 1100 Loss: 3.337506\n",
      "\tTraining batch 1101 Loss: 3.336238\n",
      "\tTraining batch 1102 Loss: 3.340904\n",
      "\tTraining batch 1103 Loss: 3.327320\n",
      "\tTraining batch 1104 Loss: 3.325575\n",
      "\tTraining batch 1105 Loss: 3.328602\n",
      "\tTraining batch 1106 Loss: 3.338353\n",
      "\tTraining batch 1107 Loss: 3.326241\n",
      "\tTraining batch 1108 Loss: 3.338574\n",
      "\tTraining batch 1109 Loss: 3.322530\n",
      "\tTraining batch 1110 Loss: 3.321629\n",
      "\tTraining batch 1111 Loss: 3.326697\n",
      "\tTraining batch 1112 Loss: 3.326025\n",
      "\tTraining batch 1113 Loss: 3.323217\n",
      "\tTraining batch 1114 Loss: 3.322058\n",
      "\tTraining batch 1115 Loss: 3.323291\n",
      "\tTraining batch 1116 Loss: 3.338232\n",
      "\tTraining batch 1117 Loss: 3.317674\n",
      "\tTraining batch 1118 Loss: 3.346944\n",
      "\tTraining batch 1119 Loss: 3.361461\n",
      "\tTraining batch 1120 Loss: 3.350018\n",
      "\tTraining batch 1121 Loss: 3.327249\n",
      "\tTraining batch 1122 Loss: 3.333655\n",
      "\tTraining batch 1123 Loss: 3.343332\n",
      "\tTraining batch 1124 Loss: 3.341075\n",
      "\tTraining batch 1125 Loss: 3.343567\n",
      "\tTraining batch 1126 Loss: 3.331521\n",
      "\tTraining batch 1127 Loss: 3.344841\n",
      "\tTraining batch 1128 Loss: 3.341144\n",
      "\tTraining batch 1129 Loss: 3.337395\n",
      "\tTraining batch 1130 Loss: 3.332851\n",
      "\tTraining batch 1131 Loss: 3.339783\n",
      "\tTraining batch 1132 Loss: 3.326343\n",
      "\tTraining batch 1133 Loss: 3.335962\n",
      "\tTraining batch 1134 Loss: 3.325469\n",
      "\tTraining batch 1135 Loss: 3.339707\n",
      "\tTraining batch 1136 Loss: 3.312820\n",
      "\tTraining batch 1137 Loss: 3.329504\n",
      "\tTraining batch 1138 Loss: 3.329651\n",
      "\tTraining batch 1139 Loss: 3.321365\n",
      "\tTraining batch 1140 Loss: 3.335293\n",
      "\tTraining batch 1141 Loss: 3.325769\n",
      "\tTraining batch 1142 Loss: 3.328212\n",
      "\tTraining batch 1143 Loss: 3.304061\n",
      "\tTraining batch 1144 Loss: 3.320259\n",
      "\tTraining batch 1145 Loss: 3.313331\n",
      "\tTraining batch 1146 Loss: 3.321332\n",
      "\tTraining batch 1147 Loss: 3.328571\n",
      "\tTraining batch 1148 Loss: 3.331805\n",
      "\tTraining batch 1149 Loss: 3.349052\n",
      "\tTraining batch 1150 Loss: 3.325137\n",
      "\tTraining batch 1151 Loss: 3.352535\n",
      "\tTraining batch 1152 Loss: 3.331624\n",
      "\tTraining batch 1153 Loss: 3.368926\n",
      "\tTraining batch 1154 Loss: 3.362644\n",
      "\tTraining batch 1155 Loss: 3.316473\n",
      "\tTraining batch 1156 Loss: 3.336132\n",
      "\tTraining batch 1157 Loss: 3.323807\n",
      "\tTraining batch 1158 Loss: 3.314569\n",
      "\tTraining batch 1159 Loss: 3.355970\n",
      "\tTraining batch 1160 Loss: 3.311578\n",
      "\tTraining batch 1161 Loss: 3.327420\n",
      "\tTraining batch 1162 Loss: 3.346153\n",
      "\tTraining batch 1163 Loss: 3.325978\n",
      "\tTraining batch 1164 Loss: 3.328601\n",
      "\tTraining batch 1165 Loss: 3.315854\n",
      "\tTraining batch 1166 Loss: 3.354334\n",
      "\tTraining batch 1167 Loss: 3.322474\n",
      "\tTraining batch 1168 Loss: 3.341144\n",
      "\tTraining batch 1169 Loss: 3.322856\n",
      "\tTraining batch 1170 Loss: 3.325355\n",
      "\tTraining batch 1171 Loss: 3.317385\n",
      "\tTraining batch 1172 Loss: 3.331577\n",
      "\tTraining batch 1173 Loss: 3.314190\n",
      "\tTraining batch 1174 Loss: 3.329962\n",
      "\tTraining batch 1175 Loss: 3.322245\n",
      "\tTraining batch 1176 Loss: 3.338741\n",
      "\tTraining batch 1177 Loss: 3.344590\n",
      "\tTraining batch 1178 Loss: 3.339223\n",
      "\tTraining batch 1179 Loss: 3.299054\n",
      "\tTraining batch 1180 Loss: 3.348503\n",
      "\tTraining batch 1181 Loss: 3.325658\n",
      "\tTraining batch 1182 Loss: 3.334432\n",
      "\tTraining batch 1183 Loss: 3.354838\n",
      "\tTraining batch 1184 Loss: 3.318588\n",
      "\tTraining batch 1185 Loss: 3.346238\n",
      "\tTraining batch 1186 Loss: 3.345756\n",
      "\tTraining batch 1187 Loss: 3.335405\n",
      "\tTraining batch 1188 Loss: 3.340997\n",
      "\tTraining batch 1189 Loss: 3.350265\n",
      "\tTraining batch 1190 Loss: 3.339184\n",
      "\tTraining batch 1191 Loss: 3.322866\n",
      "\tTraining batch 1192 Loss: 3.349237\n",
      "\tTraining batch 1193 Loss: 3.329749\n",
      "\tTraining batch 1194 Loss: 3.335095\n",
      "\tTraining batch 1195 Loss: 3.337188\n",
      "\tTraining batch 1196 Loss: 3.329772\n",
      "\tTraining batch 1197 Loss: 3.346220\n",
      "\tTraining batch 1198 Loss: 3.354349\n",
      "\tTraining batch 1199 Loss: 3.357557\n",
      "\tTraining batch 1200 Loss: 3.358928\n",
      "\tTraining batch 1201 Loss: 3.336655\n",
      "\tTraining batch 1202 Loss: 3.344243\n",
      "\tTraining batch 1203 Loss: 3.354491\n",
      "\tTraining batch 1204 Loss: 3.340842\n",
      "\tTraining batch 1205 Loss: 3.348651\n",
      "\tTraining batch 1206 Loss: 3.337517\n",
      "\tTraining batch 1207 Loss: 3.358351\n",
      "\tTraining batch 1208 Loss: 3.317149\n",
      "\tTraining batch 1209 Loss: 3.324291\n",
      "\tTraining batch 1210 Loss: 3.355605\n",
      "\tTraining batch 1211 Loss: 3.322245\n",
      "\tTraining batch 1212 Loss: 3.344735\n",
      "\tTraining batch 1213 Loss: 3.338241\n",
      "\tTraining batch 1214 Loss: 3.333519\n",
      "\tTraining batch 1215 Loss: 3.329772\n",
      "\tTraining batch 1216 Loss: 3.331792\n",
      "\tTraining batch 1217 Loss: 3.332236\n",
      "\tTraining batch 1218 Loss: 3.321983\n",
      "\tTraining batch 1219 Loss: 3.365846\n",
      "\tTraining batch 1220 Loss: 3.329534\n",
      "\tTraining batch 1221 Loss: 3.338159\n",
      "\tTraining batch 1222 Loss: 3.330540\n",
      "\tTraining batch 1223 Loss: 3.322605\n",
      "\tTraining batch 1224 Loss: 3.328937\n",
      "\tTraining batch 1225 Loss: 3.334311\n",
      "\tTraining batch 1226 Loss: 3.317718\n",
      "\tTraining batch 1227 Loss: 3.327970\n",
      "\tTraining batch 1228 Loss: 3.340385\n",
      "\tTraining batch 1229 Loss: 3.330580\n",
      "\tTraining batch 1230 Loss: 3.335839\n",
      "\tTraining batch 1231 Loss: 3.341959\n",
      "\tTraining batch 1232 Loss: 3.351129\n",
      "\tTraining batch 1233 Loss: 3.337312\n",
      "\tTraining batch 1234 Loss: 3.330207\n",
      "\tTraining batch 1235 Loss: 3.331340\n",
      "\tTraining batch 1236 Loss: 3.344758\n",
      "\tTraining batch 1237 Loss: 3.342976\n",
      "\tTraining batch 1238 Loss: 3.354633\n",
      "\tTraining batch 1239 Loss: 3.325744\n",
      "\tTraining batch 1240 Loss: 3.343076\n",
      "\tTraining batch 1241 Loss: 3.340555\n",
      "\tTraining batch 1242 Loss: 3.339545\n",
      "\tTraining batch 1243 Loss: 3.317040\n",
      "\tTraining batch 1244 Loss: 3.349116\n",
      "\tTraining batch 1245 Loss: 3.330162\n",
      "\tTraining batch 1246 Loss: 3.329194\n",
      "\tTraining batch 1247 Loss: 3.326669\n",
      "\tTraining batch 1248 Loss: 3.331901\n",
      "\tTraining batch 1249 Loss: 3.339747\n",
      "\tTraining batch 1250 Loss: 3.342860\n",
      "\tTraining batch 1251 Loss: 3.329951\n",
      "\tTraining batch 1252 Loss: 3.337639\n",
      "\tTraining batch 1253 Loss: 3.338326\n",
      "\tTraining batch 1254 Loss: 3.325840\n",
      "\tTraining batch 1255 Loss: 3.321384\n",
      "\tTraining batch 1256 Loss: 3.337801\n",
      "\tTraining batch 1257 Loss: 3.318397\n",
      "\tTraining batch 1258 Loss: 3.344532\n",
      "\tTraining batch 1259 Loss: 3.346577\n",
      "\tTraining batch 1260 Loss: 3.332834\n",
      "\tTraining batch 1261 Loss: 3.331098\n",
      "\tTraining batch 1262 Loss: 3.334235\n",
      "\tTraining batch 1263 Loss: 3.342691\n",
      "\tTraining batch 1264 Loss: 3.338489\n",
      "\tTraining batch 1265 Loss: 3.314232\n",
      "\tTraining batch 1266 Loss: 3.315161\n",
      "\tTraining batch 1267 Loss: 3.329299\n",
      "\tTraining batch 1268 Loss: 3.350015\n",
      "\tTraining batch 1269 Loss: 3.331817\n",
      "\tTraining batch 1270 Loss: 3.354590\n",
      "\tTraining batch 1271 Loss: 3.339240\n",
      "\tTraining batch 1272 Loss: 3.332086\n",
      "\tTraining batch 1273 Loss: 3.322966\n",
      "\tTraining batch 1274 Loss: 3.368319\n",
      "\tTraining batch 1275 Loss: 3.341779\n",
      "\tTraining batch 1276 Loss: 3.333070\n",
      "\tTraining batch 1277 Loss: 3.349556\n",
      "\tTraining batch 1278 Loss: 3.337134\n",
      "\tTraining batch 1279 Loss: 3.329103\n",
      "\tTraining batch 1280 Loss: 3.328071\n",
      "\tTraining batch 1281 Loss: 3.345470\n",
      "\tTraining batch 1282 Loss: 3.339963\n",
      "\tTraining batch 1283 Loss: 3.333255\n",
      "\tTraining batch 1284 Loss: 3.331713\n",
      "\tTraining batch 1285 Loss: 3.349775\n",
      "\tTraining batch 1286 Loss: 3.345276\n",
      "\tTraining batch 1287 Loss: 3.321605\n",
      "\tTraining batch 1288 Loss: 3.339379\n",
      "\tTraining batch 1289 Loss: 3.322124\n",
      "\tTraining batch 1290 Loss: 3.337065\n",
      "\tTraining batch 1291 Loss: 3.340768\n",
      "\tTraining batch 1292 Loss: 3.325779\n",
      "\tTraining batch 1293 Loss: 3.322781\n",
      "\tTraining batch 1294 Loss: 3.352118\n",
      "\tTraining batch 1295 Loss: 3.324696\n",
      "\tTraining batch 1296 Loss: 3.323280\n",
      "\tTraining batch 1297 Loss: 3.339182\n",
      "\tTraining batch 1298 Loss: 3.339743\n",
      "\tTraining batch 1299 Loss: 3.334276\n",
      "\tTraining batch 1300 Loss: 3.324449\n",
      "\tTraining batch 1301 Loss: 3.319164\n",
      "\tTraining batch 1302 Loss: 3.326342\n",
      "\tTraining batch 1303 Loss: 3.330871\n",
      "\tTraining batch 1304 Loss: 3.335196\n",
      "\tTraining batch 1305 Loss: 3.340401\n",
      "\tTraining batch 1306 Loss: 3.342248\n",
      "\tTraining batch 1307 Loss: 3.311505\n",
      "\tTraining batch 1308 Loss: 3.331188\n",
      "\tTraining batch 1309 Loss: 3.332615\n",
      "\tTraining batch 1310 Loss: 3.310678\n",
      "\tTraining batch 1311 Loss: 3.321068\n",
      "\tTraining batch 1312 Loss: 3.336229\n",
      "\tTraining batch 1313 Loss: 3.323745\n",
      "\tTraining batch 1314 Loss: 3.339181\n",
      "\tTraining batch 1315 Loss: 3.325593\n",
      "\tTraining batch 1316 Loss: 3.310630\n",
      "\tTraining batch 1317 Loss: 3.329857\n",
      "\tTraining batch 1318 Loss: 3.360611\n",
      "\tTraining batch 1319 Loss: 3.327532\n",
      "\tTraining batch 1320 Loss: 3.350482\n",
      "\tTraining batch 1321 Loss: 3.360978\n",
      "\tTraining batch 1322 Loss: 3.313076\n",
      "\tTraining batch 1323 Loss: 3.338669\n",
      "\tTraining batch 1324 Loss: 3.343921\n",
      "\tTraining batch 1325 Loss: 3.321280\n",
      "\tTraining batch 1326 Loss: 3.313512\n",
      "\tTraining batch 1327 Loss: 3.309001\n",
      "\tTraining batch 1328 Loss: 3.319891\n",
      "\tTraining batch 1329 Loss: 3.319945\n",
      "\tTraining batch 1330 Loss: 3.336534\n",
      "\tTraining batch 1331 Loss: 3.349812\n",
      "\tTraining batch 1332 Loss: 3.333838\n",
      "\tTraining batch 1333 Loss: 3.338053\n",
      "\tTraining batch 1334 Loss: 3.339400\n",
      "\tTraining batch 1335 Loss: 3.363191\n",
      "\tTraining batch 1336 Loss: 3.337835\n",
      "\tTraining batch 1337 Loss: 3.337056\n",
      "\tTraining batch 1338 Loss: 3.334824\n",
      "\tTraining batch 1339 Loss: 3.351767\n",
      "\tTraining batch 1340 Loss: 3.326083\n",
      "\tTraining batch 1341 Loss: 3.341036\n",
      "\tTraining batch 1342 Loss: 3.339230\n",
      "\tTraining batch 1343 Loss: 3.317804\n",
      "\tTraining batch 1344 Loss: 3.333610\n",
      "\tTraining batch 1345 Loss: 3.330250\n",
      "\tTraining batch 1346 Loss: 3.327838\n",
      "\tTraining batch 1347 Loss: 3.323750\n",
      "\tTraining batch 1348 Loss: 3.324984\n",
      "\tTraining batch 1349 Loss: 3.340889\n",
      "\tTraining batch 1350 Loss: 3.346946\n",
      "\tTraining batch 1351 Loss: 3.317602\n",
      "\tTraining batch 1352 Loss: 3.345782\n",
      "\tTraining batch 1353 Loss: 3.344858\n",
      "\tTraining batch 1354 Loss: 3.353979\n",
      "\tTraining batch 1355 Loss: 3.308961\n",
      "\tTraining batch 1356 Loss: 3.345475\n",
      "\tTraining batch 1357 Loss: 3.349617\n",
      "\tTraining batch 1358 Loss: 3.363331\n",
      "\tTraining batch 1359 Loss: 3.320823\n",
      "\tTraining batch 1360 Loss: 3.342708\n",
      "\tTraining batch 1361 Loss: 3.327873\n",
      "\tTraining batch 1362 Loss: 3.319620\n",
      "\tTraining batch 1363 Loss: 3.320446\n",
      "\tTraining batch 1364 Loss: 3.336799\n",
      "\tTraining batch 1365 Loss: 3.316184\n",
      "\tTraining batch 1366 Loss: 3.339586\n",
      "\tTraining batch 1367 Loss: 3.343494\n",
      "\tTraining batch 1368 Loss: 3.341327\n",
      "\tTraining batch 1369 Loss: 3.364593\n",
      "\tTraining batch 1370 Loss: 3.340407\n",
      "\tTraining batch 1371 Loss: 3.315141\n",
      "\tTraining batch 1372 Loss: 3.348927\n",
      "\tTraining batch 1373 Loss: 3.329344\n",
      "\tTraining batch 1374 Loss: 3.346136\n",
      "\tTraining batch 1375 Loss: 3.353611\n",
      "\tTraining batch 1376 Loss: 3.313661\n",
      "\tTraining batch 1377 Loss: 3.320831\n",
      "\tTraining batch 1378 Loss: 3.311231\n",
      "\tTraining batch 1379 Loss: 3.353946\n",
      "\tTraining batch 1380 Loss: 3.341153\n",
      "\tTraining batch 1381 Loss: 3.317809\n",
      "\tTraining batch 1382 Loss: 3.318008\n",
      "\tTraining batch 1383 Loss: 3.339648\n",
      "\tTraining batch 1384 Loss: 3.324135\n",
      "\tTraining batch 1385 Loss: 3.321221\n",
      "\tTraining batch 1386 Loss: 3.319346\n",
      "\tTraining batch 1387 Loss: 3.335568\n",
      "\tTraining batch 1388 Loss: 3.363030\n",
      "\tTraining batch 1389 Loss: 3.343450\n",
      "\tTraining batch 1390 Loss: 3.351283\n",
      "\tTraining batch 1391 Loss: 3.347302\n",
      "\tTraining batch 1392 Loss: 3.329224\n",
      "\tTraining batch 1393 Loss: 3.319067\n",
      "\tTraining batch 1394 Loss: 3.320840\n",
      "\tTraining batch 1395 Loss: 3.311513\n",
      "\tTraining batch 1396 Loss: 3.336647\n",
      "\tTraining batch 1397 Loss: 3.315203\n",
      "\tTraining batch 1398 Loss: 3.332478\n",
      "\tTraining batch 1399 Loss: 3.318486\n",
      "\tTraining batch 1400 Loss: 3.342171\n",
      "\tTraining batch 1401 Loss: 3.353911\n",
      "\tTraining batch 1402 Loss: 3.347771\n",
      "\tTraining batch 1403 Loss: 3.360989\n",
      "\tTraining batch 1404 Loss: 3.338610\n",
      "\tTraining batch 1405 Loss: 3.338137\n",
      "\tTraining batch 1406 Loss: 3.339376\n",
      "\tTraining batch 1407 Loss: 3.333782\n",
      "\tTraining batch 1408 Loss: 3.324232\n",
      "\tTraining batch 1409 Loss: 3.335518\n",
      "\tTraining batch 1410 Loss: 3.325870\n",
      "\tTraining batch 1411 Loss: 3.346024\n",
      "\tTraining batch 1412 Loss: 3.328478\n",
      "\tTraining batch 1413 Loss: 3.323151\n",
      "\tTraining batch 1414 Loss: 3.350623\n",
      "\tTraining batch 1415 Loss: 3.324134\n",
      "\tTraining batch 1416 Loss: 3.333709\n",
      "\tTraining batch 1417 Loss: 3.355400\n",
      "\tTraining batch 1418 Loss: 3.326984\n",
      "\tTraining batch 1419 Loss: 3.309695\n",
      "\tTraining batch 1420 Loss: 3.340401\n",
      "\tTraining batch 1421 Loss: 3.335351\n",
      "\tTraining batch 1422 Loss: 3.340836\n",
      "\tTraining batch 1423 Loss: 3.344244\n",
      "\tTraining batch 1424 Loss: 3.332053\n",
      "\tTraining batch 1425 Loss: 3.341284\n",
      "\tTraining batch 1426 Loss: 3.339212\n",
      "\tTraining batch 1427 Loss: 3.338709\n",
      "\tTraining batch 1428 Loss: 3.329767\n",
      "\tTraining batch 1429 Loss: 3.359261\n",
      "\tTraining batch 1430 Loss: 3.337981\n",
      "\tTraining batch 1431 Loss: 3.332230\n",
      "\tTraining batch 1432 Loss: 3.333051\n",
      "\tTraining batch 1433 Loss: 3.343181\n",
      "\tTraining batch 1434 Loss: 3.333508\n",
      "\tTraining batch 1435 Loss: 3.334643\n",
      "\tTraining batch 1436 Loss: 3.323208\n",
      "\tTraining batch 1437 Loss: 3.352623\n",
      "\tTraining batch 1438 Loss: 3.324095\n",
      "\tTraining batch 1439 Loss: 3.340836\n",
      "\tTraining batch 1440 Loss: 3.325910\n",
      "\tTraining batch 1441 Loss: 3.337711\n",
      "\tTraining batch 1442 Loss: 3.339495\n",
      "\tTraining batch 1443 Loss: 3.334932\n",
      "\tTraining batch 1444 Loss: 3.340371\n",
      "\tTraining batch 1445 Loss: 3.326681\n",
      "\tTraining batch 1446 Loss: 3.345809\n",
      "\tTraining batch 1447 Loss: 3.337549\n",
      "\tTraining batch 1448 Loss: 3.349167\n",
      "\tTraining batch 1449 Loss: 3.314039\n",
      "\tTraining batch 1450 Loss: 3.337525\n",
      "\tTraining batch 1451 Loss: 3.327807\n",
      "\tTraining batch 1452 Loss: 3.324448\n",
      "\tTraining batch 1453 Loss: 3.334494\n",
      "\tTraining batch 1454 Loss: 3.344523\n",
      "\tTraining batch 1455 Loss: 3.345281\n",
      "\tTraining batch 1456 Loss: 3.327499\n",
      "\tTraining batch 1457 Loss: 3.338773\n",
      "\tTraining batch 1458 Loss: 3.333371\n",
      "\tTraining batch 1459 Loss: 3.328732\n",
      "\tTraining batch 1460 Loss: 3.335390\n",
      "\tTraining batch 1461 Loss: 3.313166\n",
      "\tTraining batch 1462 Loss: 3.334213\n",
      "\tTraining batch 1463 Loss: 3.321120\n",
      "\tTraining batch 1464 Loss: 3.319039\n",
      "\tTraining batch 1465 Loss: 3.314632\n",
      "\tTraining batch 1466 Loss: 3.339471\n",
      "\tTraining batch 1467 Loss: 3.335297\n",
      "\tTraining batch 1468 Loss: 3.328770\n",
      "\tTraining batch 1469 Loss: 3.341701\n",
      "\tTraining batch 1470 Loss: 3.327327\n",
      "\tTraining batch 1471 Loss: 3.345537\n",
      "\tTraining batch 1472 Loss: 3.337736\n",
      "\tTraining batch 1473 Loss: 3.337859\n",
      "\tTraining batch 1474 Loss: 3.316337\n",
      "\tTraining batch 1475 Loss: 3.319956\n",
      "\tTraining batch 1476 Loss: 3.327868\n",
      "\tTraining batch 1477 Loss: 3.327634\n",
      "\tTraining batch 1478 Loss: 3.334918\n",
      "\tTraining batch 1479 Loss: 3.328736\n",
      "\tTraining batch 1480 Loss: 3.334339\n",
      "\tTraining batch 1481 Loss: 3.337755\n",
      "\tTraining batch 1482 Loss: 3.341524\n",
      "\tTraining batch 1483 Loss: 3.337326\n",
      "\tTraining batch 1484 Loss: 3.327931\n",
      "\tTraining batch 1485 Loss: 3.339583\n",
      "\tTraining batch 1486 Loss: 3.352989\n",
      "\tTraining batch 1487 Loss: 3.332644\n",
      "\tTraining batch 1488 Loss: 3.340182\n",
      "\tTraining batch 1489 Loss: 3.333171\n",
      "\tTraining batch 1490 Loss: 3.338763\n",
      "\tTraining batch 1491 Loss: 3.344577\n",
      "\tTraining batch 1492 Loss: 3.340223\n",
      "\tTraining batch 1493 Loss: 3.344386\n",
      "\tTraining batch 1494 Loss: 3.336612\n",
      "\tTraining batch 1495 Loss: 3.339895\n",
      "\tTraining batch 1496 Loss: 3.327743\n",
      "\tTraining batch 1497 Loss: 3.330444\n",
      "\tTraining batch 1498 Loss: 3.357410\n",
      "\tTraining batch 1499 Loss: 3.323966\n",
      "\tTraining batch 1500 Loss: 3.346753\n",
      "\tTraining batch 1501 Loss: 3.345841\n",
      "\tTraining batch 1502 Loss: 3.325855\n",
      "\tTraining batch 1503 Loss: 3.338128\n",
      "\tTraining batch 1504 Loss: 3.325705\n",
      "\tTraining batch 1505 Loss: 3.344273\n",
      "\tTraining batch 1506 Loss: 3.353957\n",
      "\tTraining batch 1507 Loss: 3.335719\n",
      "\tTraining batch 1508 Loss: 3.334030\n",
      "\tTraining batch 1509 Loss: 3.318076\n",
      "\tTraining batch 1510 Loss: 3.303652\n",
      "\tTraining batch 1511 Loss: 3.328987\n",
      "\tTraining batch 1512 Loss: 3.328413\n",
      "\tTraining batch 1513 Loss: 3.331674\n",
      "\tTraining batch 1514 Loss: 3.357470\n",
      "\tTraining batch 1515 Loss: 3.347206\n",
      "\tTraining batch 1516 Loss: 3.330640\n",
      "\tTraining batch 1517 Loss: 3.326611\n",
      "\tTraining batch 1518 Loss: 3.335765\n",
      "\tTraining batch 1519 Loss: 3.334954\n",
      "\tTraining batch 1520 Loss: 3.345339\n",
      "\tTraining batch 1521 Loss: 3.339609\n",
      "\tTraining batch 1522 Loss: 3.345441\n",
      "\tTraining batch 1523 Loss: 3.343791\n",
      "\tTraining batch 1524 Loss: 3.315274\n",
      "\tTraining batch 1525 Loss: 3.332890\n",
      "\tTraining batch 1526 Loss: 3.344860\n",
      "\tTraining batch 1527 Loss: 3.348054\n",
      "\tTraining batch 1528 Loss: 3.326198\n",
      "\tTraining batch 1529 Loss: 3.328914\n",
      "\tTraining batch 1530 Loss: 3.322603\n",
      "\tTraining batch 1531 Loss: 3.340525\n",
      "\tTraining batch 1532 Loss: 3.312139\n",
      "\tTraining batch 1533 Loss: 3.333746\n",
      "\tTraining batch 1534 Loss: 3.325324\n",
      "\tTraining batch 1535 Loss: 3.331428\n",
      "\tTraining batch 1536 Loss: 3.326834\n",
      "\tTraining batch 1537 Loss: 3.353607\n",
      "\tTraining batch 1538 Loss: 3.325270\n",
      "\tTraining batch 1539 Loss: 3.337166\n",
      "\tTraining batch 1540 Loss: 3.331152\n",
      "\tTraining batch 1541 Loss: 3.330155\n",
      "\tTraining batch 1542 Loss: 3.335624\n",
      "\tTraining batch 1543 Loss: 3.344065\n",
      "\tTraining batch 1544 Loss: 3.330390\n",
      "\tTraining batch 1545 Loss: 3.370123\n",
      "\tTraining batch 1546 Loss: 3.321051\n",
      "\tTraining batch 1547 Loss: 3.345314\n",
      "\tTraining batch 1548 Loss: 3.319998\n",
      "\tTraining batch 1549 Loss: 3.326456\n",
      "\tTraining batch 1550 Loss: 3.348907\n",
      "\tTraining batch 1551 Loss: 3.334400\n",
      "\tTraining batch 1552 Loss: 3.344656\n",
      "\tTraining batch 1553 Loss: 3.350966\n",
      "\tTraining batch 1554 Loss: 3.337632\n",
      "\tTraining batch 1555 Loss: 3.347133\n",
      "\tTraining batch 1556 Loss: 3.342766\n",
      "\tTraining batch 1557 Loss: 3.346754\n",
      "\tTraining batch 1558 Loss: 3.350731\n",
      "\tTraining batch 1559 Loss: 3.333938\n",
      "\tTraining batch 1560 Loss: 3.331641\n",
      "\tTraining batch 1561 Loss: 3.345264\n",
      "\tTraining batch 1562 Loss: 3.316160\n",
      "\tTraining batch 1563 Loss: 3.329360\n",
      "\tTraining batch 1564 Loss: 3.333992\n",
      "\tTraining batch 1565 Loss: 3.332156\n",
      "\tTraining batch 1566 Loss: 3.329276\n",
      "\tTraining batch 1567 Loss: 3.329075\n",
      "\tTraining batch 1568 Loss: 3.330670\n",
      "\tTraining batch 1569 Loss: 3.347542\n",
      "\tTraining batch 1570 Loss: 3.361614\n",
      "\tTraining batch 1571 Loss: 3.329128\n",
      "\tTraining batch 1572 Loss: 3.348485\n",
      "\tTraining batch 1573 Loss: 3.342998\n",
      "\tTraining batch 1574 Loss: 3.333458\n",
      "\tTraining batch 1575 Loss: 3.330640\n",
      "\tTraining batch 1576 Loss: 3.331524\n",
      "\tTraining batch 1577 Loss: 3.319973\n",
      "\tTraining batch 1578 Loss: 3.355490\n",
      "\tTraining batch 1579 Loss: 3.335675\n",
      "\tTraining batch 1580 Loss: 3.328827\n",
      "\tTraining batch 1581 Loss: 3.325031\n",
      "\tTraining batch 1582 Loss: 3.346959\n",
      "\tTraining batch 1583 Loss: 3.332413\n",
      "\tTraining batch 1584 Loss: 3.332312\n",
      "\tTraining batch 1585 Loss: 3.342672\n",
      "\tTraining batch 1586 Loss: 3.346981\n",
      "\tTraining batch 1587 Loss: 3.347780\n",
      "\tTraining batch 1588 Loss: 3.326227\n",
      "\tTraining batch 1589 Loss: 3.332551\n",
      "\tTraining batch 1590 Loss: 3.328366\n",
      "\tTraining batch 1591 Loss: 3.331953\n",
      "\tTraining batch 1592 Loss: 3.340111\n",
      "\tTraining batch 1593 Loss: 3.336807\n",
      "\tTraining batch 1594 Loss: 3.320412\n",
      "\tTraining batch 1595 Loss: 3.335052\n",
      "\tTraining batch 1596 Loss: 3.337564\n",
      "\tTraining batch 1597 Loss: 3.331523\n",
      "\tTraining batch 1598 Loss: 3.326742\n",
      "\tTraining batch 1599 Loss: 3.330207\n",
      "\tTraining batch 1600 Loss: 3.340801\n",
      "\tTraining batch 1601 Loss: 3.327324\n",
      "\tTraining batch 1602 Loss: 3.322529\n",
      "\tTraining batch 1603 Loss: 3.332038\n",
      "\tTraining batch 1604 Loss: 3.320061\n",
      "\tTraining batch 1605 Loss: 3.338099\n",
      "\tTraining batch 1606 Loss: 3.321885\n",
      "\tTraining batch 1607 Loss: 3.320711\n",
      "\tTraining batch 1608 Loss: 3.335613\n",
      "\tTraining batch 1609 Loss: 3.332639\n",
      "\tTraining batch 1610 Loss: 3.340690\n",
      "\tTraining batch 1611 Loss: 3.315008\n",
      "\tTraining batch 1612 Loss: 3.337690\n",
      "\tTraining batch 1613 Loss: 3.334460\n",
      "\tTraining batch 1614 Loss: 3.320475\n",
      "\tTraining batch 1615 Loss: 3.346795\n",
      "\tTraining batch 1616 Loss: 3.343661\n",
      "\tTraining batch 1617 Loss: 3.328682\n",
      "\tTraining batch 1618 Loss: 3.319402\n",
      "\tTraining batch 1619 Loss: 3.350303\n",
      "\tTraining batch 1620 Loss: 3.347605\n",
      "\tTraining batch 1621 Loss: 3.325420\n",
      "\tTraining batch 1622 Loss: 3.341461\n",
      "\tTraining batch 1623 Loss: 3.341004\n",
      "\tTraining batch 1624 Loss: 3.342211\n",
      "\tTraining batch 1625 Loss: 3.351953\n",
      "\tTraining batch 1626 Loss: 3.351188\n",
      "\tTraining batch 1627 Loss: 3.353325\n",
      "\tTraining batch 1628 Loss: 3.334775\n",
      "\tTraining batch 1629 Loss: 3.333014\n",
      "\tTraining batch 1630 Loss: 3.344587\n",
      "\tTraining batch 1631 Loss: 3.339795\n",
      "\tTraining batch 1632 Loss: 3.333413\n",
      "\tTraining batch 1633 Loss: 3.340676\n",
      "\tTraining batch 1634 Loss: 3.348327\n",
      "\tTraining batch 1635 Loss: 3.327048\n",
      "\tTraining batch 1636 Loss: 3.328969\n",
      "\tTraining batch 1637 Loss: 3.340593\n",
      "\tTraining batch 1638 Loss: 3.332555\n",
      "\tTraining batch 1639 Loss: 3.330128\n",
      "\tTraining batch 1640 Loss: 3.329234\n",
      "\tTraining batch 1641 Loss: 3.346405\n",
      "\tTraining batch 1642 Loss: 3.348613\n",
      "\tTraining batch 1643 Loss: 3.327152\n",
      "\tTraining batch 1644 Loss: 3.329873\n",
      "\tTraining batch 1645 Loss: 3.340183\n",
      "\tTraining batch 1646 Loss: 3.330886\n",
      "\tTraining batch 1647 Loss: 3.332290\n",
      "\tTraining batch 1648 Loss: 3.325151\n",
      "\tTraining batch 1649 Loss: 3.325385\n",
      "\tTraining batch 1650 Loss: 3.328662\n",
      "\tTraining batch 1651 Loss: 3.324435\n",
      "\tTraining batch 1652 Loss: 3.341212\n",
      "\tTraining batch 1653 Loss: 3.332771\n",
      "\tTraining batch 1654 Loss: 3.349408\n",
      "\tTraining batch 1655 Loss: 3.332836\n",
      "\tTraining batch 1656 Loss: 3.328327\n",
      "\tTraining batch 1657 Loss: 3.325505\n",
      "\tTraining batch 1658 Loss: 3.323005\n",
      "\tTraining batch 1659 Loss: 3.340388\n",
      "\tTraining batch 1660 Loss: 3.333106\n",
      "\tTraining batch 1661 Loss: 3.341121\n",
      "\tTraining batch 1662 Loss: 3.307728\n",
      "\tTraining batch 1663 Loss: 3.339338\n",
      "\tTraining batch 1664 Loss: 3.364484\n",
      "\tTraining batch 1665 Loss: 3.323919\n",
      "\tTraining batch 1666 Loss: 3.332644\n",
      "\tTraining batch 1667 Loss: 3.342285\n",
      "\tTraining batch 1668 Loss: 3.314819\n",
      "\tTraining batch 1669 Loss: 3.335726\n",
      "\tTraining batch 1670 Loss: 3.331648\n",
      "\tTraining batch 1671 Loss: 3.341716\n",
      "\tTraining batch 1672 Loss: 3.356224\n",
      "\tTraining batch 1673 Loss: 3.333700\n",
      "\tTraining batch 1674 Loss: 3.318677\n",
      "\tTraining batch 1675 Loss: 3.332615\n",
      "\tTraining batch 1676 Loss: 3.351184\n",
      "\tTraining batch 1677 Loss: 3.329292\n",
      "\tTraining batch 1678 Loss: 3.330007\n",
      "\tTraining batch 1679 Loss: 3.324183\n",
      "\tTraining batch 1680 Loss: 3.333944\n",
      "\tTraining batch 1681 Loss: 3.321621\n",
      "\tTraining batch 1682 Loss: 3.333617\n",
      "\tTraining batch 1683 Loss: 3.327847\n",
      "\tTraining batch 1684 Loss: 3.334905\n",
      "\tTraining batch 1685 Loss: 3.313191\n",
      "\tTraining batch 1686 Loss: 3.341511\n",
      "\tTraining batch 1687 Loss: 3.352710\n",
      "\tTraining batch 1688 Loss: 3.320853\n",
      "\tTraining batch 1689 Loss: 3.336434\n",
      "\tTraining batch 1690 Loss: 3.339563\n",
      "\tTraining batch 1691 Loss: 3.337231\n",
      "\tTraining batch 1692 Loss: 3.301337\n",
      "\tTraining batch 1693 Loss: 3.329764\n",
      "\tTraining batch 1694 Loss: 3.327475\n",
      "\tTraining batch 1695 Loss: 3.326753\n",
      "\tTraining batch 1696 Loss: 3.337615\n",
      "\tTraining batch 1697 Loss: 3.335399\n",
      "\tTraining batch 1698 Loss: 3.341351\n",
      "\tTraining batch 1699 Loss: 3.332715\n",
      "\tTraining batch 1700 Loss: 3.334486\n",
      "\tTraining batch 1701 Loss: 3.340723\n",
      "\tTraining batch 1702 Loss: 3.363982\n",
      "\tTraining batch 1703 Loss: 3.345709\n",
      "\tTraining batch 1704 Loss: 3.316711\n",
      "\tTraining batch 1705 Loss: 3.338653\n",
      "\tTraining batch 1706 Loss: 3.332606\n",
      "\tTraining batch 1707 Loss: 3.338776\n",
      "\tTraining batch 1708 Loss: 3.336923\n",
      "\tTraining batch 1709 Loss: 3.330359\n",
      "\tTraining batch 1710 Loss: 3.339391\n",
      "\tTraining batch 1711 Loss: 3.326823\n",
      "\tTraining batch 1712 Loss: 3.344323\n",
      "\tTraining batch 1713 Loss: 3.354460\n",
      "\tTraining batch 1714 Loss: 3.322911\n",
      "\tTraining batch 1715 Loss: 3.326969\n",
      "\tTraining batch 1716 Loss: 3.311846\n",
      "\tTraining batch 1717 Loss: 3.332431\n",
      "\tTraining batch 1718 Loss: 3.349169\n",
      "\tTraining batch 1719 Loss: 3.341959\n",
      "\tTraining batch 1720 Loss: 3.341484\n",
      "\tTraining batch 1721 Loss: 3.352634\n",
      "\tTraining batch 1722 Loss: 3.334730\n",
      "\tTraining batch 1723 Loss: 3.332143\n",
      "\tTraining batch 1724 Loss: 3.336862\n",
      "\tTraining batch 1725 Loss: 3.333296\n",
      "\tTraining batch 1726 Loss: 3.314900\n",
      "\tTraining batch 1727 Loss: 3.341124\n",
      "\tTraining batch 1728 Loss: 3.321236\n",
      "\tTraining batch 1729 Loss: 3.324332\n",
      "\tTraining batch 1730 Loss: 3.335638\n",
      "\tTraining batch 1731 Loss: 3.338229\n",
      "\tTraining batch 1732 Loss: 3.321744\n",
      "\tTraining batch 1733 Loss: 3.338464\n",
      "\tTraining batch 1734 Loss: 3.317668\n",
      "\tTraining batch 1735 Loss: 3.334647\n",
      "\tTraining batch 1736 Loss: 3.342034\n",
      "\tTraining batch 1737 Loss: 3.334486\n",
      "\tTraining batch 1738 Loss: 3.347093\n",
      "\tTraining batch 1739 Loss: 3.322262\n",
      "\tTraining batch 1740 Loss: 3.334717\n",
      "\tTraining batch 1741 Loss: 3.333259\n",
      "\tTraining batch 1742 Loss: 3.343472\n",
      "\tTraining batch 1743 Loss: 3.348230\n",
      "\tTraining batch 1744 Loss: 3.313279\n",
      "\tTraining batch 1745 Loss: 3.328289\n",
      "\tTraining batch 1746 Loss: 3.356842\n",
      "\tTraining batch 1747 Loss: 3.343981\n",
      "\tTraining batch 1748 Loss: 3.365056\n",
      "\tTraining batch 1749 Loss: 3.342080\n",
      "\tTraining batch 1750 Loss: 3.335249\n",
      "\tTraining batch 1751 Loss: 3.340684\n",
      "\tTraining batch 1752 Loss: 3.334500\n",
      "\tTraining batch 1753 Loss: 3.324823\n",
      "\tTraining batch 1754 Loss: 3.347604\n",
      "\tTraining batch 1755 Loss: 3.340659\n",
      "\tTraining batch 1756 Loss: 3.337439\n",
      "\tTraining batch 1757 Loss: 3.327930\n",
      "\tTraining batch 1758 Loss: 3.353659\n",
      "\tTraining batch 1759 Loss: 3.323076\n",
      "\tTraining batch 1760 Loss: 3.326633\n",
      "\tTraining batch 1761 Loss: 3.332197\n",
      "\tTraining batch 1762 Loss: 3.351200\n",
      "\tTraining batch 1763 Loss: 3.339727\n",
      "\tTraining batch 1764 Loss: 3.332197\n",
      "\tTraining batch 1765 Loss: 3.353747\n",
      "\tTraining batch 1766 Loss: 3.335573\n",
      "\tTraining batch 1767 Loss: 3.304526\n",
      "\tTraining batch 1768 Loss: 3.317879\n",
      "\tTraining batch 1769 Loss: 3.322250\n",
      "\tTraining batch 1770 Loss: 3.335307\n",
      "\tTraining batch 1771 Loss: 3.344752\n",
      "\tTraining batch 1772 Loss: 3.330610\n",
      "\tTraining batch 1773 Loss: 3.348256\n",
      "\tTraining batch 1774 Loss: 3.343660\n",
      "\tTraining batch 1775 Loss: 3.325038\n",
      "\tTraining batch 1776 Loss: 3.323944\n",
      "\tTraining batch 1777 Loss: 3.348619\n",
      "\tTraining batch 1778 Loss: 3.340342\n",
      "\tTraining batch 1779 Loss: 3.316191\n",
      "\tTraining batch 1780 Loss: 3.350110\n",
      "\tTraining batch 1781 Loss: 3.345411\n",
      "\tTraining batch 1782 Loss: 3.336638\n",
      "\tTraining batch 1783 Loss: 3.340315\n",
      "\tTraining batch 1784 Loss: 3.311763\n",
      "\tTraining batch 1785 Loss: 3.334400\n",
      "\tTraining batch 1786 Loss: 3.340160\n",
      "\tTraining batch 1787 Loss: 3.333060\n",
      "\tTraining batch 1788 Loss: 3.345220\n",
      "\tTraining batch 1789 Loss: 3.320273\n",
      "\tTraining batch 1790 Loss: 3.335219\n",
      "\tTraining batch 1791 Loss: 3.344919\n",
      "\tTraining batch 1792 Loss: 3.331560\n",
      "\tTraining batch 1793 Loss: 3.330466\n",
      "\tTraining batch 1794 Loss: 3.325371\n",
      "\tTraining batch 1795 Loss: 3.328073\n",
      "\tTraining batch 1796 Loss: 3.322943\n",
      "\tTraining batch 1797 Loss: 3.335549\n",
      "\tTraining batch 1798 Loss: 3.338277\n",
      "\tTraining batch 1799 Loss: 3.362038\n",
      "\tTraining batch 1800 Loss: 3.328622\n",
      "\tTraining batch 1801 Loss: 3.335847\n",
      "\tTraining batch 1802 Loss: 3.344283\n",
      "\tTraining batch 1803 Loss: 3.347222\n",
      "\tTraining batch 1804 Loss: 3.338782\n",
      "\tTraining batch 1805 Loss: 3.338131\n",
      "\tTraining batch 1806 Loss: 3.325210\n",
      "\tTraining batch 1807 Loss: 3.337806\n",
      "\tTraining batch 1808 Loss: 3.330456\n",
      "\tTraining batch 1809 Loss: 3.336111\n",
      "\tTraining batch 1810 Loss: 3.345716\n",
      "\tTraining batch 1811 Loss: 3.325475\n",
      "\tTraining batch 1812 Loss: 3.344115\n",
      "\tTraining batch 1813 Loss: 3.345644\n",
      "\tTraining batch 1814 Loss: 3.324420\n",
      "\tTraining batch 1815 Loss: 3.340894\n",
      "\tTraining batch 1816 Loss: 3.356633\n",
      "\tTraining batch 1817 Loss: 3.349012\n",
      "\tTraining batch 1818 Loss: 3.337303\n",
      "\tTraining batch 1819 Loss: 3.320446\n",
      "\tTraining batch 1820 Loss: 3.339377\n",
      "\tTraining batch 1821 Loss: 3.333410\n",
      "\tTraining batch 1822 Loss: 3.330984\n",
      "\tTraining batch 1823 Loss: 3.337422\n",
      "\tTraining batch 1824 Loss: 3.328786\n",
      "\tTraining batch 1825 Loss: 3.331742\n",
      "\tTraining batch 1826 Loss: 3.335983\n",
      "\tTraining batch 1827 Loss: 3.347528\n",
      "\tTraining batch 1828 Loss: 3.326591\n",
      "\tTraining batch 1829 Loss: 3.340097\n",
      "\tTraining batch 1830 Loss: 3.339396\n",
      "\tTraining batch 1831 Loss: 3.319819\n",
      "\tTraining batch 1832 Loss: 3.349105\n",
      "\tTraining batch 1833 Loss: 3.329338\n",
      "\tTraining batch 1834 Loss: 3.343196\n",
      "\tTraining batch 1835 Loss: 3.323235\n",
      "\tTraining batch 1836 Loss: 3.339957\n",
      "\tTraining batch 1837 Loss: 3.322649\n",
      "\tTraining batch 1838 Loss: 3.355929\n",
      "Training set: Average loss: 3.334625\n",
      "Validation set: Average loss: 3.334421, Accuracy: 884/25201 (4%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # use 'Adam' optimizer to adjust weights\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # specify loss criteria\n",
    "# loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# # track metrics in arrays\n",
    "# epoch_nums = []\n",
    "# training_loss = []\n",
    "# validation_loss = []\n",
    "\n",
    "# # train over 10 epochs\n",
    "# epochs = 3\n",
    "# print('Training on', device)\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "#     test_loss = test(model, device, test_loader)\n",
    "#     epoch_nums.append(epoch)\n",
    "#     training_loss.append(train_loss)\n",
    "#     validation_loss.append(test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs for model accuracy and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracies(history):\n",
    "    \"\"\"Plot history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs')\n",
    "\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    \"\"\"Plot losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs')\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model on test dataset and get results\n",
    "test_loader = DeviceDataLoader(test_loader,device)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3d88c894382e161097c0eb81ecce4a09f585543f214b88d2eaaeced97de66c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
